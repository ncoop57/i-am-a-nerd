<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Open-Dialog Chatbots for Learning New Languages [Part 1] | IAmANerd</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Open-Dialog Chatbots for Learning New Languages [Part 1]" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to fine-tune the DialoGPT model on a new dataset or language for open-dialog conversational chatbots." />
<meta property="og:description" content="How to fine-tune the DialoGPT model on a new dataset or language for open-dialog conversational chatbots." />
<link rel="canonical" href="https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html" />
<meta property="og:url" content="https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html" />
<meta property="og:site_name" content="IAmANerd" />
<meta property="og:image" content="https://nathancooper.io/i-am-a-nerd/images/chatbot.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-12T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html","@type":"BlogPosting","headline":"Open-Dialog Chatbots for Learning New Languages [Part 1]","dateModified":"2020-05-12T00:00:00-05:00","datePublished":"2020-05-12T00:00:00-05:00","image":"https://nathancooper.io/i-am-a-nerd/images/chatbot.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html"},"description":"How to fine-tune the DialoGPT model on a new dataset or language for open-dialog conversational chatbots.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/i-am-a-nerd/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nathancooper.io/i-am-a-nerd/feed.xml" title="IAmANerd" /><link rel="shortcut icon" type="image/x-icon" href="/i-am-a-nerd/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/i-am-a-nerd/">IAmANerd</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/i-am-a-nerd/about/">About Me</a><a class="page-link" href="/i-am-a-nerd/search/">Search</a><a class="page-link" href="/i-am-a-nerd/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Open-Dialog Chatbots for Learning New Languages [Part 1]</h1><p class="page-description">How to fine-tune the DialoGPT model on a new dataset or language for open-dialog conversational chatbots.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-05-12T00:00:00-05:00" itemprop="datePublished">
        May 12, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      24 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/i-am-a-nerd/categories/#chatbot">chatbot</a>
        &nbsp;
      
        <a class="category-tags-link" href="/i-am-a-nerd/categories/#deep-learning">deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/i-am-a-nerd/categories/#GPT2">GPT2</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          
          
          
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#This-notebook-was-adapted-from-the-following-project:">This notebook was adapted from the following project: </a>
<ul>
<li class="toc-entry toc-h3"><a href="#LICENSE">LICENSE </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#About">About </a></li>
<li class="toc-entry toc-h1"><a href="#Background">Background </a>
<ul>
<li class="toc-entry toc-h2"><a href="#What-is-GPT2?">What is GPT2? </a></li>
<li class="toc-entry toc-h2"><a href="#GPT2-as-a-chatbot">GPT2 as a chatbot </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#The-Data!">The Data! </a></li>
<li class="toc-entry toc-h1"><a href="#Training-and-Evaluating">Training and Evaluating </a></li>
<li class="toc-entry toc-h1"><a href="#Chatting-with-our-Model">Chatting with our Model </a></li>
<li class="toc-entry toc-h1"><a href="#Conclusion">Conclusion </a></li>
<li class="toc-entry toc-h1"><a href="#PS">PS </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-05-12-chatbot-part-1.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>Image by <a href="https://pixabay.com/users/mohamed_hassan-5229782/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3589528">mohamed Hassan</a> from <a href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3589528">Pixabay</a></p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="This-notebook-was-adapted-from-the-following-project:">
<a class="anchor" href="#This-notebook-was-adapted-from-the-following-project:" aria-hidden="true"><span class="octicon octicon-link"></span></a>This notebook was adapted from the following project:<a class="anchor-link" href="#This-notebook-was-adapted-from-the-following-project:"> </a>
</h2>
<ol>
<li><a href="https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py">https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py</a></li>
</ol>
<p>Original license of the project this notebook was adapted from: <a href="https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py">https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py</a></p>
<h3 id="LICENSE">
<a class="anchor" href="#LICENSE" aria-hidden="true"><span class="octicon octicon-link"></span></a>LICENSE<a class="anchor-link" href="#LICENSE"> </a>
</h3>
<pre><code># Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="About">
<a class="anchor" href="#About" aria-hidden="true"><span class="octicon octicon-link"></span></a>About<a class="anchor-link" href="#About"> </a>
</h1>
<p>Hola! Today we will be creating a chatbot, but not just any chatbot. In this tutorial, you will create your own open-dialog chatbot, one that doesn't just have premade responses to very specific questions or commands!</p>
<p>The overall goal of this tutorial is to create a language learning companion where you can practice simple conversations in a language you care about. We will focus on the beautiful Spanish language in this series as I have been trying to learn the language for the past 5 years, however, you should be able to adapt this tutorial to other languages as well.</p>
<p>First we are going to cover some of the background material for how all this works (if you are already familiar with the GPT2 model, go ahead and skip this background section). Let's get to it!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Background">
<a class="anchor" href="#Background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background<a class="anchor-link" href="#Background"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-GPT2?">
<a class="anchor" href="#What-is-GPT2?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is GPT2?<a class="anchor-link" href="#What-is-GPT2?"> </a>
</h2>
<p>In this post, we are going to use the GPT2 model (Generative Pre-Training 2), from the amazing paper <a href="https://openai.com/blog/better-language-models/">"Language Models are Unsupervised Multitask Learners"</a> by Alex Radford et. al. I will be giving a brief overview of this model. However, if you want a more in-depth explanation I highly recommend the blog post <a href="http://jalammar.github.io/illustrated-gpt2/">"The Illustrated GPT-2"</a> by Jay Alammar.</p>
<p>GPT2 is what is called an autoregressive language model. This may sound complicated, but it is actually quiet simple, so lets break down what this means. Autoregressive means that the output of the model is fedback into the model as input. Here is a nice example of how that works:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://github.com/ncoop57/i-am-a-nerd/blob/master/images/autoregressive.gif?raw=1" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>Image From <a href="https://deepmind.com/blog/article/wavenet-generative-model-raw-audio">Deepmind</a></p>
</blockquote>
<p>Now, a language model is usually some statistical model that gives the probability of some word given the context. So, take the following example:</p>
<blockquote>
<p><em>An [blank] a day keeps the doctor away</em></p>
</blockquote>
<p>A good language model would give higher probability to the word "apple" occuring in the [blank] than say the word "crocodile" since most likely encountering a crocodile daily would probably have the opposite effect.</p>
<p>Putting them together, we get an autoregressive language model where given some context</p>
<blockquote>
<p><em>How much wood could a woodchuck chuck, if a woodchuck could [blank]</em></p>
</blockquote>
<p>The statistical model then gives some probability to what the next word will be, which we will use in selecting the word. Once we have the selection we add it to our sentence and repeat the whole process again!</p>
<blockquote>
<p><em>How much wood could a woodchuck chuck, if a woodchuck could chuck [blank]</em></p>
</blockquote>
<p>Now, to train our autoregressive language model we just need to get a bunch of example sentences or just chunks of text, hide the last word, and use these sentences with the missing word as our inputs and the last words as the target. This is essentially the whole idea behind GPT2 and many other autoregressive language models, where they learn how language works by using the context to infer the next word.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="GPT2-as-a-chatbot">
<a class="anchor" href="#GPT2-as-a-chatbot" aria-hidden="true"><span class="octicon octicon-link"></span></a>GPT2 as a chatbot<a class="anchor-link" href="#GPT2-as-a-chatbot"> </a>
</h2>
<p>Great, so you may be asking yourself, "how do we use GPT2 as a chatbot?" To answer this question we need to turn our attention to another paper, <a href="https://arxiv.org/abs/1911.00536">"DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation"</a>. To see how we can repurpose this generator, GPT2, look at the following example:</p>
<blockquote>
<p><em>Hi, how are you? [end_of_turn] I'm good, what about you? [end_of_turn] Not so good, lots of long nights at work. [end_of_turn] Darn, that sucks :( [end_of_conversation]</em></p>
</blockquote>
<p>This is a sample conversation between two speakers. What's special about it is that there are special tokens that signify when one of the speakers has finished talking, which we in the biz call a turn. If we treat this example like our previous one with the autorgressive language mode, we can do some interesting things:</p>
<blockquote>
<p><em>Hi, how are you? [end_of_turn] [blank]</em></p>
</blockquote>
<p>If we use the same logic as we did previously, it is easy to see how we can now use GPT2 to guess the next word in this conversation.</p>
<blockquote>
<p><em>Hi, how are you? [end_of_turn] I'm [blank]</em></p>
</blockquote>
<p>We keep feeding back the prediction of our model and there ya have it! A chatting GPT2, where all we need to do is show the model a bunch of these example conversations and have it predict the next word in the conversation.</p>
<p>I think that is plenty of background, we will revisit exactly how we design a system where we actually hold a conversation with GPT2 once we have the model trained ;).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> pip -q install <span class="nv">transformers</span><span class="o">==</span><span class="m">2</span>.9.0 gdown
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     |████████████████████████████████| 645kB 5.5MB/s 
     |████████████████████████████████| 1.0MB 44.6MB/s 
     |████████████████████████████████| 890kB 45.8MB/s 
     |████████████████████████████████| 3.8MB 34.9MB/s 
  Building wheel for sacremoses (setup.py) ... done
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's define to configuration variables so we don't have a bunch of magic numbers and strings!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Args to allow for easy convertion of python script to notebook</span>
<span class="k">class</span> <span class="nc">Args</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">=</span> <span class="s1">'output'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_type</span> <span class="o">=</span> <span class="s1">'gpt2'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s1">'microsoft/DialoGPT-small'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config_name</span> <span class="o">=</span> <span class="s1">'microsoft/DialoGPT-small'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_name</span> <span class="o">=</span> <span class="s1">'microsoft/DialoGPT-small'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_dir</span> <span class="o">=</span> <span class="s1">'cached'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">512</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_train</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_eval</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_during_training</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">per_gpu_train_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">per_gpu_eval_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">5e-5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adam_epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_grad_norm</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_train_epochs</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logging_steps</span> <span class="o">=</span> <span class="mi">1000</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_steps</span> <span class="o">=</span> <span class="mi">3500</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_total_limit</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_all_checkpoints</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">no_cuda</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">overwrite_output_dir</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">overwrite_cache</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">should_continue</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fp16</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fp16_opt_level</span> <span class="o">=</span> <span class="s1">'O1'</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">Args</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-Data!">
<a class="anchor" href="#The-Data!" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Data!<a class="anchor-link" href="#The-Data!"> </a>
</h1>
<p>To train our chatbot we will be using conversations scraped from subtitles of Spanish TV shows and movies. I've gone ahead and formated the data for us already, however, if you would like to use a different language to train your chatbot you can use <a href="https://colab.research.google.com/drive/1kKErlSSpewQbWexFPEj1rPWsYpMx69ZS?usp=sharing">this script</a> to generate a csv with the same format I am going to use in the rest of this tutorial.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> gdown https://drive.google.com/uc?id<span class="o">=</span>1Lp-diuMohUTGyB9BSTFgeGZyY3dkNuEg
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Downloading...
From: https://drive.google.com/uc?id=1Lp-diuMohUTGyB9BSTFgeGZyY3dkNuEg
To: /content/final_es_conv.csv
20.3MB [00:00, 55.6MB/s]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'final_es_conv.csv'</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">trn_df</span><span class="p">,</span> <span class="n">val_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">trn_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>response</th>
      <th>context</th>
      <th>context/0</th>
      <th>context/1</th>
      <th>context/2</th>
      <th>context/3</th>
      <th>context/4</th>
      <th>context/5</th>
      <th>context/6</th>
      <th>context/7</th>
      <th>context/8</th>
      <th>context/9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>36917</th>
      <td>Es tan simple.</td>
      <td>No se que te detiene.</td>
      <td>¡Muy persuasiva!</td>
      <td>No es crimen librarse de una alimaña.</td>
      <td>¿Por qué tener lastima de un hombre tan vil?</td>
      <td>Además, también ha puesto sus ojos en Okayo.</td>
      <td>Hace 4 años que soy victima de mi marido.</td>
      <td>Solo estoy siendo franca contigo.</td>
      <td>Calmate.</td>
      <td>¡Eres peor que el diablo!</td>
      <td>¿Comprendes?</td>
      <td>Okayo me recuerda constantemente mi fracaso.</td>
    </tr>
    <tr>
      <th>5449</th>
      <td>Muy torpe, Joyce.</td>
      <td>A la sala de interrogación rápido. ¡Muévanse!</td>
      <td>De pie, muchachos.</td>
      <td>A la sala de interrogación rápido. ¡Muévanse!</td>
      <td>De pie, muchachos.</td>
      <td>¡Use su cuchillo, hombre!</td>
      <td>¡Adelántese, Thomson!</td>
      <td>¡Bien hecho, Jenkins!</td>
      <td>Gracias.</td>
      <td>Muy bien.</td>
      <td>El bungaló del mayor Warden está al final del ...</td>
      <td>Continúe, conductor.</td>
    </tr>
    <tr>
      <th>37004</th>
      <td>Pídemelo.</td>
      <td>Sólo lo que quieras tú.</td>
      <td>Ya no soy yo.</td>
      <td>Eres preciosa y maravillosa.</td>
      <td>¿No?</td>
      <td>Así te gustaré.</td>
      <td>Haré y diré lo que quieras.</td>
      <td>Nunca.</td>
      <td>Así nunca querrás estar con otras, ¿verdad?</td>
      <td>Siempre diré lo que tú desees y haré lo que tú...</td>
      <td>Pero yo sí.</td>
      <td>Pero...</td>
    </tr>
    <tr>
      <th>47077</th>
      <td>¡Boris!</td>
      <td>¡Nicolás, que alegría a mi corazón, volviste!</td>
      <td>¡Regresan los Vencedores!</td>
      <td>¡Miren!</td>
      <td>¡Ahí vienen!</td>
      <td>Está vivo.</td>
      <td>Boris está vivo.</td>
      <td>Dasha prometió avisarme cuando regrese.</td>
      <td>Pero, en la fábrica dicen que él está en una u...</td>
      <td>Tampoco hay noticias de Stepan.</td>
      <td>¡Quién sabe!</td>
      <td>¿Por qué entonces, no hay noticias de él?</td>
    </tr>
    <tr>
      <th>41450</th>
      <td>Entonces por qué no estamos en mejor situación...</td>
      <td>Dora Hartley era una buena prueba.</td>
      <td>Mire, lo que hace usted creer ¿Qué los indios ...</td>
      <td>Aleja esa arma.</td>
      <td>Buenas noches.</td>
      <td>Es hora de ir a la cama.</td>
      <td>Seguro.</td>
      <td>Sí. recuerde que es un secreto.</td>
      <td>Es bonita.</td>
      <td>Está bien.</td>
      <td>¿Ann Martin?</td>
      <td>Hola, Bax.</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">trn_df</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_df</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(40374, 10094)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_counter_and_lens</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="n">flatten</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">l</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
    <span class="n">toks</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">len</span><span class="p">,</span> <span class="n">toks</span><span class="p">)),</span> <span class="n">Counter</span><span class="p">(</span><span class="n">flatten</span><span class="p">(</span><span class="n">toks</span><span class="p">)),</span> <span class="n">Counter</span><span class="p">(</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">cache_dir</span><span class="p">)</span>
<span class="n">lens</span><span class="p">,</span> <span class="n">tok_cnt</span><span class="p">,</span> <span class="n">word_cnt</span> <span class="o">=</span> <span class="n">get_counter_and_lens</span><span class="p">(</span><span class="n">trn_df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tokenizer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>


</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot_counts</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">top_k</span> <span class="o">=</span> <span class="mi">30</span><span class="p">):</span>
    <span class="n">labels</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">()[:</span><span class="n">top_k</span><span class="p">])</span>

    <span class="n">indexes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
    <span class="n">width</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">'w'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">indexes</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">indexes</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_counts</span><span class="p">(</span><span class="n">tok_cnt</span><span class="p">,</span> <span class="n">top_k</span> <span class="o">=</span> <span class="mi">30</span><span class="p">)</span>
<span class="n">plot_counts</span><span class="p">(</span><span class="n">word_cnt</span><span class="p">,</span> <span class="n">top_k</span> <span class="o">=</span> <span class="mi">30</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABCsAAADQCAYAAAAu0euYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAJOgAACToB8GSSSgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3xU5Z3H8e/kJqDNrbAIGy7VpQ2iVC5BmMkwgZgQQqCAzVAXA5TFFott2RUJQipgpOXS0rK74OoCFRewZngpInSBBgwkJAjEgmEJclljQwAlJiRpFJKZnP2DzTSREFAzl+Dn/dfkmTPze57JkzlnvjnnGZNhGIYAAAAAAAD8RICvOwAAAAAAANAUYQUAAAAAAPArhBUAAAAAAMCvEFYAAAAAAAC/EuTrDtzIoEGDdO+99/q6GwAAAAAAwEPOnj2rwsLC69r9Nqy49957lZWV5etuAAAAAAAAD7Hb7S22cxkIAAAAAADwK4QVAAAAAADArxBWAAAAAAAAv0JYAQAAAAAA/AphBQAAAAAA8CuEFQAAAAAAwK8QVgAAAAAAAL8S5OsO3I56z9vhs9olS8f4rDYAAAAAAG2BMysAAAAAAIBfaTWsaGho0LRp02S1WhUbG6uTJ08qLy9PZrNZsbGxKioqkiRdvHhRiYmJslgs2rhxoyTJ5XJp+vTpslqtmj17tvs5V61aJYvFonHjxqm6utqDQwMAAAAAAO1Rq2HF0aNHdfXqVeXm5upXv/qVVq5cqQULFmjHjh3avHmz0tPTJUnLli3T3LlztW/fPq1evVpXrlzR9u3b1b17d+Xm5qq2tlYFBQUqLy/Xtm3blJeXp0mTJmn16tVeGSQAAAAAAGg/Wl2zIioqSoZhyDAMVVZW6s4771RgYKAiIiIUERGhiooKSdKhQ4f0m9/8RgEBARo8eLCOHz+u/Px8jRlzbf2EpKQkHThwQJcvX5bNZpPJZFJSUpKmTp3arJ7D4ZDD4ZAklZaWemK8tz3WywAAAAAAtHethhWdO3dWcHCwoqOjdeXKFeXm5upnP/vZ3x4cFKS6ujrV19crIODaSRphYWGqqKhQZWWlQkNDb9rWVGpqqlJTUyVJdru97UYJAAAAAADajVbDit27dysoKEjvv/++jhw5oqeeeqrZOhNOp1MhISEKDg5WQ0ODAgICVFVVpcjISIWHh7u3bdp25syZZm0AAAAAAABNtbpmhWEY+uY3vynp2lkWNTU1cjqdunz5skpLS91hQ0xMjHJycuR0OlVYWKh+/frJbDYrOztbkrRr1y5ZLBbFxMRo//79zdoAAAAAAACaavXMioSEBL388suy2Wy6evWqVq5cKafTqeTkZJlMJq1Zs0aSlJ6erilTpigjI0MzZ85Ux44dlZKSoq1bt8pqtWrAgAEaNmyYJGnMmDGyWCyKiIjQpk2bPD9CAAAAAADQrpgMwzB83YmW2O12ZWVl+bobX4ovF7n0JRbYBAAAAAB8ETf67N/qZSAAAAAAAADeRlgBAAAAAAD8CmEFAAAAAADwK4QVAAAAAADArxBWAAAAAAAAv0JYAQAAAAAA/AphBQAAAAAA8CuEFQAAAAAAwK8QVgAAAAAAAL9CWAEAAAAAAPwKYQUAAAAAAPArhBUAAAAAAMCvEFYAAAAAAAC/0mpYUVBQoLi4OMXFxenb3/62/vmf/1l5eXkym82KjY1VUVGRJOnixYtKTEyUxWLRxo0bJUkul0vTp0+X1WrV7Nmz3c+5atUqWSwWjRs3TtXV1R4cGgAAAAAAaI9aDSuGDRumnJwc5eTkyGw2a/z48VqwYIF27NihzZs3Kz09XZK0bNkyzZ07V/v27dPq1at15coVbd++Xd27d1dubq5qa2tVUFCg8vJybdu2TXl5eZo0aZJWr17tlUECAAAAAID245YuA6mrq9OhQ4c0ePBgBQYGKiIiQj179lRFRYUk6dChQxo5cqSCgoI0ePBgHT9+XPn5+UpMTJQkJSUl6cCBAzp8+LBsNptMJpO7DQAAAAAAoKmgW9koOztb8fHxqqqqUmho6N8eHBSkuro61dfXKyDgWu4RFhamiooKVVZWurdtra0ph8Mhh8MhSSotLf3qowMAAAAAAO3OLZ1Z4XA4lJqaqvDw8GbrTDidToWEhCg4OFgNDQ2SpKqqKkVGRjbbtrW2plJTU5WVlaWsrCz16NGjTQYIAAAAAADal5uGFfX19Tp8+LBiY2PVqVMnOZ1OXb58WaWlpe6wISYmRjk5OXI6nSosLFS/fv1kNpuVnZ0tSdq1a5csFotiYmK0f//+Zm0AAAAAAABN3fQykOzsbI0cOdJ9mcfzzz+v5ORkmUwmrVmzRpKUnp6uKVOmKCMjQzNnzlTHjh2VkpKirVu3ymq1asCAARo2bJgkacyYMbJYLIqIiNCmTZs8ODQAAAAAANAemQzDMHzdiZbY7XZlZWX5uhtfSu95O3zdBZ8oWTrG110AAAAAALQjN/rsf0trVgAAAAAAAHgLYQUAAAAAAPArhBUAAAAAAMCvEFYAAAAAAAC/QlgBAAAAAAD8CmEFAAAAAADwK4QVAAAAAADArxBWAAAAAAAAv0JYAQAAAAAA/AphBQAAAAAA8CuEFQAAAAAAwK8QVgAAAAAAAL9CWAEAAAAAAPzKTcOKnJwcxcfHa8SIEXrjjTeUl5cns9ms2NhYFRUVSZIuXryoxMREWSwWbdy4UZLkcrk0ffp0Wa1WzZ492/18q1atksVi0bhx41RdXe2hYQEAAAAAgPaq1bDis88+029+8xv993//t95++21NmDBBCxYs0I4dO7R582alp6dLkpYtW6a5c+dq3759Wr16ta5cuaLt27ere/fuys3NVW1trQoKClReXq5t27YpLy9PkyZN0urVq70ySAAAAAAA0H60GlYUFBSoY8eOGjt2rCZMmKALFy4oMDBQERER6tmzpyoqKiRJhw4d0siRIxUUFKTBgwfr+PHjys/PV2JioiQpKSlJBw4c0OHDh2Wz2WQymdxtAAAAAAAATQW1dudHH32kM2fO6ODBg8rOztbChQsVGhr6twcHBamurk719fUKCLiWe4SFhamiokKVlZXubVtra8rhcMjhcEiSSktL226UAAAAAACg3Wj1zIrw8HBZLBaFhIQoPj5ef/7zn5utM+F0OhUSEqLg4GA1NDRIkqqqqhQZGanw8HD3tq21NZWamqqsrCxlZWWpR48ebTpQAAAAAADQPrQaVsTExKi4uFiGYejo0aO677775HQ6dfnyZZWWlrrDhpiYGOXk5MjpdKqwsFD9+vWT2WxWdna2JGnXrl2yWCyKiYnR/v37m7UBAAAAAAA01eplIJ07d9aECRPc60ysX79eZWVlSk5Olslk0po1ayRJ6enpmjJlijIyMjRz5kx17NhRKSkp2rp1q6xWqwYMGKBhw4ZJksaMGSOLxaKIiAht2rTJ8yMEAAAAAADtiskwDMPXnWiJ3W5XVlaWr7vxpfSet8PXXfCJkqVjfN0FAAAAAEA7cqPP/q1eBgIAAAAAAOBthBUAAAAAAMCvEFYAAAAAAAC/QlgBAAAAAAD8SqvfBgJ8ESwsCgAAAABoC5xZAQAAAAAA/AphBQAAAAAA8CuEFQAAAAAAwK8QVgAAAAAAAL9CWAEAAAAAAPwKYQUAAAAAAPArhBUAAAAAAMCvEFYAAAAAAAC/0mpYUVJSoi5duiguLk5xcXG6dOmSHA6HzGaz4uPjde7cOUnSyZMnNXz4cJnNZu3Zs0eSVFtbq4kTJyo2NlbLly93P2d6erqsVqvS0tJUX1/vwaEBAAAAAID26KZnVthsNuXk5CgnJ0cRERFauXKlcnJy9NxzzykzM1OSNH/+fK1bt047d+7Us88+K0lau3atkpOTlZeXp71796qsrEzHjh1TWVmZcnNzFR0drS1btnh2dAAAAAAAoN25aVhx4MABWa1WzZ8/X6dPn1bfvn0VEhIii8Wi9957T5J0/vx59enTR6GhoYqMjFR5ebny8/OVmJgoSUpISFBBQUGztqSkJB04cMCDQwMAAAAAAO1RUGt3duvWTWfOnFGnTp30+OOP6/XXX1doaKj7fpfLJUlqaGhwt4WFhamiokKVlZXubZu2devWrVlbUw6HQw6HQ5JUWlraBsMDAAAAAADtTatnVtxxxx268847ZTKZNHHiRB07dkzV1dXu+wMDA689ScDfnqaqqkqRkZEKDw93b9taW1OpqanKyspSVlaWevTo0TYjBAAAAAAA7UqrYUVNTY37dm5ursaMGaPi4mLV1dUpPz9f/fv3l3TtDIyzZ8+qpqZGFRUV6ty5s8xms7KzsyVJ2dnZGjp0aLO2Xbt2yWKxeGpcAAAAAACgnWr1MpC8vDxlZGSoU6dO+ta3vqXMzEx16NBBcXFx6tChgzZs2CBJWrJkiaZNmyaXy6XFixdLkmbMmKHHHntM69evV0pKiqKiohQVFaWuXbvKarWqZ8+emjNnjudHCAAAAAAA2hWTYRiGrzvRErvdrqysLF9340vpPW+Hr7sALypZOsbXXQAAAACAdulGn/1v+m0gAAAAAAAA3kRYAQAAAAAA/AphBQAAAAAA8CuEFQAAAAAAwK8QVgAAAAAAAL9CWAEAAAAAAPwKYQUAAAAAAPArhBUAAAAAAMCvEFYAAAAAAAC/QlgBAAAAAAD8CmEFAAAAAADwK4QVAAAAAADArxBWAAAAAAAAv3JLYcWrr76qLl26SJIcDofMZrPi4+N17tw5SdLJkyc1fPhwmc1m7dmzR5JUW1uriRMnKjY2VsuXL3c/V3p6uqxWq9LS0lRfX9/W4wEAAAAAAO1c0M02cLlccjgc6tGjh5xOp1auXKl9+/bp8OHDyszM1Isvvqj58+dr3bp16tq1q0aPHq34+HitXbtWycnJmjFjhpKSkjR58mSVl5errKxMubm5WrJkibZs2aJHH33UG+MEPKb3vB0+q12ydIzPagMAAACAp9z0zIpXX31VqampCggI0OnTp9W3b1+FhITIYrHovffekySdP39effr0UWhoqCIjI1VeXq78/HwlJiZKkhISElRQUNCsLSkpSQcOHPDg0AAAAAAAQHvUaljhcrmUlZWlSZMmSZIqKysVGhra7H5JamhocLeFhYWpoqKi2battTXlcDhkt9tlt9tVWlraBsMDAAAAAADtTauXgWzcuFF2u10BAdcyjfDwcFVXV7vvDwwMlCT3/ZJUVVWlyMhI97bh4eGqqqpSr1695HQ63Y9v3K6p1NRUpaamSpLsdnsbDA8AAAAAALQ3rZ5ZceLECb3yyitKSkrS6dOn9W//9m8qLi5WXV2d8vPz1b9/f0lSt27ddPbsWdXU1KiiokKdO3eW2WxWdna2JCk7O1tDhw5t1rZr1y5ZLBYPDw8AAAAAALQ3rZ5ZsWzZMvftwYMH64UXXtBrr72muLg4dejQQRs2bJAkLVmyRNOmTZPL5dLixYslSTNmzNBjjz2m9evXKyUlRVFRUYqKilLXrl1ltVrVs2dPzZkzx4NDAwAAAAAA7ZHJMAzD151oid1uV1ZWlq+78aX48tsh8PXCt4EAAAAAaM9u9Nn/pt8GAgAAAAAA4E2EFQAAAAAAwK+0umYFAP/my0uOuAQFAAAAgKdwZgUAAAAAAPArhBUAAAAAAMCvEFYAAAAAAAC/QlgBAAAAAAD8CgtsAvhSWNwTAAAAgKdwZgUAAAAAAPArhBUAAAAAAMCvEFYAAAAAAAC/wpoVANod1ssAAAAAbm+EFQDwBRCUAAAAAJ7X6mUgH330kcxms2w2m0aOHKkLFy4oLy9PZrNZsbGxKioqkiRdvHhRiYmJslgs2rhxoyTJ5XJp+vTpslqtmj17tvs5V61aJYvFonHjxqm6utqDQwMAAAAAAO1Rq2FF586dlZeXp3379mnKlClat26dFixYoB07dmjz5s1KT0+XJC1btkxz587Vvn37tHr1al25ckXbt29X9+7dlZubq9raWhUUFKi8vFzbtm1TXl6eJk2apNWrV3tlkAAAAAAAoP1oNawIDAxUQMC1TWpqanTvvfcqMDBQERER6tmzpyoqKiRJhw4d0siRIxUUFKTBgwfr+PHjys/PV2JioiQpKSlJBw4c0OHDh2Wz2WQymdxtAAAAAAAATd10zYqjR4/qxz/+sS5fvqzdu3frtdde+9uDg4JUV1en+vp6d6gRFhamiooKVVZWKjQ09KZtTTkcDjkcDklSaWlp24wQAAAAAAC0KzcNKx588EG98847ysrK0pIlS5qtM+F0OhUSEqLg4GA1NDQoICBAVVVVioyMVHh4uHvbpm1nzpxp1tZUamqqUlNTJUl2u73NBgkAAAAAANqPVi8Dqaurc98OCwvTXXfdJafTqcuXL6u0tNQdNsTExCgnJ0dOp1OFhYXq16+fzGazsrOzJUm7du2SxWJRTEyM9u/f36wNAAAAAACgqVbPrDh69KjmzJmjwMBAdejQQevXr9fp06eVnJwsk8mkNWvWSJLS09M1ZcoUZWRkaObMmerYsaNSUlK0detWWa1WDRgwQMOGDZMkjRkzRhaLRREREdq0aZPnRwgAtwm+NhUAAABfFybDMAxfd6IldrtdWVlZvu7Gl+LLDxQA4AmEFQAAAPCEG332v+maFQAAcFYHAAAAvKnVNSsAAAAAAAC8jbACAAAAAAD4FcIKAAAAAADgVwgrAAAAAACAX2GBTQCAX2NxTwAAgK8fzqwAAAAAAAB+hbACAAAAAAD4FcIKAAAAAADgVwgrAAAAAACAXyGsAAAAAAAAfoWwAgAAAAAA+BXCCgAAAAAA4FdaDSsOHTqkYcOGafjw4Xr00UdVX18vh8Mhs9ms+Ph4nTt3TpJ08uRJDR8+XGazWXv27JEk1dbWauLEiYqNjdXy5cvdz5meni6r1aq0tDTV19d7cGgAAAAAAKA9ajWs6NGjh/bu3av9+/erd+/eevPNN7Vy5Url5OToueeeU2ZmpiRp/vz5WrdunXbu3Klnn31WkrR27VolJycrLy9Pe/fuVVlZmY4dO6aysjLl5uYqOjpaW7Zs8fwIAQAAAABAuxLU2p3dunVz3w4JCdH777+vvn37KiQkRBaLRXPmzJEknT9/Xn369JEkRUZGqry8XPn5+VqxYoUkKSEhQQUFBbp06ZISExMlSUlJSfr973+vRx991CMDAwDgq+o9b4fPapcsHeOz2gAAAL7WaljR6MMPP9Tu3bu1dOlSXbp0yd3ucrkkSQ0NDe62sLAwVVRUqLKyUqGhode1NQYgjW1NORwOORwOSVJpaelXGBYAAO0bQQkAAPg6u2lYUV1drbS0NL388styuVyqrq523xcYGChJCgj429UkVVVVioyMVHh4uKqrqxUeHq6qqir16tVLTqfT/fjG7ZpKTU1VamqqJMlut3/10QEAgC+MoAQAAPhaq2tWOJ1O/eAHP9DChQv1ne98R3369FFxcbHq6uqUn5+v/v37S7p2ucjZs2dVU1OjiooKde7cWWazWdnZ2ZKk7OxsDR06tFnbrl27ZLFYPDw8AAAAAADQ3rR6ZsWrr76qd955R5mZmcrMzNQTTzyh2bNnKy4uTh06dNCGDRskSUuWLNG0adPkcrm0ePFiSdKMGTP02GOPaf369UpJSVFUVJSioqLUtWtXWa1W9ezZ073mBQAAAAAAQCOTYRiGrzvRErvdrqysLF9340vx5emzAAC0Z1wGAgDA18uNPvvf0gKbAAAA3sB6GQAAQCKsAAAAkERQAgCAP2l1gU0AAAAAAABv48wKAAAAH+OsDgAAmuPMCgAAAAAA4FcIKwAAAAAAgF8hrAAAAAAAAH6FsAIAAAAAAPgVFtgEAAD4GmNxTwCAPyKsAAAAgE/4MijxJUIaALg5LgMBAAAAAAB+hbACAAAAAAD4FS4DAQAAALyIdUIA4OZaDSuqqqqUkJCgEydO6ODBg7r//vvlcDj029/+Vh07dtSGDRsUFRWlkydP6kc/+pGcTqcyMzMVHx+v2tpapaWl6eOPP9a4ceM0d+5cSVJ6erry8/PVu3dvrV+/XsHBwV4ZKAAAAPB1R1ACoL1oNazo1KmTduzYoaefflqS5HQ6tXLlSu3bt0+HDx9WZmamXnzxRc2fP1/r1q1T165dNXr0aMXHx2vt2rVKTk7WjBkzlJSUpMmTJ6u8vFxlZWXKzc3VkiVLtGXLFj366KNeGSgAAAAA3yEoAfBFtBpWBAcHq0uXLu6fT58+rb59+yokJEQWi0Vz5syRJJ0/f159+vSRJEVGRqq8vFz5+flasWKFJCkhIUEFBQW6dOmSEhMTJUlJSUn6/e9/T1gBAAAAwKMISoD25wutWVFZWanQ0FD3zy6XS5LU0NDgbgsLC1NFRUWzbZu2devWrVlbUw6HQw6HQ5JUWlr6JYYDAAAAAADauy8UVoSHh6u6utr9c2BgoCQpIOBvXypSVVWlyMhI97bh4eGqqqpSr1695HQ63Y9v3K6p1NRUpaamSpLsdvuXGxEAAAAA+AlfntUB7+NMmrbzhcKKPn36qLi4WHV1dTpy5Ij69+8vSerWrZvOnj2rv/u7v1NFRYU6d+4ss9ms7OxsTZ8+XdnZ2frP//xPlZeXa+XKlZoyZYp27doli8XikUEBAAAAAOBtXHLUdm4aViQnJ+vo0aN6//339eMf/1izZ89WXFycOnTooA0bNkiSlixZomnTpsnlcmnx4sWSpBkzZuixxx7T+vXrlZKSoqioKEVFRalr166yWq3q2bOne80LAAAAAACARjcNK/74xz9e1zZp0qRmP993333Kzc1t1nbXXXdp69at1z22cdFNAAAAAACAlgTcfBMAAAAAAADvIawAAAAAAAB+hbACAAAAAAD4FcIKAAAAAADgVwgrAAAAAACAXyGsAAAAAAAAfoWwAgAAAAAA+BXCCgAAAAAA4FcIKwAAAAAAgF8hrAAAAAAAAH6FsAIAAAAAAPgVwgoAAAAAAOBXCCsAAAAAAIBf8UlYkZ6eLqvVqrS0NNXX1/uiCwAAAAAAwE95Paw4duyYysrKlJubq+joaG3ZssXbXQAAAAAAAH7M62FFfn6+EhMTJUlJSUk6cOCAt7sAAAAAAAD8WJC3C1ZWVqpbt26SpLCwMFVUVLjvczgccjgckqQjR47Ibrd7u3ttYogPa5eWlqpHjx7Upja1qU1talOb2tSmNrWpTe2vUe1hw573We2v4uzZsy3fYXjZ6tWrjQ0bNhiGYRhHjhwxZs2a5e0u3NZSU1OpTW1qU5va1KY2talNbWpTm9rUbte8fhmI2WxWdna2JGnXrl2yWCze7gIAAAAAAPBjgYsWLVrkzYJ333238vPzlZmZqbq6Oj3zzDMKDAz0Zhdue/369aM2talNbWpTm9rUpja1qU1talO73TIZhmH4uhMAAAAAAACNvH4ZCAAAAAAAQGsIK24Tx48f17Rp03zdja+NnJwczZkzx9fdkCQNHjzY113Abcqf5rk3lJSUyGQy6dChQ5Kk7du3y8tXSsKLvm7zG75VUlKi3bt3e72uP8zz1o5Rp02bpuPHj3u1PxcvXtTChQu9WrOkpETf//73vVqz0datW/Xxxx83a3v++ee1b9++Nq9VUlKiLl26KC4uTnFxcXrmmWfavMYXre/t4+ScnBwtWrRIcXFxXq17K44cOaJ3333X1934QggrAAD4f/fdd5+WL1/u6274pYaGBl93Abe523mO+SqswPXuvvtuLV682Nfd8JrPhxWffvqpYmJiZLPZPFLPZrMpJydHOTk5+tWvfuWRGv5c3xuuXLmimTNnymazyWKxyOFw3PQxDQ0NWrZsmfr27euFHrYdwop2zOl0ym636+GHH9Zvf/tbSdLOnTtltVplNpv16quveqzeP/3TP2natGnN0srG2+Xl5Ro/frxGjhypyZMny+VytWk/mvroo480YsQIWa1Wff/73/dorZb8y7/8i2w2m4YMGaKjR496vJ5hGPrpT3+qESNG6OGHH9a5c+c8XvNGdfv27aupU6fqwQcf1KZNmzxWOycnR0lJSZowYYK++93v6vjx4/rDH/6ghx56SEOHDtWuXbs8WrvxP1KN/xkaOHCgnnzyST300ENatmyZx2o3+vnPf678/HxJ0u7du7VgwQKP1/w8b73e0vVz7S9/+Yvi4uI0YsQIfe973/NobUnq27evnE6nTp065W7z5PhvdY796U9/ks1mU0xMjJYuXdpmtRMTEzV27FjFxMSoqKioxbHGxcVp7ty5GjVqVJvUbVp/1KhR7r/t1157TaNGjdKQIUP0ySef6Je//KVsNpuGDx+uoqKiNq3dVEtj/uEPfyir1aq4uDiVlJS0ec2DBw/qoYce0ogRI7Ro0SKP7rsbGYahWbNmyWq1asSIEcrNzVVsbKwsFov7gH7RokVKS0tTcnKybDabPvvss69Us6U51tJ+s+kc89R+/fOveUvHL570wgsv6LXXXlNcXJxWrlzp/n3v3bvX47Wllo9XPDnPvX2M+nmtvb8UFhZ6/CwHbx2f3myf+cEHH2jnzp364Q9/qLlz56qoqEijR4/WwoUL9eSTT3qkT01VVFR4dR/uD4YMGaInn3xSr7zyisdqZGZm6p577tG+ffu0e/durVixQidPnmz1MX/5y180f/58dezY0WP98ggffm0qviKHw2E888wzhmEYxgsvvGBMmTLFMJvNxtWrVw2n02mYzWbD6XR6rN7UqVONQYMGue9vvP3UU08Ze/bsMQzDMJYuXWo4HI4268PnXb161aivrzcMwzB+9rOfGbt37/ZYrabefvtt46mnnjJqa2sNwzCMd9991/jHf/xHj9d96623jF/84heGYRjGwYMHjVmzZjX7HXizbnh4uFFVVWVUVVUZQ4YM8Vjtt99+2xg5cqRhGIbxxz/+0Zg9e7bRv39/47PPPjOqqqo8Ov7G37NhGEZRUZExdepU41vf+pZRUlJiOJ1Oo1+/fh6r3aiwsNB44oknDMMwjClTphjFxcUer9no7bff9urrbRjXz7Xo6Gjj6aefNgzDMFwul0drf/DBB8Yjjzxi5ObmGjNmzDDeeustIyMjw6Pjv9U51vhe43K5jMGDBxuffvppm9S2WLHG5FQAAAlRSURBVCxGQ0ODceLECSMlJaXFsdpsNiM7O/sr12up/sMPP2wYhmG8+OKLxvjx4w3DMIzf/e53xqpVq4wpU6YYhmEYZWVlxrhx4zxSv6X5XVdXZwwbNsxoaGgwDMMz8y4jI8PYsWOHYRiGe3/tqX13ozfffNN48skn3T+npKQYJ06cMBoaGoyEhATjgw8+MBYuXGgsXrzYMAzDmDt3rvHmm29+pZqfn2Njx45tcb/ZdI55ar/e9DV3uVwtHr94UuPfenl5uTFq1CijoaHB+Otf/2rYbDav1P386+7peX6rx6hTp041ioqK2rS2YbT+/rJu3TrjkUceafOaTX1+Hr/00kseqXkr+8ymr3Ftba37dz5mzBjj1KlTbdqfDz74wOjcubNhs9kMm81m/O53v/PaPvxG9b3x9+0tP/rRjwzDMIzvfOc7xpUrV9zta9euNRYvXuw+jjEMw6ipqXG/vxw+fNiIi4szYmNjjRUrVni9319FkK/DEnx5Z86c0aBBgyRJMTEx2rZtm06dOqXExERJ0uXLl3Xp0iXdfffdHql38ODBZvcb///FMidOnNA777yj5557Tp999pnS0tLapH5LPvnkEz3xxBOqrKzU+fPnNXDgQI/VasmKFSuUnZ0tSQoK8vyf04kTJ/TGG29o//79MgxDPXr08HjNG9W95557FBoaKkkeP6PlwQcflCT16NFDly9fVs+ePdWhQwd16NBBwcHBcjqdHnn9TSaT+3bj/I6IiFCvXr0kSR06dGjzmp83cOBAnThxQlVVVSotLVV0dLTHazZVU1Pjtddbun6uDRo0SHfeeacmT56sAQMGeOXa69jYWD377LO6cOGCLl265NHx3+ocKyws1OLFi1VfX6+SkhJ9/PHH7m2+igEDBshkMqlv3746efKkoqOjrxurdO093xP69+8vSerevbv79t///d/r7Nmzys/Pd1/z66mvOG9pfptMJs2aNUtpaWn65je/qSVLluiuu+5q07qzZs3S888/r02bNmnUqFEe3Xc3Ki4ubnba98WLF92nAw8cOFBnz56VdG1OSNfebysrK79y3aZz7MKFCzfcbzbOMU/t15u+5pMnT252n+HFL8Y7e/as/ud//kcjRoyQJF26dMkrdT//ugcHB3t0nt/qMaon3ej95cMPP/RoXen6eRwZGemROl90n3nhwgVlZmbKMAyVlJTo/Pnz6tOnT5v2yWazacuWLZKunWGzZMkSr+7Dm9aXpP/6r//yeE1vaTwLrK6uTnfccYe7PSoqSocPH77h4+bNm6fXX39dERERGjt2rNLS0tS1a1eP97ctEFa0Y//wD/+gP//5z3rkkUd05MgRde7cWdHR0dq9e7dCQkJUX1+v4OBgj9WTrh1A1tTUSJL+93//V5IUHR2tCRMmyGq1SpLq6+vbrA+ft3nzZqWkpGjGjBn66U9/6tUDjk8++UQHDx5UXl6eCgsL9dRTT3m8ZnR0tOx2u37xi19IuvbaDhs27Lat26jpBzqn06kPP/xQV65cUV1dnerq6jz2wTkiIsJ9qc2xY8eu64u3pKSkaObMmT45hfIb3/iGDh065JXXW7p+rtXU1Ogb3/iGJCkxMVF2u109e/b0WP1Gs2fP1oIFCzR+/Hjl5+d7bPy3OseWL1+u//iP/9A999yjgQMHttl73dGjR2UYhk6dOqXo6Ogb/m0FBHjmqtGmY216++rVq7LZbFq7dq0kz+1HWprfJpNJdrtdkydP1i9/+Uu9/vrrmjJlSpvWDQsL07//+7+rrq5OgwYN8ui+u1Hfvn2VnZ3tPv29S5cuKi4uVnR0tN59913NnDlTubm5LQZoX0XTOXb33XfrT3/6U4v7zcY55qn9+udf806dOl13/OJJwcHBcrlcuueee9S/f39t375dJpPJo8dIjVo6XnG5XB6d594+Rm3Jjd5fvHGs+Pl53KtXL49cznYr+8zGuSdJq1at0tSpUzVixAglJyd7/LWor693L2bqzX347cgwDL3yyit6/PHHFRISoqtXr7oDi3Pnzql79+43nOfvvfeeJkyYIEmqrKxUaWkpYQU8b/z48frDH/6g+Ph4ffvb31ZAQIAyMjKUkJCggIAAdenSRVlZWR6rJ8l9/euQIUPUvXt3SdKCBQv0+OOPu9+cli9f7rHrQePj45WWlqa33nrL69dgRUREKDIyUnFxcRo6dKhXao4dO1Z79+7ViBEjZDKZrvvv0O1WtyWBgYGaN2+ehg8froCAAD3//PMeq/XAAw/o008/VUJCgu6//36P1bmZyZMnKyMjQ6tWrfJ6bW++3tL1c81msyk7O1sBAQGKiopSVFSUR+s37ce8efM8Pv5bnWOPPPKIJkyYoAceeMB9INoWwsLCNHbsWH300Udat26djh8/7rXfdWs6deqkPn36yGazKSAgQAkJCZo/f36b12np91tTU6Pvfe97MplMMplMHlmT58UXX9Trr78up9OpadOm6f777/fYvrvR2LFjtXPnTsXGxio4OFiLFi3SjBkzZBiGxowZo969e7d5Ten6OZaRkdHqftNT+/XPv+ZdunS57vjFkx544AE988wzeuKJJ/SDH/xANptNgYGBeuCBB/Sv//qvHq3d0vGKp+e5t49R/Y23jk9vZZ85evRozZ49Ww8//LBGjx6tn/zkJ+rbt6/HQuh9+/a5z4oLCAhQfX29V/fhTevfd999Hq/nLStWrNDIkSMlSRMnTtSqVas0d+5c1dbWau3atdqwYYPCw8NVVlYm6W//AJGk7373u9qyZYvCwsLkcrk89rv3BJPhzX9F47Zx/Phx/frXv9bLL7/s664At72LFy9q5syZ2rp1q6+7gttITk6Otm/frl//+te+7gpuU8wxAPjqTpw4odmzZ2vnzp0KCAjQZ599pp///OcqLi7WqVOn9MILL2jixImSpJ/85CcqKiqSzWZTXl6ecnJyVFhYqLlz56qhoUF33HGH3njjjXaz0CZnVgCAHztw4ICefvppDvYBAAC+hoqLixUbG6uAgAB9+OGH6tWrl1566SVJ0ksvvaRt27a5w4o1a9Zc9/hBgwZpz549Xu1zW+HMCgAAAAAA/NBf//pXTZgwQSaTST179nSv5/R1QFgBAAAAAAD8SvtZXQMAAAAAAHwtEFYAAAAAAAC/QlgBAAAAAAD8CmEFAAAAAADwK/8Hcc0DBrnBrr8AAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot_hist</span><span class="p">(</span><span class="n">lens</span><span class="p">,</span> <span class="n">n_bins</span> <span class="o">=</span> <span class="mi">50</span><span class="p">):</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">lens</span><span class="p">,</span> <span class="n">n_bins</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Mean: </span><span class="si">{</span><span class="n">mean</span><span class="p">(</span><span class="n">lens</span><span class="p">)</span><span class="si">}</span><span class="s1">, Median: </span><span class="si">{</span><span class="n">median</span><span class="p">(</span><span class="n">lens</span><span class="p">)</span><span class="si">}</span><span class="s1">, Standard Deviation: </span><span class="si">{</span><span class="n">stdev</span><span class="p">(</span><span class="n">lens</span><span class="p">)</span><span class="si">}</span><span class="s1">, 90th Percentile: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">lens</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="n">plot_hist</span><span class="p">(</span><span class="n">lens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Mean: 150.01203744984397, Median: 141.0, Standard Deviation: 44.59412209701778, 90th Percentile: 513.0
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's get our data into a format that we can feed into our model using Pytorch's Dataset and Dataloader API. All these methods do are convert our dataframes where we have multiple historical dialog, i.e., context, and a response, into a single conversation string that is separated a special token that tells our model when a person is finished speaking.</p>
<p>These conversation strings are then tokenized using HuggingFace's awesome tokenizers into their numerical representation that our model actual understands!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">construct_conv</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">eos</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="c1"># from: https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists</span>
    <span class="n">flatten</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">l</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">row</span><span class="p">]))</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conv</span>

<span class="k">class</span> <span class="nc">ConversationDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>

        <span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span> <span class="o">-</span> <span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">max_len</span> <span class="o">-</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">max_len_single_sentence</span><span class="p">)</span>

        <span class="n">directory</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cache_dir</span>
        <span class="n">cached_features_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">directory</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model_type</span> <span class="o">+</span> <span class="s2">"_cached_lm_"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">cached_features_file</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">overwrite_cache</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Loading features from cached file </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">cached_features_file</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">cached_features_file</span><span class="p">,</span> <span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">handle</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">examples</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Creating features from dataset file at </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">directory</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">examples</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
                <span class="n">conv</span> <span class="o">=</span> <span class="n">construct_conv</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">block_size</span><span class="p">:</span> <span class="k">continue</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">examples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span>

            <span class="c1"># Note that we are loosing the last truncated example here for the sake of simplicity (no padding)</span>
            <span class="c1"># If your dataset is small, first you should loook for a bigger one :-) and second you</span>
            <span class="c1"># can change this behavior by adding (model specific) padding.</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Saving features into cached file </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">cached_features_file</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">cached_features_file</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">handle</span><span class="p">:</span>
                <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">examples</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle</span><span class="o">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">examples</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">examples</span><span class="p">[</span><span class="n">item</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Training-and-Evaluating">
<a class="anchor" href="#Training-and-Evaluating" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training and Evaluating<a class="anchor-link" href="#Training-and-Evaluating"> </a>
</h1>
<p>Now that we have THE DATA we can finally create our model and start training it! The training and evaluation loop are quite simple. We simplely take a batch of examples from our dataloader and use it both as our inputs and labels. We do this because GPT2 is an auto-regressive model, meaning it uses some context to predict the next token. This prediction is then added to the original context and fed back in as the new context for generating the next token.</p>
<p>To evaluate our model, we use the metric perplexity, which is a simple, but powerful metric. Perplexity is a measure of how unsure the model is in its choice of the next token. The more unsure our model is, the higher its perplexity. One fascinating thing about perplexity is that it correlates very well with what humans think of when it comes to coherent and specific natural conversations, which was shown in the amazing paper <a href="https://arxiv.org/abs/2001.09977">"Towards a Human-like Open-Domain Chatbot"</a> by Daniel Adiwardana, et. al.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Training of model</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">PreTrainedModel</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
    <span class="sd">""" Train the model """</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]:</span>
        <span class="n">tb_writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">()</span>

    <span class="n">args</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">per_gpu_train_batch_size</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">collate</span><span class="p">(</span><span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">_pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">train_dataset</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate</span><span class="p">,</span> <span class="n">drop_last</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">t_total</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span>
        <span class="n">args</span><span class="o">.</span><span class="n">num_train_epochs</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">//</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">//</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">t_total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">//</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">num_train_epochs</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">module</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">"module"</span><span class="p">)</span> <span class="k">else</span> <span class="n">model</span>  <span class="c1"># Take care of distributed/parallel training</span>
    <span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
    <span class="c1"># add_special_tokens_(model, tokenizer)</span>


    <span class="c1"># Prepare optimizer and schedule (linear warmup and decay)</span>
    <span class="n">no_decay</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"bias"</span><span class="p">,</span> <span class="s2">"LayerNorm.weight"</span><span class="p">]</span>
    <span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">"params"</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">nd</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">nd</span> <span class="ow">in</span> <span class="n">no_decay</span><span class="p">)],</span>
            <span class="s2">"weight_decay"</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="p">{</span><span class="s2">"params"</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">nd</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">nd</span> <span class="ow">in</span> <span class="n">no_decay</span><span class="p">)],</span> <span class="s2">"weight_decay"</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">},</span>
    <span class="p">]</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">optimizer_grouped_parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">adam_epsilon</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=</span><span class="n">t_total</span>
    <span class="p">)</span>

    <span class="c1"># Check if saved optimizer or scheduler states exist</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span>
        <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="s2">"optimizer.pt"</span><span class="p">))</span>
        <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="s2">"scheduler.pt"</span><span class="p">))</span>
    <span class="p">):</span>
        <span class="c1"># Load in optimizer and scheduler states</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="s2">"optimizer.pt"</span><span class="p">)))</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="s2">"scheduler.pt"</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">apex</span> <span class="kn">import</span> <span class="n">amp</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">"Please install apex from https://www.github.com/nvidia/apex to use fp16 training."</span><span class="p">)</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">amp</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">opt_level</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">fp16_opt_level</span><span class="p">)</span>

    <span class="c1"># multi-gpu training (should be after apex fp16 initialization)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># Distributed training (should be after apex fp16 initialization)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">,</span> <span class="n">find_unused_parameters</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="c1"># Train!</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"***** Running training *****"</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Num examples = </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Num Epochs = </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">num_train_epochs</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Instantaneous batch size per GPU = </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">per_gpu_train_batch_size</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="s2">"  Total train batch size (w. parallel, distributed &amp; accumulation) = </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span>
        <span class="n">args</span><span class="o">.</span><span class="n">train_batch_size</span>
        <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>
        <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Gradient Accumulation steps = </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Total optimization steps = </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="n">t_total</span><span class="p">)</span>

    <span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">epochs_trained</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">steps_trained_in_current_epoch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Check if continuing training from a checkpoint</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># set global_step to gobal_step of last saved checkpoint from model path</span>
            <span class="n">checkpoint_suffix</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"-"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"/"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">global_step</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">checkpoint_suffix</span><span class="p">)</span>
            <span class="n">epochs_trained</span> <span class="o">=</span> <span class="n">global_step</span> <span class="o">//</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">//</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span><span class="p">)</span>
            <span class="n">steps_trained_in_current_epoch</span> <span class="o">=</span> <span class="n">global_step</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">//</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span><span class="p">)</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Continuing training from checkpoint, will skip to saved global_step"</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Continuing training from epoch </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="n">epochs_trained</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Continuing training from global step </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Will skip the first </span><span class="si">%d</span><span class="s2"> steps in the first epoch"</span><span class="p">,</span> <span class="n">steps_trained_in_current_epoch</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Starting fine-tuning."</span><span class="p">)</span>

    <span class="n">tr_loss</span><span class="p">,</span> <span class="n">logging_loss</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>

    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">train_iterator</span> <span class="o">=</span> <span class="n">trange</span><span class="p">(</span>
        <span class="n">epochs_trained</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_train_epochs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Epoch"</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">set_seed</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># Added here for reproducibility</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">train_iterator</span><span class="p">:</span>
        <span class="n">epoch_iterator</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Iteration"</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">epoch_iterator</span><span class="p">):</span>

            <span class="c1"># Skip past any already trained steps if resuming training</span>
            <span class="k">if</span> <span class="n">steps_trained_in_current_epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">steps_trained_in_current_epoch</span> <span class="o">-=</span> <span class="mi">1</span>
                <span class="k">continue</span>

            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1024</span><span class="p">:</span> <span class="k">continue</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># model outputs are always tuple in transformers (see doc)</span>

            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># mean() to average on multi-gpu parallel training</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>

            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">amp</span><span class="o">.</span><span class="n">scale_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span> <span class="k">as</span> <span class="n">scaled_loss</span><span class="p">:</span>
                    <span class="n">scaled_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="n">tr_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">amp</span><span class="o">.</span><span class="n">master_params</span><span class="p">(</span><span class="n">optimizer</span><span class="p">),</span> <span class="n">args</span><span class="o">.</span><span class="n">max_grad_norm</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">args</span><span class="o">.</span><span class="n">max_grad_norm</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update learning rate schedule</span>
                <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">global_step</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># Log metrics</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">evaluate_during_training</span>
                    <span class="p">):</span>  <span class="c1"># Only evaluate when single GPU otherwise metrics may not average well</span>
                        <span class="n">results</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                            <span class="n">tb_writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">"eval_</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="n">value</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
                    <span class="n">tb_writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">"lr"</span><span class="p">,</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">global_step</span><span class="p">)</span>
                    <span class="n">tb_writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">"loss"</span><span class="p">,</span> <span class="p">(</span><span class="n">tr_loss</span> <span class="o">-</span> <span class="n">logging_loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
                    <span class="n">logging_loss</span> <span class="o">=</span> <span class="n">tr_loss</span>

                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">save_steps</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">global_step</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">save_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">checkpoint_prefix</span> <span class="o">=</span> <span class="s2">"checkpoint"</span>
                    <span class="c1"># Save model checkpoint</span>
                    <span class="n">output_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"</span><span class="si">{}</span><span class="s2">-</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">checkpoint_prefix</span><span class="p">,</span> <span class="n">global_step</span><span class="p">))</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="n">model_to_save</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">model</span><span class="o">.</span><span class="n">module</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">"module"</span><span class="p">)</span> <span class="k">else</span> <span class="n">model</span>
                    <span class="p">)</span>  <span class="c1"># Take care of distributed/parallel training</span>
                    <span class="n">model_to_save</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
                    <span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>

                    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"training_args.bin"</span><span class="p">))</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Saving model checkpoint to </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">)</span>

                    <span class="n">_rotate_checkpoints</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">checkpoint_prefix</span><span class="p">)</span>

                    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"optimizer.pt"</span><span class="p">))</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"scheduler.pt"</span><span class="p">))</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Saving optimizer and scheduler states to </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">global_step</span> <span class="o">&gt;</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span><span class="p">:</span>
                <span class="n">epoch_iterator</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
                <span class="k">break</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">global_step</span> <span class="o">&gt;</span> <span class="n">args</span><span class="o">.</span><span class="n">max_steps</span><span class="p">:</span>
            <span class="n">train_iterator</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
            <span class="k">break</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]:</span>
        <span class="n">tb_writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">global_step</span><span class="p">,</span> <span class="n">tr_loss</span> <span class="o">/</span> <span class="n">global_step</span>

<span class="c1"># Evaluation of some model</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">PreTrainedModel</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">df_trn</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">""</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="c1"># Loop to handle MNLI double evaluation (matched, mis-matched)</span>
    <span class="n">eval_output_dir</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">output_dir</span>

    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">load_and_cache_examples</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">df_trn</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">evaluate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">eval_output_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">per_gpu_eval_batch_size</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span><span class="p">)</span>
    <span class="c1"># Note that DistributedSampler samples randomly</span>

    <span class="k">def</span> <span class="nf">collate</span><span class="p">(</span><span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">_pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>

    <span class="n">eval_sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>
    <span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">eval_dataset</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">eval_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate</span><span class="p">,</span> <span class="n">drop_last</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># multi-gpu evaluate</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># Eval!</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"***** Running evaluation </span><span class="si">{}</span><span class="s2"> *****"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prefix</span><span class="p">))</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Num examples = </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">))</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  Batch size = </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">)</span>
    <span class="n">eval_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">nb_eval_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Evaluating"</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
            <span class="n">lm_loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">eval_loss</span> <span class="o">+=</span> <span class="n">lm_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">nb_eval_steps</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">eval_loss</span> <span class="o">=</span> <span class="n">eval_loss</span> <span class="o">/</span> <span class="n">nb_eval_steps</span>
    <span class="n">perplexity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">eval_loss</span><span class="p">))</span>

    <span class="n">result</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"perplexity"</span><span class="p">:</span> <span class="n">perplexity</span><span class="p">}</span>

    <span class="n">output_eval_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eval_output_dir</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="s2">"eval_results.txt"</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_eval_file</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"***** Eval results </span><span class="si">{}</span><span class="s2"> *****"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prefix</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"  </span><span class="si">%s</span><span class="s2"> = </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="n">key</span><span class="p">]))</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">"</span><span class="si">%s</span><span class="s2"> = </span><span class="si">%s</span><span class="se">\n</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="n">key</span><span class="p">])))</span>

    <span class="k">return</span> <span class="n">result</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's put it all together into our runner function and let our baby cook away!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Main show runner</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">df_trn</span><span class="p">,</span> <span class="n">df_val</span><span class="p">):</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">Args</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">should_continue</span><span class="p">:</span>
        <span class="n">sorted_checkpoints</span> <span class="o">=</span> <span class="n">_sorted_checkpoints</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sorted_checkpoints</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Used --should_continue but no checkpoint was found in --output_dir."</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span> <span class="o">=</span> <span class="n">sorted_checkpoints</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
        <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
        <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">do_train</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">overwrite_output_dir</span>
        <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">should_continue</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">"Output directory (</span><span class="si">{}</span><span class="s2">) already exists and is not empty. Use --overwrite_output_dir to overcome."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">args</span><span class="o">.</span><span class="n">output_dir</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Setup CUDA, GPU &amp; distributed training</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
    <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">args</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

    <span class="c1"># Setup logging</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
        <span class="nb">format</span><span class="o">=</span><span class="s2">"</span><span class="si">%(asctime)s</span><span class="s2"> - </span><span class="si">%(levelname)s</span><span class="s2"> - </span><span class="si">%(name)s</span><span class="s2"> -   </span><span class="si">%(message)s</span><span class="s2">"</span><span class="p">,</span>
        <span class="n">datefmt</span><span class="o">=</span><span class="s2">"%m/</span><span class="si">%d</span><span class="s2">/%Y %H:%M:%S"</span><span class="p">,</span>
        <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">else</span> <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
        <span class="s2">"Process rank: </span><span class="si">%s</span><span class="s2">, device: </span><span class="si">%s</span><span class="s2">, n_gpu: </span><span class="si">%s</span><span class="s2">, distributed training: </span><span class="si">%s</span><span class="s2">, 16-bits training: </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span>
        <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">,</span>
        <span class="n">device</span><span class="p">,</span>
        <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span><span class="p">,</span>
        <span class="nb">bool</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">args</span><span class="o">.</span><span class="n">fp16</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Set seed</span>
    <span class="n">set_seed</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config_name</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">cache_dir</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">tokenizer_name</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">cache_dir</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="p">,</span>
        <span class="n">from_tf</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">cache_dir</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Training/evaluation parameters </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

    <span class="c1"># Training</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">do_train</span><span class="p">:</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">load_and_cache_examples</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">df_trn</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">evaluate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">global_step</span><span class="p">,</span> <span class="n">tr_loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">" global_step = </span><span class="si">%s</span><span class="s2">, average loss = </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span> <span class="n">tr_loss</span><span class="p">)</span>

    <span class="c1"># Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">do_train</span><span class="p">:</span>
        <span class="c1"># Create output directory if needed</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Saving model checkpoint to </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
        <span class="c1"># Save a trained model, configuration and tokenizer using `save_pretrained()`.</span>
        <span class="c1"># They can then be reloaded using `from_pretrained()`</span>
        <span class="n">model_to_save</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">module</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">"module"</span><span class="p">)</span> <span class="k">else</span> <span class="n">model</span>
        <span class="p">)</span>  <span class="c1"># Take care of distributed/parallel training</span>
        <span class="n">model_to_save</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>

        <span class="c1"># Good practice: save your training arguments together with the trained model</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"training_args.bin"</span><span class="p">))</span>

        <span class="c1"># Load a trained model and vocabulary that you have fine-tuned</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Evaluation</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">do_eval</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]:</span>
        <span class="n">checkpoints</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_all_checkpoints</span><span class="p">:</span>
            <span class="n">checkpoints</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">+</span> <span class="s2">"/**/"</span> <span class="o">+</span> <span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
            <span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">"transformers.modeling_utils"</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">)</span>  <span class="c1"># Reduce logging</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Evaluate the following checkpoints: </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">checkpoints</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">checkpoint</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">:</span>
            <span class="n">global_step</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"-"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">""</span>
            <span class="n">prefix</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"/"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s2">"checkpoint"</span><span class="p">)</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="s2">""</span>

            <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">df_trn</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">k</span> <span class="o">+</span> <span class="s2">"_</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">global_step</span><span class="p">),</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="n">results</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">results</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> tensorboard
<span class="o">%</span><span class="k">tensorboard</span> --logdir runs
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="4657db90-be74-497f-8391-adc3c94d96fa"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#4657db90-be74-497f-8391-adc3c94d96fa');

        (async () => {
            const url = await google.colab.kernel.proxyPort(6006, {"cache": true});
            const iframe = document.createElement('iframe');
            iframe.src = url;
            iframe.setAttribute('width', '100%');
            iframe.setAttribute('height', '800');
            iframe.setAttribute('frameborder', 0);
            document.body.appendChild(iframe);
        })();
    
</script>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we run our model! I found this can take anywhere from an hour to three hours depending on the GPU Google give to you to finish training a model that can sort of hold a coherent conversation for the Spanish language. If you are using a different language, you'll have to play around with how long to cook your model for.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">main</span><span class="p">(</span><span class="n">trn_df</span><span class="p">,</span> <span class="n">val_df</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Chatting-with-our-Model">
<a class="anchor" href="#Chatting-with-our-Model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Chatting with our Model<a class="anchor-link" href="#Chatting-with-our-Model"> </a>
</h1>
<p>Now that we have our model trained, let's it out for a spin and have our first conversation with it!</p>
<p>In order to allow us to chitchat with our new bot we need to figure out when the model has finished its turn, i.e. when it has generated the [end_of_turn] token. When the model generates this token, we can switch back control of the conversation to the user so they can respond. Luckily, this is very easy to do with the Huggingface framework!</p>
<p>The below code is copied pretty much verbatim from the creators of the DialoGPT model, which you can find <a href="https://huggingface.co/microsoft/DialoGPT-small">here</a>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'microsoft/DialoGPT-small'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'output'</span><span class="p">)</span>

<span class="c1"># Let's chat for 5 lines</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="c1"># encode the new user input, add the eos_token and return a tensor in Pytorch</span>
    <span class="n">new_user_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="nb">input</span><span class="p">(</span><span class="s2">"&gt;&gt; User:"</span><span class="p">)</span> <span class="o">+</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)</span>
    <span class="c1"># print(new_user_input_ids)</span>

    <span class="c1"># append the new user input tokens to the chat history</span>
    <span class="n">bot_input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">chat_history_ids</span><span class="p">,</span> <span class="n">new_user_input_ids</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">new_user_input_ids</span>

    <span class="c1"># generated a response while limiting the total chat history to 1000 tokens, </span>
    <span class="n">chat_history_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">bot_input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.92</span><span class="p">,</span> <span class="n">top_k</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="p">)</span>
    
    <span class="c1"># pretty print last ouput tokens from bot</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"DialoGPT: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">chat_history_ids</span><span class="p">[:,</span> <span class="n">bot_input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:][</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>05/13/2020 00:27:10 - INFO - filelock -   Lock 139706162979168 acquired on /root/.cache/torch/transformers/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5.lock
05/13/2020 00:27:10 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpkhif9g52
05/13/2020 00:27:10 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/config.json in cache at /root/.cache/torch/transformers/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5
05/13/2020 00:27:10 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5
05/13/2020 00:27:10 - INFO - filelock -   Lock 139706162979168 released on /root/.cache/torch/transformers/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5.lock
05/13/2020 00:27:10 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/config.json from cache at /root/.cache/torch/transformers/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5
05/13/2020 00:27:10 - INFO - transformers.configuration_utils -   Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50257
}

05/13/2020 00:27:10 - INFO - transformers.tokenization_utils -   Model name 'microsoft/DialoGPT-small' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'microsoft/DialoGPT-small' is a path, a model identifier, or url to a directory containing tokenizer files.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>05/13/2020 00:27:11 - INFO - filelock -   Lock 139706164883072 acquired on /root/.cache/torch/transformers/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock
05/13/2020 00:27:11 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpaeb7ikva
05/13/2020 00:27:12 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/vocab.json in cache at /root/.cache/torch/transformers/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
05/13/2020 00:27:12 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
05/13/2020 00:27:12 - INFO - filelock -   Lock 139706164883072 released on /root/.cache/torch/transformers/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>05/13/2020 00:27:12 - INFO - filelock -   Lock 139706162979168 acquired on /root/.cache/torch/transformers/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock
05/13/2020 00:27:12 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp4k0b0lt0
05/13/2020 00:27:13 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/merges.txt in cache at /root/.cache/torch/transformers/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
05/13/2020 00:27:13 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
05/13/2020 00:27:13 - INFO - filelock -   Lock 139706162979168 released on /root/.cache/torch/transformers/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>05/13/2020 00:27:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/vocab.json from cache at /root/.cache/torch/transformers/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
05/13/2020 00:27:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/merges.txt from cache at /root/.cache/torch/transformers/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
05/13/2020 00:27:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/added_tokens.json from cache at None
05/13/2020 00:27:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/special_tokens_map.json from cache at None
05/13/2020 00:27:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/tokenizer_config.json from cache at None
05/13/2020 00:27:14 - INFO - transformers.configuration_utils -   loading configuration file output/config.json
05/13/2020 00:27:14 - INFO - transformers.configuration_utils -   Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50257
}

05/13/2020 00:27:14 - INFO - transformers.modeling_utils -   loading weights file output/pytorch_model.bin
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&gt;&gt; User:Hola.
DialoGPT: ¿Qué estás haciendo?
&gt;&gt; User:Estoy leyendo un libro nuevo.
DialoGPT: ¿Qué leyendo?
&gt;&gt; User:Se llama "The Witcher."
DialoGPT: ¿Qué quieres decir?
&gt;&gt; User:Es un libro sobre magia y monstruos.
DialoGPT: ¿Qué quieres decir?
&gt;&gt; User:¿Te gusta libros?
DialoGPT: ¿Qué te pasa?
&gt;&gt; User:Nada mucho.
DialoGPT: ¿Por qué no me lo dijiste?
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, it ain't the best, however, training it for longer or using the DialogGPT-medium instead of DialogGPT-small does improve results, at least in my experiments. I decided to only include the DialogGPT-small in this tutorial due to the limited (But still AMAZING) resources of Google Colab. I've went ahead and trained a bigger DialogGPT-medium model for longer and have uploaded it to Huggingface for anyone to try out! Find the <a href="https://huggingface.co/ncoop57/DiGPTame-medium">model card here</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h1>
<p>In this tutorial, you learned how to train an Open-Dialog chatbot in any language we want to practice with! This involved learning about the amazing <code>transformers</code> library by Huggingface that has seen a lot of popularity recently. You've also learned what an Open-Dialog chatbot is and some of the difficulties that come with training them such as constructing training examples and generating repetitive text.</p>
<p>This is just part one in what I am hoping will be a three part series! In the next part, we will take our model and integrate it into a web app using the awesome <a href="https://www.streamlit.io/">Streamlit</a> library. Finally, part three will then be generating an Android application for chatting with your new language companion!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="PS">
<a class="anchor" href="#PS" aria-hidden="true"><span class="octicon octicon-link"></span></a>PS<a class="anchor-link" href="#PS"> </a>
</h1>
<p>If you do train a new chatbot for a language of your interest, please share it! I'd love to hear about your progress with it and I'm sure others would be also interested in it as these models can be quite expensive to train.</p>
<p>If you want an ease way to share it, I suggest submitting your trained model to Huggingface's model zoo, where others can view and download your model to use as a starting point for their applications! Here is a simple way for taking the model trained in this tutorial and uploading it to Hugginface's website following the instructions on the Huggingface <a href="https://huggingface.co/transformers/model_sharing.html">website</a>:</p>
<p>First make sure you have a Huggingface account: <a href="https://huggingface.co/join">https://huggingface.co/join</a>. Next Run the following code snippets and that's it!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> rm -rf output/checkpoint-*
<span class="o">!</span> mv output &lt;name_of_model&gt;
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> transformers-cli login
<span class="c1"># log in using the same credentials as on huggingface.co</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> transformers-cli upload &lt;name_of_model&gt;
</pre></div>

    </div>
</div>
</div>

</div>
    

</div>

<script type="application/vnd.jupyter.widget-state+json">
{"5a3aefdbfa4140aeaa8eacef6e05d527": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_view_name": "HBoxView", "_dom_classes": [], "_model_name": "HBoxModel", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.5.0", "box_style": "", "layout": "IPY_MODEL_42ab64db40464775abbd563d2d398130", "_model_module": "@jupyter-widgets/controls", "children": ["IPY_MODEL_ad54c5a7b2e04c4d8c0e74563bf68445", "IPY_MODEL_5b37bf4feaae4f0a8b259bd1e23df457"]}}, "42ab64db40464775abbd563d2d398130": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "ad54c5a7b2e04c4d8c0e74563bf68445": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "state": {"_view_name": "ProgressView", "style": "IPY_MODEL_21147020f28c4941896a983e7d3dee58", "_dom_classes": [], "description": "Downloading: 100%", "_model_name": "FloatProgressModel", "bar_style": "success", "max": 554, "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": 554, "_view_count": null, "_view_module_version": "1.5.0", "orientation": "horizontal", "min": 0, "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_78bcb3bb3aa8439aaebbd68e0d5f2d8a"}}, "5b37bf4feaae4f0a8b259bd1e23df457": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_view_name": "HTMLView", "style": "IPY_MODEL_99dbd6be5282455abe545fec10ad8dfa", "_dom_classes": [], "description": "", "_model_name": "HTMLModel", "placeholder": "\u200b", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": " 554/554 [00:02&lt;00:00, 259B/s]", "_view_count": null, "_view_module_version": "1.5.0", "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_5ffee55ffa394080a07609226086b803"}}, "21147020f28c4941896a983e7d3dee58": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_view_name": "StyleView", "_model_name": "ProgressStyleModel", "description_width": "initial", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "bar_color": null, "_model_module": "@jupyter-widgets/controls"}}, "78bcb3bb3aa8439aaebbd68e0d5f2d8a": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "99dbd6be5282455abe545fec10ad8dfa": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_view_name": "StyleView", "_model_name": "DescriptionStyleModel", "description_width": "", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "_model_module": "@jupyter-widgets/controls"}}, "5ffee55ffa394080a07609226086b803": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "aded096bbf744fb0a1b05880b3522cae": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_view_name": "HBoxView", "_dom_classes": [], "_model_name": "HBoxModel", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.5.0", "box_style": "", "layout": "IPY_MODEL_71986f23b3c348f395365d0da817c90a", "_model_module": "@jupyter-widgets/controls", "children": ["IPY_MODEL_d8bd8ad1181344c49ccd46db1c7ae3ad", "IPY_MODEL_4f90af41548e4eda94999d1241b52414"]}}, "71986f23b3c348f395365d0da817c90a": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "d8bd8ad1181344c49ccd46db1c7ae3ad": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "state": {"_view_name": "ProgressView", "style": "IPY_MODEL_a6cc8cfe61d84d0a9e4904869371b753", "_dom_classes": [], "description": "Downloading: 100%", "_model_name": "FloatProgressModel", "bar_style": "success", "max": 1042301, "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": 1042301, "_view_count": null, "_view_module_version": "1.5.0", "orientation": "horizontal", "min": 0, "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_10805dbd211940c2ba320e17c1df1555"}}, "4f90af41548e4eda94999d1241b52414": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_view_name": "HTMLView", "style": "IPY_MODEL_231b27853c0547f3bc6b5600ee2fde9b", "_dom_classes": [], "description": "", "_model_name": "HTMLModel", "placeholder": "\u200b", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": " 1.04M/1.04M [00:01&lt;00:00, 851kB/s]", "_view_count": null, "_view_module_version": "1.5.0", "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_6588e145be214b74a3daba8c33d668b0"}}, "a6cc8cfe61d84d0a9e4904869371b753": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_view_name": "StyleView", "_model_name": "ProgressStyleModel", "description_width": "initial", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "bar_color": null, "_model_module": "@jupyter-widgets/controls"}}, "10805dbd211940c2ba320e17c1df1555": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "231b27853c0547f3bc6b5600ee2fde9b": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_view_name": "StyleView", "_model_name": "DescriptionStyleModel", "description_width": "", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "_model_module": "@jupyter-widgets/controls"}}, "6588e145be214b74a3daba8c33d668b0": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "bd9a26c919cd43e68af659f08c840ed7": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_view_name": "HBoxView", "_dom_classes": [], "_model_name": "HBoxModel", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.5.0", "box_style": "", "layout": "IPY_MODEL_d776fe4027a24ff2b7a05a4bf93725f2", "_model_module": "@jupyter-widgets/controls", "children": ["IPY_MODEL_2ce62470d0b845bb877bd8e284af1783", "IPY_MODEL_4db46f7fa885490db935bb3ed0533e8f"]}}, "d776fe4027a24ff2b7a05a4bf93725f2": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "2ce62470d0b845bb877bd8e284af1783": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "state": {"_view_name": "ProgressView", "style": "IPY_MODEL_31e22b9fd09a4aa99b496944d9ff5aa8", "_dom_classes": [], "description": "Downloading: 100%", "_model_name": "FloatProgressModel", "bar_style": "success", "max": 456318, "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": 456318, "_view_count": null, "_view_module_version": "1.5.0", "orientation": "horizontal", "min": 0, "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_e6d3aff091e34a8fb23a61568c31cd6b"}}, "4db46f7fa885490db935bb3ed0533e8f": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_view_name": "HTMLView", "style": "IPY_MODEL_c0f3a2716bec4dca9827ff0fbafd43ea", "_dom_classes": [], "description": "", "_model_name": "HTMLModel", "placeholder": "\u200b", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": " 456k/456k [00:01&lt;00:00, 286kB/s]", "_view_count": null, "_view_module_version": "1.5.0", "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_daf5041e797e40c3b02e8649e1bbba4f"}}, "31e22b9fd09a4aa99b496944d9ff5aa8": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_view_name": "StyleView", "_model_name": "ProgressStyleModel", "description_width": "initial", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "bar_color": null, "_model_module": "@jupyter-widgets/controls"}}, "e6d3aff091e34a8fb23a61568c31cd6b": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "c0f3a2716bec4dca9827ff0fbafd43ea": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_view_name": "StyleView", "_model_name": "DescriptionStyleModel", "description_width": "", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "_model_module": "@jupyter-widgets/controls"}}, "daf5041e797e40c3b02e8649e1bbba4f": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "e45254bb4f794a88a929c6c857afbb83": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_view_name": "HBoxView", "_dom_classes": [], "_model_name": "HBoxModel", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.5.0", "box_style": "", "layout": "IPY_MODEL_2218bf1642b04d5ba0765f0f734ceb7c", "_model_module": "@jupyter-widgets/controls", "children": ["IPY_MODEL_8d47d84a9ad8463f879072489a607e86", "IPY_MODEL_ba2b3e9b59b3445ba84382e0ca5e2be8"]}}, "2218bf1642b04d5ba0765f0f734ceb7c": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "8d47d84a9ad8463f879072489a607e86": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "state": {"_view_name": "ProgressView", "style": "IPY_MODEL_64594d21ea31486592fb7faf9f236f95", "_dom_classes": [], "description": "Downloading: 100%", "_model_name": "FloatProgressModel", "bar_style": "success", "max": 554, "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": 554, "_view_count": null, "_view_module_version": "1.5.0", "orientation": "horizontal", "min": 0, "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_3e98ca9215b747068270fa7d64f3899f"}}, "ba2b3e9b59b3445ba84382e0ca5e2be8": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_view_name": "HTMLView", "style": "IPY_MODEL_0e022de8639340b7a26a6ce701961a57", "_dom_classes": [], "description": "", "_model_name": "HTMLModel", "placeholder": "\u200b", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": " 554/554 [00:00&lt;00:00, 639B/s]", "_view_count": null, "_view_module_version": "1.5.0", "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_e2d2843ff50e4b448beca35526f523c3"}}, "64594d21ea31486592fb7faf9f236f95": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_view_name": "StyleView", "_model_name": "ProgressStyleModel", "description_width": "initial", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "bar_color": null, "_model_module": "@jupyter-widgets/controls"}}, "3e98ca9215b747068270fa7d64f3899f": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "0e022de8639340b7a26a6ce701961a57": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_view_name": "StyleView", "_model_name": "DescriptionStyleModel", "description_width": "", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "_model_module": "@jupyter-widgets/controls"}}, "e2d2843ff50e4b448beca35526f523c3": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "3281f9ba3a2440c0aa28c42745a03df2": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_view_name": "HBoxView", "_dom_classes": [], "_model_name": "HBoxModel", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.5.0", "box_style": "", "layout": "IPY_MODEL_f8672f80090940d18519e4684c900703", "_model_module": "@jupyter-widgets/controls", "children": ["IPY_MODEL_f232c70932c64df5b7cfb1e4096a0129", "IPY_MODEL_3bdca6d24d80427e9b4ef8f30eb622df"]}}, "f8672f80090940d18519e4684c900703": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "f232c70932c64df5b7cfb1e4096a0129": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "state": {"_view_name": "ProgressView", "style": "IPY_MODEL_4a4ab84ce1fe44e396c90dc87f877613", "_dom_classes": [], "description": "Downloading: 100%", "_model_name": "FloatProgressModel", "bar_style": "success", "max": 1042301, "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": 1042301, "_view_count": null, "_view_module_version": "1.5.0", "orientation": "horizontal", "min": 0, "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_9e66001b01e24173b3d062c7857bfbc9"}}, "3bdca6d24d80427e9b4ef8f30eb622df": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_view_name": "HTMLView", "style": "IPY_MODEL_7f46e9511a9a46a4b2184b486a3e73be", "_dom_classes": [], "description": "", "_model_name": "HTMLModel", "placeholder": "\u200b", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": " 1.04M/1.04M [00:02&lt;00:00, 356kB/s]", "_view_count": null, "_view_module_version": "1.5.0", "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_e718f9dbf16e41508d4e139f898feaa0"}}, "4a4ab84ce1fe44e396c90dc87f877613": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_view_name": "StyleView", "_model_name": "ProgressStyleModel", "description_width": "initial", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "bar_color": null, "_model_module": "@jupyter-widgets/controls"}}, "9e66001b01e24173b3d062c7857bfbc9": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "7f46e9511a9a46a4b2184b486a3e73be": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_view_name": "StyleView", "_model_name": "DescriptionStyleModel", "description_width": "", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "_model_module": "@jupyter-widgets/controls"}}, "e718f9dbf16e41508d4e139f898feaa0": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "05b0d970c6de4ccea7bf268e390974a4": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_view_name": "HBoxView", "_dom_classes": [], "_model_name": "HBoxModel", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.5.0", "box_style": "", "layout": "IPY_MODEL_c4ab36adf4c24640aade28cd6cbccf2c", "_model_module": "@jupyter-widgets/controls", "children": ["IPY_MODEL_90009b2929e34639bef82bdfdcb99d79", "IPY_MODEL_e2652b36d9dc4cdda2a9b79ccf42eb7c"]}}, "c4ab36adf4c24640aade28cd6cbccf2c": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "90009b2929e34639bef82bdfdcb99d79": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "state": {"_view_name": "ProgressView", "style": "IPY_MODEL_b502bcff4fb5479fba3b4acd5c155614", "_dom_classes": [], "description": "Downloading: 100%", "_model_name": "FloatProgressModel", "bar_style": "success", "max": 456318, "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": 456318, "_view_count": null, "_view_module_version": "1.5.0", "orientation": "horizontal", "min": 0, "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_c958b278aa7b47e084b3f65d07a51d99"}}, "e2652b36d9dc4cdda2a9b79ccf42eb7c": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_view_name": "HTMLView", "style": "IPY_MODEL_063970ce61cb4156a7925955b7e2518d", "_dom_classes": [], "description": "", "_model_name": "HTMLModel", "placeholder": "\u200b", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": " 456k/456k [00:01&lt;00:00, 303kB/s]", "_view_count": null, "_view_module_version": "1.5.0", "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_ec3bce1043cb47d19d99b9201f9a132c"}}, "b502bcff4fb5479fba3b4acd5c155614": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_view_name": "StyleView", "_model_name": "ProgressStyleModel", "description_width": "initial", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "bar_color": null, "_model_module": "@jupyter-widgets/controls"}}, "c958b278aa7b47e084b3f65d07a51d99": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "063970ce61cb4156a7925955b7e2518d": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_view_name": "StyleView", "_model_name": "DescriptionStyleModel", "description_width": "", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "_model_module": "@jupyter-widgets/controls"}}, "ec3bce1043cb47d19d99b9201f9a132c": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}}
</script>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ncoop57/i-am-a-nerd"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/i-am-a-nerd/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/i-am-a-nerd/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/i-am-a-nerd/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A singular place for all of my nerdy ramblings.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/ncoop57" title="ncoop57"><svg class="svg-icon grey"><use xlink:href="/i-am-a-nerd/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/ncooper57" title="ncooper57"><svg class="svg-icon grey"><use xlink:href="/i-am-a-nerd/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
