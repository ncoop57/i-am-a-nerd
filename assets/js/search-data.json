{
  
    
        "post0": {
            "title": "Automatic Code Summarization Using Deep Learning",
            "content": "About . Hi there, in this post you&#39;ll learn how to finetune the a RoBERT based model that&#39;s been trained on code data to automatically generate comments for code! . We will be focusing on the Java programming language, but you can apply the same techniques in this post for any programming language that interests you. Additionally, you&#39;ll see how to incorporate this code commenter into a VSCode extension so that you can generate comments for code snippets you highlight: . (Insert GIF of tool working) . As always, we&#39;ll start with a bit of background of the data and model we are using, but feel free to skip if you want to get straight to the awesomeness ;). Alright, let&#39;s GO! . Background . Data . We will be using the awesome CodeSearchNet Challenge dataset, which contains millions of pairs of methods and their docstrings for a large variety of programming languages. The dataset was initially constructed for evaluating how well different approaches perform at searching for code. However, we can easily repurpose it for us and lucky for us, the awesome authors did an awesome job collecting, documenting, and cleaning the data. . We&#39;ll be performing a bit more cleaning and formatting of the data as well as adding some more examples. These examples won&#39;t be method/docstring pairs, but code snippet/inline comment pairs. This allows our model to generate comments for arbitrary code snippets that a developer may want to document instead of just generating the docstring of a method. . CodeBERT . The pretrained model we will be finetuning comes from the awesome paper from Microsoft&#39;s research division aptly named CodeBERT: A Pre-Trained Model for Programming and Natural Languages. This model also used the CodeSearchNet challenge dataset, but instead of using it to generate comments it used to teach a RoBERTa based model to represent code and natural language in a useful way. This practice of eaching these large language models to represent text in a useful way is common practice now since these representations have been shown to be helpful in finetuning these models on other tasks. The CodeBERT paper showed these representations are helpful by finetuning them on the programming task of code search and comment generation, exactly what we will be doing! The difference between their comment generation task and ours is that we will do a bit more preprocessing and our model will be able to generate inline comments of code snippets and not just method level comments. . So, how does CodeBERT learn these representations? It combines two different training objectives that&#39;s been shown to be useful for natural language. The Masked Language Modeling objective (MLM), which is from the original BERT paper, and Replaced Token Detection (RTD) objective, which is from the ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators paper. The MLM objective is where we randomly mask out parts of the text that we feed into the model and ask the model to predict those masked out pieces. The RTD objective is where random tokens in the text are replaced and the model has to determine which of these tokens are replaced. However, to make it harder for the model, these replaced tokens attempt to be plausible alternatives and not just random words. The CodeBERT model actually used a n-gram based model to generate these alternatives where as the ELECTRA paper used a small BERT based model. . (From ELECTRA Paper) . Instead of using only natural language to apply these training objectives to, CodeBERT used code and docstrings. This allowed the CodeBERT model to learn a useful representation of code that could be used for other tasks. . Alright with that quick background knowledge down, lets get into actually finetuning our model! . ! nvidia-smi . Sat Dec 5 16:48:30 2020 +--+ | NVIDIA-SMI 455.45.01 Driver Version: 418.67 CUDA Version: 10.1 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P4 Off | 00000000:00:04.0 Off | 0 | | N/A 36C P8 7W / 75W | 0MiB / 7611MiB | 0% Default | | | | ERR! | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . Data . First we&#39;ll install the necessary packages and download our data! . # Download and install the necessary dependencies ! pip install -q torch==1.4.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html ! pip install -q transformers==3.5.0 fast-trees ! git clone -q https://github.com/microsoft/CodeXGLUE.git # Download the CodeSearchNet Challenge dataset for the Java programming language ! wget -q https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip ! unzip -qq java.zip . Next let&#39;s read in our data and since these models take a long time to train, we will only select a subset of the data. . import pandas as pd from pathlib import Path from typing import List, Optional # Code from CodeSearchNetChallenge: https://github.com/github/CodeSearchNet/blob/master/notebooks/ExploreData.ipynb def jsonl_list_to_dataframe(file_list, columns=[&#39;code&#39;, &#39;docstring&#39;]): &quot;&quot;&quot;Load a list of jsonl.gz files into a pandas DataFrame.&quot;&quot;&quot; return pd.concat([pd.read_json(f, orient=&#39;records&#39;, compression=&#39;gzip&#39;, lines=True)[columns] for f in file_list], sort=False) def get_dfs(path: Path) -&gt; List[pd.DataFrame]: &quot;&quot;&quot;Grabs the different data splits and converts them into dataframes&quot;&quot;&quot; dfs = [] for split in [&quot;train&quot;, &quot;valid&quot;, &quot;test&quot;]: files = sorted((path/split).glob(&quot;**/*.gz&quot;)) df = jsonl_list_to_dataframe(files).rename(columns = {&#39;code&#39;: &#39;mthd&#39;, &#39;docstring&#39;: &#39;cmt&#39;}) dfs.append(df) return dfs path = Path(&#39;.&#39;) df_trn, df_val, df_tst = get_dfs(path/&quot;java/final/jsonl&quot;) sample = 0.01 df_trn = df_trn.sample(frac = sample) df_val = df_val.sample(frac = sample) df_tst = df_tst.sample(frac = sample) len(df_trn), len(df_val), len(df_tst) . . (4545, 153, 269) . Let&#39;s see how the data looks. As shown, we have the data in a good format with one column all of the methods (input into the model) and the other all of the comments (output of the model). . df_trn.head() . mthd cmt . 1686 public static String reduceSurtToAssignmentLev... | Truncate SURT to its topmost assigned domain s... | . 2593 @Override n public DeleteGroupResult delete... | &lt;p&gt; nDeletes a group. Currently only groups wi... | . 29260 public void Not() n t{ n t tif( periods == nul... | NOT operator | . 16350 public static File createDirectory(File parent... | Create directory with given name in specified ... | . 22970 public final void ruleTerminalTokenElement() t... | InternalXtext.g:1204:1: ruleTerminalTokenEleme... | . Data Cleaning . Now, that we have the data, let&#39;s clean it! First, we&#39;ll remove any non-ascii characters to simplify the problem so that the model only has to think about generating English comments. . # From https://stackoverflow.com/a/27084708/5768407 def is_ascii(s): &#39;&#39;&#39; Determines if the given string contains only ascii characters :param s: the string to check :returns: whether or not the given string contains only ascii characters &#39;&#39;&#39; try: s.encode(encoding=&#39;utf-8&#39;).decode(&#39;ascii&#39;) except UnicodeDecodeError: return False else: return True df_trn = df_trn[df_trn[&#39;mthd&#39;].apply(lambda x: is_ascii(x))] df_val = df_val[df_val[&#39;mthd&#39;].apply(lambda x: is_ascii(x))] df_tst = df_tst[df_tst[&#39;mthd&#39;].apply(lambda x: is_ascii(x))] df_trn = df_trn[df_trn[&#39;cmt&#39;].apply(lambda x: is_ascii(x))] df_val = df_val[df_val[&#39;cmt&#39;].apply(lambda x: is_ascii(x))] df_tst = df_tst[df_tst[&#39;cmt&#39;].apply(lambda x: is_ascii(x))] len(df_trn), len(df_val), len(df_tst) . (4417, 143, 263) . Next, we&#39;ll remove any outdated comments by checking to see if the JavaDoc&#39;s parameter list is different from the method&#39;s parameter list. This also will remove pairs where the docstring doesn&#39;t actually document the parameters, which probably means the pairs are poor quality (you should always properly document your code :) ). . import re from fast_trees.core import FastParser parser = FastParser(&#39;java&#39;) def get_cmt_params(cmt: str) -&gt; List[str]: &#39;&#39;&#39; Grabs the parameter identifier names from a JavaDoc comment :param cmt: the comment to extract the parameter identifier names from :returns: an array of the parameter identifier names found in the given comment &#39;&#39;&#39; params = re.findall(&#39;@param+ s+ w+&#39;, cmt) param_names = [] for param in params: param_names.append(param.split()[1]) return param_names def is_outdated(mthd: str, cmt: str, parser: FastParser) -&gt; bool: &#39;&#39;&#39; Determines if a given method and comment are outdated by checking if the method&#39;s parameter identifier names match the comment&#39;s :param mthd: the method to compare against its corresponding comment :param cmt: the comment to compare against its corresponding method :param parser: parser for easily getting the parameter identifier names from a given method :returns: wheather or not a given comment is outdated compared to its corresponding method &#39;&#39;&#39; try: mthd_params = parser.get_params(mthd) except: return False cmt_params = get_cmt_params(cmt) return mthd_params != cmt_params df_trn = df_trn[ ~df_trn.apply( lambda x: is_outdated(x.mthd, x.cmt, parser), axis = 1 ) ] df_val = df_val[ ~df_val.apply( lambda x: is_outdated(x.mthd, x.cmt, parser), axis = 1 ) ] df_tst = df_tst[ ~df_tst.apply( lambda x: is_outdated(x.mthd, x.cmt, parser), axis = 1 ) ] len(df_trn), len(df_val), len(df_tst) . Downloading repo https://github.com/tree-sitter/tree-sitter-java to /usr/local/lib/python3.6/dist-packages/fast_trees/tree-sitter-java. . (4417, 143, 263) . Now we&#39;ll add in the additional pairs of code snippets/inline comments. . P.S. One thing to note with adding these pairs is that the inline comments will appear twice in the datasets. The first in the method where the inline comment came from and the second in the target for the code snippet. This is only a problem for the training set since it allows for the model to cheat by simply remembering the inline comment from the example method it came from. However, in my testing, I found this to not be an issue and the model seems to still work well despite this problem. Just thought ya should know :). . from tqdm.auto import tqdm def get_inline_pairs(mthd): &#39;&#39;&#39; Get all pairs of inline comments and corresponding code snippets :param mthd: the method to retrieve the pairs of comments and corresponding code snippets from :returns: all pairs of comments and corresponding code snippets &#39;&#39;&#39; pairs = [[]] comment = False bracket = False indent_lvl = -1 lines = mthd.split(&quot; n&quot;) for line in lines: if &quot;//&quot; in line and not bracket and not &quot;://&quot; in line: pairs[-1].append(line) if &#39; t&#39; in line: indent_lvl = line.count(&#39; t&#39;) else: indent_lvl = line.split(&quot;//&quot;)[0].count(&#39; &#39;) comment = True bracket = False elif comment: if &#39;{&#39; in line and not bracket: bracket = True pairs[-1].append(line) elif &#39;}&#39; in line: line_indent = -1 if &#39; t&#39; in line: line_indent = line.count(&#39; t&#39;) else: line_indent = line.split(&quot;//&quot;)[0].count(&#39; &#39;) if indent_lvl == line_indent: pairs[-1].append(line) if not bracket: pairs.append([]) comment = False bracket = False elif line.isspace() or line == &#39;&#39; and not bracket: pairs.append([]) comment = False else: pairs[-1].append(line) # Convert pairs into proper format of (code snippet, inline comment) dataframe code_snippets = [] comments = [] for pair in pairs: if pair and len(pair) &lt; 5: code = [] comment = [] skip = False for line in pair: if &quot;TODO&quot; in line: break if &quot;//&quot; in line: comment.append(line.replace(&#39;//&#39;, &#39;&#39;)) else: code.append(line) if len(code) &gt; 1 and len(comment) &gt; 0: code_snippets.append(&#39; n&#39;.join(code)) comments.append(&#39; n&#39;.join(comment)) pairs = pd.DataFrame(zip(code_snippets, comments), columns = [&quot;mthd&quot;, &quot;cmt&quot;]) return pairs def add_inline(df: pd.DataFrame) -&gt; pd.DataFrame: &#39;&#39;&#39; Helper function to go through all methods in a given dataframe and add all pairs of inline comments and corresponding code snippets :param df: the dataframe to retrieve and add all pairs of inline comments and corresponding code snippets to :returns: a new dataframe with the newly added pairs of inline comments and corresponding code snippets &#39;&#39;&#39; new_df = df[df[&#39;mthd&#39;].str.contains(&quot;//&quot;)] all_pairs = [] for mthd in tqdm(new_df.mthd.values): pairs = get_inline_pairs(mthd) all_pairs.append(pairs) df_pairs = pd.concat([pairs for pairs in all_pairs]) return pd.concat([df, df_pairs]) df_trn = add_inline(df_trn) df_val = add_inline(df_val) df_tst = add_inline(df_tst) len(df_trn), len(df_val), len(df_tst) . . (4593, 146, 272) . We&#39;ll also remove pairs where the size of the code is smaller than the comment. This is because I found that in these cases the comments contain a bunch of extra information that the model won&#39;t have access to such as how the method is being used by other methods in the software system. . df_trn = df_trn[df_trn.apply(lambda row: len(row.mthd) &gt; len(row.cmt), axis = 1)] df_val = df_val[df_val.apply(lambda row: len(row.mthd) &gt; len(row.cmt), axis = 1)] df_tst = df_tst[df_tst.apply(lambda row: len(row.mthd) &gt; len(row.cmt), axis = 1)] len(df_trn), len(df_val), len(df_tst) . (3695, 114, 226) . Next, we&#39;ll remove any examples that have the special tag since these also tend to contain extra information that the model doesn&#39;t have a good hope of generating.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def has_code(cmt: str) -&gt; bool: &#39;&#39;&#39; Determinine if the given comment contains the HTML &lt;code&gt; tag :param cmt: the comment to check whether it contains the HTML &lt;code&gt; tag :returns: whether or not the given comment contains the HTML &lt;code&gt; tag &#39;&#39;&#39; if &#39;&lt;code&gt;&#39; in cmt: return True else: return False df_trn = df_trn[~df_trn[&#39;cmt&#39;].apply(lambda x: has_code(x))] df_val = df_val[~df_val[&#39;cmt&#39;].apply(lambda x: has_code(x))] df_tst = df_tst[~df_tst[&#39;cmt&#39;].apply(lambda x: has_code(x))] len(df_trn), len(df_val), len(df_tst) . (3544, 103, 214) . Lastly, we&#39;re gonna remove the JavaDoc parts of the comments other than the description since that is really all we care about. The other pieces of information can usually be autogenerated or may require external knowledge to document them. . def remove_jdocs(df: pd.DataFrame) -&gt; pd.DataFrame: &#39;&#39;&#39; Remove the JavaDocs leaving only the description of the comment :param df: the pandas dataframe to remove the JavaDocs from :returns: a new pandas dataframe with the JavaDocs removed &#39;&#39;&#39; methods = [] comments = [] for i, row in tqdm(list(df.iterrows())): comment = row[&quot;cmt&quot;] # Remove {} text in comments from https://stackoverflow.com/questions/14596884/remove-text-between-and-in-python/14598135 comment = re.sub(&quot;([ { []).*?([ ) }])&quot;, &#39;&#39;, comment) cleaned = [] for line in comment.split(&#39; n&#39;): if &quot;@&quot; in line: break cleaned.append(line) comments.append(&#39; n&#39;.join(cleaned)) methods.append(row[&quot;mthd&quot;]) new_df = pd.DataFrame(zip(methods, comments), columns = [&quot;mthd&quot;, &quot;cmt&quot;]) return new_df df_trn = remove_jdocs(df_trn); df_val = remove_jdocs(df_val); df_tst = remove_jdocs(df_tst); . . Almost there! In this step, we&#39;ll remove any HTML tags from the comments so the model doesn&#39;t have to also learn HTML. Bless those that do... . def clean_html(cmt: str) -&gt; str: &#39;&#39;&#39; Remove any HTML tags from a given comment :param cmt: the comment to remove any HTML tags from :returns: the comment with any HTML tags removed &#39;&#39;&#39; result = re.sub(r&quot;&lt;.?span[^&gt;]*&gt;|&lt;.?code[^&gt;]*&gt;|&lt;.?p[^&gt;]*&gt;|&lt;.?hr[^&gt;]*&gt;|&lt;.?h[1-3][^&gt;]*&gt;|&lt;.?a[^&gt;]*&gt;|&lt;.?b[^&gt;]*&gt;|&lt;.?blockquote[^&gt;]*&gt;|&lt;.?del[^&gt;]*&gt;|&lt;.?dd[^&gt;]*&gt;|&lt;.?dl[^&gt;]*&gt;|&lt;.?dt[^&gt;]*&gt;|&lt;.?em[^&gt;]*&gt;|&lt;.?i[^&gt;]*&gt;|&lt;.?img[^&gt;]*&gt;|&lt;.?kbd[^&gt;]*&gt;|&lt;.?li[^&gt;]*&gt;|&lt;.?ol[^&gt;]*&gt;|&lt;.?pre[^&gt;]*&gt;|&lt;.?s[^&gt;]*&gt;|&lt;.?sup[^&gt;]*&gt;|&lt;.?sub[^&gt;]*&gt;|&lt;.?strong[^&gt;]*&gt;|&lt;.?strike[^&gt;]*&gt;|&lt;.?ul[^&gt;]*&gt;|&lt;.?br[^&gt;]*&gt;&quot;, &quot;&quot;, cmt) return result df_trn.cmt = df_trn.cmt.apply(clean_html) df_val.cmt = df_val.cmt.apply(clean_html) df_tst.cmt = df_tst.cmt.apply(clean_html) . FINALLY!! We&#39;ll make everything lower case, remove extra whitespace, remove empty comments, and remove duplicates. . df_trn = df_trn.applymap(lambda x: &#39; &#39;.join(x.split()).lower()) df_val = df_val.applymap(lambda x: &#39; &#39;.join(x.split()).lower()) df_tst = df_tst.applymap(lambda x: &#39; &#39;.join(x.split()).lower()) df_trn = df_trn[~(df_trn[&#39;cmt&#39;] == &#39;&#39;)] df_val = df_val[~(df_val[&#39;cmt&#39;] == &#39;&#39;)] df_tst = df_tst[~(df_tst[&#39;cmt&#39;] == &#39;&#39;)] df_trn = df_trn[~df_trn[&#39;cmt&#39;].duplicated()] df_val = df_val[~df_val[&#39;cmt&#39;].duplicated()] df_tst = df_tst[~df_tst[&#39;cmt&#39;].duplicated()] len(df_trn), len(df_val), len(df_tst) . (3044, 93, 195) . Now let&#39;s see what the data looks like. . df_trn.head() . mthd cmt . 0 public static string reducesurttoassignmentlev... | truncate surt to its topmost assigned domain s... | . 1 public void not() { if( periods == null ) { if... | not operator | . 2 public final void ruleterminaltokenelement() t... | internalxtext.g:1204:1: ruleterminaltokeneleme... | . 3 public void placefusedring(iring ring, iatomco... | generated coordinates for a given ring, which ... | . 4 @override public signaturevisitor visitparamet... | method signatures | . Data Exploring . As good Data Scientists, we will also explore our data to uncover any secrets. Data can be sneaky like that :). . import numpy as np from collections import Counter from statistics import mean, median, stdev from transformers import AutoTokenizer def get_counter(df: pd.DataFrame, tokenizer: AutoTokenizer, col: str) -&gt; Counter: &#39;&#39;&#39; Get the counts for each token in a given pandas dataframe column :param df: the pandas dataframe to get the counts of tokens from :param tokenizer: the tokenizer to use for tokenizing the rows in the pandas dataframe :param col: the column to grab rows from when tokenizing :returns: the counts of each token in the given pandas dataframe column &#39;&#39;&#39; toks = [] for i, row in df.iterrows(): toks.extend(tokenizer.tokenize(row[col])) cnt = Counter() for tok in toks: cnt[tok] += 1 return cnt tokenizer = AutoTokenizer.from_pretrained(&#39;microsoft/codebert-base&#39;) mthd_cnt = get_counter(df_trn, tokenizer, &#39;mthd&#39;) cmt_cnt = get_counter(df_trn, tokenizer, &#39;cmt&#39;) mthd_lens = df_trn.mthd.apply(lambda x: len(tokenizer.tokenize(x))).values cmt_lens = df_trn.cmt.apply(lambda x: len(tokenizer.tokenize(x))).values max_mthd_len = int(np.quantile(mthd_lens, 0.95)) max_cmt_len = int(np.quantile(cmt_lens, 0.95)) . . import matplotlib.pyplot as plt def plot_counts(counts:Counter, top_k: Optional[int] = 30): &#39;&#39;&#39; Plot a bar chart of the most common tokens :param counts: the counts of each token :param top_k: the number of tokens to display in the plot &#39;&#39;&#39; labels, values = zip(*counts.most_common()[:top_k]) indexes = np.arange(len(labels)) width = 1 plt.figure(num=None, figsize=(22, 4), dpi=60, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.bar(indexes, values, width) plt.xticks(indexes + width * 0.5, labels) plt.show() . Let&#39;s look at the most common tokens in our methods and comments. . plot_counts(mthd_cnt, top_k = 30) plot_counts(cmt_cnt, top_k = 30) . def plot_hist(lens: List[int], n_bins: Optional[int] = 50): &#39;&#39;&#39; Plot a histogram of the given number of tokens in a column :param lens: the number of tokens in a column :param n_bins: the number of bins to sort the number of tokens into &#39;&#39;&#39; n, bins, patches = plt.hist(lens, n_bins, facecolor=&#39;blue&#39;, alpha=0.9) plt.show() . Now, let&#39;s look at the distribution of method and comment lengths. . print(mean(mthd_lens), median(mthd_lens), stdev(mthd_lens)) plot_hist(mthd_lens) print(mean(cmt_lens), median(cmt_lens), stdev(cmt_lens)) plot_hist(cmt_lens) . 184 104.0 325.0169226363452 . 17 12.0 19.078784028338912 . Using this new information on the length distribution, we can remove outliers by filter by lengths of methods that fall outside of 95th percentile (chosen for completely arbitrary reasons)! . def filter_len( row: pd.Series, tokenizer: AutoTokenizer, mthd_len: int, cmt_len: int ) -&gt; bool: &#39;&#39;&#39; Determine if a given panda dataframe row has a method or comment that has more tokens than max length :param row: the row to check if it has a method or comment that is too long :param tokenizer: the tokenizer to tokenize a method or comment :param mthd_len: the max number of tokens a method can have :param cmt_len: the max number of tokens a comment can have :returns: whether or not the given row have a method or comment that have more tokens than a max length &#39;&#39;&#39; return len(tokenizer.tokenize(row.mthd)) &lt; mthd_len and len(tokenizer.tokenize(row.cmt)) &lt; cmt_len df_trn = df_trn[df_trn.apply( lambda row: filter_len( row, tokenizer, max_mthd_len, max_cmt_len ), axis = 1 )] df_val = df_val[df_val.apply( lambda row: filter_len( row, tokenizer, max_mthd_len, max_cmt_len ), axis = 1 )] df_tst = df_tst[df_tst.apply( lambda row: filter_len( row, tokenizer, max_mthd_len, max_cmt_len ), axis = 1 )] len(df_trn), len(df_val), len(df_tst) . (2770, 89, 178) . max_mthd_len, max_cmt_len . (568, 49) . We could do a lot more exploring of our data as the above exploration was the bare minimum. As an exercise, I suggest for you to explore the data on your own using whatever means necessary! . Training . Now that we have our data processed and in a format we like, let&#39;s go ahead and start training! To accomplish this we will be using code from the awesome CodeXGLUE repository. This repository is similar to the NLP equivalent GLUE benchmarks where a ton of awesome code related benchmarks are standardized and put into one place for the community to use! They have a ton of interesting ones and I highly suggest looking through their repo if you are interested in other code related tasks. . cd ./CodeXGLUE/Code-Text/code-to-text/code . /content/CodeXGLUE/Code-Text/code-to-text/code . Okay, I lied, sorry :(. One last processing step is required of our data, which is to just output the data into the structure that the awesome CodeXGLUE Code-Text benchmark expects. . import json df_trn[&#39;code_tokens&#39;] = df_trn.mthd.apply(lambda x: x.split()) df_trn[&#39;docstring_tokens&#39;] = df_trn.cmt.apply(lambda x: x.split()) with open(&#39;java/train.jsonl&#39;,&#39;w&#39;) as f: for _, row in df_trn.iterrows(): f.write(json.dumps(row.to_dict()) + &#39; n&#39;) df_val[&#39;code_tokens&#39;] = df_val.mthd.apply(lambda x: x.split()) df_val[&#39;docstring_tokens&#39;] = df_val.cmt.apply(lambda x: x.split()) with open(&#39;java/valid.jsonl&#39;,&#39;w&#39;) as f: for _, row in df_val.iterrows(): f.write(json.dumps(row.to_dict()) + &#39; n&#39;) df_tst[&#39;code_tokens&#39;] = df_tst.mthd.apply(lambda x: x.split()) df_tst[&#39;docstring_tokens&#39;] = df_tst.cmt.apply(lambda x: x.split()) with open(&#39;java/test.jsonl&#39;,&#39;w&#39;) as f: for _, row in df_tst.iterrows(): f.write(json.dumps(row.to_dict()) + &#39; n&#39;) . lang = &#39;java&#39; # programming language lr = 5e-5 batch_size = 8 # change depending on the GPU Colab gives you beam_size = 10 source_length = 256 target_length = max_cmt_len data_dir = &#39;.&#39; output_dir = f&#39;model/{lang}&#39; train_file = f&#39;{data_dir}/{lang}/train.jsonl&#39; dev_file = f&#39;{data_dir}/{lang}/valid.jsonl&#39; epochs = 10 pretrained_model = &#39;microsoft/codebert-base&#39; ! python run.py --do_train --do_eval --do_lower_case --model_type roberta --model_name_or_path {pretrained_model} --train_filename {train_file} --dev_filename {dev_file} --output_dir {output_dir} --max_source_length {source_length} --max_target_length {target_length} --beam_size {beam_size} --train_batch_size {batch_size} --eval_batch_size {batch_size} --learning_rate {lr} --num_train_epochs {epochs} . Yay! Our model has finished baking and we can now see how well it turned out by evaluating it! . batch_size=64 dev_file=f&quot;{data_dir}/{lang}/valid.jsonl&quot; test_file=f&quot;{data_dir}/{lang}/test.jsonl&quot; test_model=f&quot;{output_dir}/checkpoint-best-bleu/pytorch_model.bin&quot; #checkpoint for test ! python run.py --do_test --model_type roberta --model_name_or_path microsoft/codebert-base --load_model_path {test_model} --dev_filename {dev_file} --test_filename {test_file} --output_dir {output_dir} --max_source_length {source_length} --max_target_length {target_length} --beam_size {beam_size} --eval_batch_size {batch_size} . Let&#39;s now load up our model and take it for a spin! . import torch import torch.nn as nn from model import Seq2Seq from transformers import RobertaConfig, RobertaModel config = RobertaConfig.from_pretrained(pretrained_model) encoder = RobertaModel.from_pretrained(pretrained_model, config = config) decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads) decoder = nn.TransformerDecoder(decoder_layer, num_layers=6) model = Seq2Seq(encoder = encoder,decoder = decoder,config=config, beam_size=beam_size,max_length=target_length, sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id) model.load_state_dict(torch.load(Path(output_dir)/&quot;checkpoint-last/pytorch_model.bin&quot;)) model.to(&#39;cuda&#39;) . &lt;All keys matched successfully&gt; . idx = 0 TEXT_TO_SUMMARIZE = df_val.mthd.values[idx] print(&#39;Code:&#39;, TEXT_TO_SUMMARIZE) print(&#39;Original Comment:&#39;, df_val.cmt.values[idx]) . Code: @override public void oncomponenttag(final component _component, final componenttag _tag) { super.oncomponenttag(_component, _tag); _tag.put(&#34;data-dojo-type&#34;, &#34;dijit/layout/bordercontainer&#34;); _tag.append(&#34;data-dojo-props&#34;, &#34;design: &#39;&#34; + this.design.key + &#34;&#39;&#34;, &#34;,&#34;); _tag.append(&#34;class&#34;, &#34;tundra efapsbordercontainer&#34;, &#34; &#34;); } Original Comment: the tag of the related component must be set, so that a dojo bordercontainer will be rendered. . from run import convert_examples_to_features, Example class Args: max_source_length = source_length max_target_length = target_length args = Args() def get_preds(df: pd.DataFrame): ps = [] for idx, row in tqdm(df.iterrows(), total=len(df)): examples = [ Example(idx, source = row.mthd, target = row.cmt) ] eval_features = convert_examples_to_features( examples, tokenizer, args, stage=&#39;test&#39; ) source_ids = torch.tensor(eval_features[0].source_ids, dtype = torch.long).unsqueeze(0).to(&#39;cuda&#39;) source_mask = torch.tensor(eval_features[0].source_mask, dtype = torch.long).unsqueeze(0).to(&#39;cuda&#39;) with torch.no_grad(): preds = model(source_ids = source_ids, source_mask = source_mask) for pred in preds: t = pred[0].cpu().numpy() t = list(t) if 0 in t: t = t[:t.index(0)] text = tokenizer.decode(t,clean_up_tokenization_spaces=False) ps.append(text) return ps . preds = get_preds(df_val.head(10)) for idx, row in df_val.head(10).iterrows(): print(&#39;Code:&#39;, row.mthd) print(&#39;Original Comment:&#39;, row.cmt) print(&#39;Generated Comment:&#39;, preds[idx]) print(row.loss) print(&#39;=&#39;*40) . &#39;returns the given object.&#39; . The model seems to be doing a good job, but if you play with it some more you&#39;ll realize it is mostly taking the name of the method and using that to guide the comment. This makes sense, but it probably isn&#39;t learning much more than this association, at least with this small model. Let&#39;s explore it a bit more by looking at all the examples in the validation set it is failing the most on. . def get_preds_losses(df: pd.DataFrame): ps = [] losses = [] for idx, row in tqdm(df.iterrows(), total=len(df)): examples = [ Example(idx, source = row.mthd, target = row.cmt) ] eval_features = convert_examples_to_features( examples, tokenizer, args, stage=&#39;test&#39; ) source_ids = torch.tensor([f.source_ids for f in eval_features], dtype = torch.long).to(&#39;cuda&#39;) source_mask = torch.tensor([f.source_mask for f in eval_features], dtype = torch.long).to(&#39;cuda&#39;) target_ids = torch.tensor([f.target_ids for f in eval_features], dtype = torch.long).to(&#39;cuda&#39;) target_mask = torch.tensor([f.target_mask for f in eval_features], dtype = torch.long).to(&#39;cuda&#39;) with torch.no_grad(): _, loss, _ = model( source_ids = source_ids, source_mask = source_mask, target_ids = target_ids, target_mask = target_mask ) preds = model(source_ids = source_ids, source_mask = source_mask) for pred in preds: t = pred[0].cpu().numpy() t = list(t) if 0 in t: t = t[:t.index(0)] text = tokenizer.decode(t,clean_up_tokenization_spaces=False) ps.append(text) losses.append(loss.item()) return ps, losses . df_head = df_val.copy() ps, losses = get_preds_losses(df_head) df_head[&#39;pred&#39;] = ps df_head[&#39;loss&#39;] = losses df_sorted_losses = df_head.sort_values(&#39;loss&#39;, ascending = False) for _, row in df_sorted_losses.head(10).iterrows(): print(&#39;Code:&#39;, row.mthd) print(&#39;Original Comment:&#39;, row.cmt) print(&#39;Generated Comment:&#39;, row.pred) print(row.loss) print(&#39;=&#39;*40) . Code: protected browsercanvas renderurl(url url, dimension pagesize) throws ioexception, saxexception { documentsource src = new defaultdocumentsource(url); pageurl = src.geturl(); inputstream is = src.getinputstream(); string mime = src.getcontenttype(); if (mime == null) mime = &#34;text/html&#34;; int p = mime.indexof(&#39;;&#39;); if (p != -1) mime = mime.substring(0, p).trim(); log.info(&#34;file type: &#34; + mime); if (mime.equals(&#34;application/pdf&#34;)) { pddocument doc = loadpdf(is); browsercanvas canvas = new pdfbrowsercanvas(doc, null, pagesize, src.geturl()); doc.close(); pagetitle = &#34;&#34;; return canvas; } else { domsource parser = new defaultdomsource(src); document doc = parser.parse(); pagetitle = findpagetitle(doc); string encoding = parser.getcharset(); mediaspec media = new mediaspec(&#34;screen&#34;); //updatecurrentmedia(media); domanalyzer da = new domanalyzer(doc, src.geturl()); if (encoding == null) encoding = da.getcharacterencoding(); da.setdefaultencoding(encoding); da.setmediaspec(media); da.attributestostyles(); da.addstylesheet(null, cssnorm.stdstylesheet(), domanalyzer.origin.agent); da.addstylesheet(null, cssnorm.userstylesheet(), domanalyzer.origin.agent); da.addstylesheet(null, cssnorm.formsstylesheet(), domanalyzer.origin.agent); da.getstylesheets(); browsercanvas contentcanvas = new browsercanvas(da.getroot(), da, src.geturl()); contentcanvas.getconfig().setloadimages(false); contentcanvas.getconfig().setloadbackgroundimages(false); contentcanvas.getconfig().setloadfonts(false); contentcanvas.getconfig().setreplaceimageswithalt(replaceimageswithalt); contentcanvas.createlayout(pagesize); src.close(); return contentcanvas; } } Original Comment: =================================================================== Generated Comment: creates a new object. 19.359130859375 ======================================== Code: public void execute(string[] args, boolean debugenabled, boolean quiet) { try { this.debugenabled = debugenabled; debug(&#34;checkreplicationtool#execute(string[]): args = &#34; + arrays.aslist(args)); parsecommandlinearguments(args); system.out.println(&#34;connecting...&#34;); debug(&#34;checkreplicationtool#execute(string[]): creating checkreplicationtool.processortask with parameters: clustersproperties = &#34; + clustersproperties + &#34;, timeout = &#34; + timeout + &#34;, regionname = &#34; + regionname); processortask task = new processortask(clustersproperties, timeout, regionname, debugenabled, quiet); debug(&#34;checkreplicationtool#execute(string[]): starting checkreplicationtool.processortask&#34;); utils.execute(task, timeout + delta_timeout); int exitcode = task.getexitcode(); debug(&#34;checkreplicationtool#execute(string[]): checkreplicationtool.processortask finished with exitcode = &#34; + exitcode); if (exitcode == 0) { utils.exitwithsuccess(); } utils.exitwithfailure(); } catch (throwable t) { debug( &#34;checkreplicationtool#execute(string[]): throwable caught with message = &#34; + t.getmessage(), t); utils.exitwithfailure(&#34;unexpected throwable&#34;, t); } } Original Comment: runs the tool. all the tools run in this way. Generated Comment: creates a new object. 18.94735336303711 ======================================== Code: public bsondocument asdocument() { bsondocument readconcern = new bsondocument(); if (level != null) { readconcern.put(&#34;level&#34;, new bsonstring(level.getvalue())); } return readconcern; } Original Comment: gets this read concern as a document. Generated Comment: returns the given object. 18.77771759033203 ======================================== Code: public void reset() { if (getlistviewwrapper() == null) { throw new illegalstateexception(&#34;call setabslistview() on this animationadapter first!&#34;); } assert mviewanimator != null; mviewanimator.reset(); mgridviewpossiblymeasuring = true; mgridviewmeasuringposition = -1; if (getdecoratedbaseadapter() instanceof animationadapter) { ((animationadapter) getdecoratedbaseadapter()).reset(); } } Original Comment: call this method to reset animation status on all views. the next time } is called on the base adapter, all views will animate again. Generated Comment: returns the given object. 18.72751235961914 ======================================== Code: public &lt;databaseobjecttype extends databaseobject&gt; set&lt;databaseobjecttype&gt; get(class&lt;databaseobjecttype&gt; type) { return allfound.get(type); } Original Comment: returns all objects of the given type that are already included in this snapshot. Generated Comment: returns the object. 18.644784927368164 ======================================== Code: public mustacheenginebuilder addtemplatelocator(templatelocator locator) { checkargumentnotnull(locator); checknotbuilt(); this.templatelocators.add(locator); return this; } Original Comment: adds a template locator. Generated Comment: returns the list. 18.576839447021484 ======================================== Code: public string toquerystring(criteria criteria, criteriaquerybuilder querybuilder) { stringbuilder builder = new stringbuilder(); builder.append( querybuilder.createpositionalparameter() ); if( negate ) { builder.append(&#34; not&#34;); } builder.append( &#34; member of &#34; ); builder.append( querybuilder.getabsolutepath(criteria, relativepath) ); return builder.tostring(); } Original Comment: /* Generated Comment: returns the given object. 18.488502502441406 ======================================== Code: @override public void oncomponenttag(final component _component, final componenttag _tag) { super.oncomponenttag(_component, _tag); _tag.put(&#34;data-dojo-type&#34;, &#34;dijit/layout/bordercontainer&#34;); _tag.append(&#34;data-dojo-props&#34;, &#34;design: &#39;&#34; + this.design.key + &#34;&#39;&#34;, &#34;,&#34;); _tag.append(&#34;class&#34;, &#34;tundra efapsbordercontainer&#34;, &#34; &#34;); } Original Comment: the tag of the related component must be set, so that a dojo bordercontainer will be rendered. Generated Comment: returns the given object. 18.482847213745117 ======================================== Code: public indexrequest sphereversion(final integer sphereversion) { if (sphereversion != null) { istrueargument(&#34;sphereindexversion must be 1, 2 or 3&#34;, valid_sphere_index_versions.contains(sphereversion)); } this.sphereversion = sphereversion; return this; } Original Comment: sets the 2dsphere index version number. Generated Comment: returns the given object. 18.472774505615234 ======================================== Code: public void update(final dbobject update) { bulkwriteoperation.addrequest(new updaterequest(query, update, true, upsert, querycodec, collation, arrayfilters)); } Original Comment: adds a request to update all documents in the collection that match the query with which this builder was created. Generated Comment: returns the value. 17.62411880493164 ======================================== . What&#39;s Next? . If you&#39;d like to see how you can integrate this code comment summarizer model into the popular VSCode IDE, check out my video that goes over just that! . youtube . &lt;/div&gt; .",
            "url": "https://nathancooper.io/i-am-a-nerd/code/summarization/deep-learning/seq2seq/2020/12/26/Improved_Code_Commenter.html",
            "relUrl": "/code/summarization/deep-learning/seq2seq/2020/12/26/Improved_Code_Commenter.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Open-Dialog Chatbots for Learning New Languages [Part 1]",
            "content": "Image by mohamed Hassan from Pixabay . This notebook was adapted from the following project: . https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py | Original license of the project this notebook was adapted from: https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py . LICENSE . # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team. # Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. . About . Hola! Today we will be creating a chatbot, but not just any chatbot. In this tutorial, you will create your own open-dialog chatbot, one that doesn&#39;t just have premade responses to very specific questions or commands! . The overall goal of this tutorial is to create a language learning companion where you can practice simple conversations in a language you care about. We will focus on the beautiful Spanish language in this series as I have been trying to learn the language for the past 5 years, however, you should be able to adapt this tutorial to other languages as well. . First we are going to cover some of the background material for how all this works (if you are already familiar with the GPT2 model, go ahead and skip this background section). Let&#39;s get to it! . Background . What is GPT2? . In this post, we are going to use the GPT2 model (Generative Pre-Training 2), from the amazing paper &quot;Language Models are Unsupervised Multitask Learners&quot; by Alex Radford et. al. I will be giving a brief overview of this model. However, if you want a more in-depth explanation I highly recommend the blog post &quot;The Illustrated GPT-2&quot; by Jay Alammar. . GPT2 is what is called an autoregressive language model. This may sound complicated, but it is actually quiet simple, so lets break down what this means. Autoregressive means that the output of the model is fedback into the model as input. Here is a nice example of how that works: . . Image From Deepmind . Now, a language model is usually some statistical model that gives the probability of some word given the context. So, take the following example: . An [blank] a day keeps the doctor away . A good language model would give higher probability to the word &quot;apple&quot; occuring in the [blank] than say the word &quot;crocodile&quot; since most likely encountering a crocodile daily would probably have the opposite effect. . Putting them together, we get an autoregressive language model where given some context . How much wood could a woodchuck chuck, if a woodchuck could [blank] . The statistical model then gives some probability to what the next word will be, which we will use in selecting the word. Once we have the selection we add it to our sentence and repeat the whole process again! . How much wood could a woodchuck chuck, if a woodchuck could chuck [blank] . Now, to train our autoregressive language model we just need to get a bunch of example sentences or just chunks of text, hide the last word, and use these sentences with the missing word as our inputs and the last words as the target. This is essentially the whole idea behind GPT2 and many other autoregressive language models, where they learn how language works by using the context to infer the next word. . GPT2 as a chatbot . Great, so you may be asking yourself, &quot;how do we use GPT2 as a chatbot?&quot; To answer this question we need to turn our attention to another paper, &quot;DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation&quot;. To see how we can repurpose this generator, GPT2, look at the following example: . Hi, how are you? [end_of_turn] I&#39;m good, what about you? [end_of_turn] Not so good, lots of long nights at work. [end_of_turn] Darn, that sucks :( [end_of_conversation] . This is a sample conversation between two speakers. What&#39;s special about it is that there are special tokens that signify when one of the speakers has finished talking, which we in the biz call a turn. If we treat this example like our previous one with the autorgressive language mode, we can do some interesting things: . Hi, how are you? [end_of_turn] [blank] . If we use the same logic as we did previously, it is easy to see how we can now use GPT2 to guess the next word in this conversation. . Hi, how are you? [end_of_turn] I&#39;m [blank] . We keep feeding back the prediction of our model and there ya have it! A chatting GPT2, where all we need to do is show the model a bunch of these example conversations and have it predict the next word in the conversation. . I think that is plenty of background, we will revisit exactly how we design a system where we actually hold a conversation with GPT2 once we have the model trained ;). . ! pip -q install transformers==2.9.0 gdown . |████████████████████████████████| 645kB 5.5MB/s |████████████████████████████████| 1.0MB 44.6MB/s |████████████████████████████████| 890kB 45.8MB/s |████████████████████████████████| 3.8MB 34.9MB/s Building wheel for sacremoses (setup.py) ... done . Let&#39;s define to configuration variables so we don&#39;t have a bunch of magic numbers and strings! . # Args to allow for easy convertion of python script to notebook class Args(): def __init__(self): self.output_dir = &#39;output&#39; self.model_type = &#39;gpt2&#39; self.model_name_or_path = &#39;microsoft/DialoGPT-small&#39; self.config_name = &#39;microsoft/DialoGPT-small&#39; self.tokenizer_name = &#39;microsoft/DialoGPT-small&#39; self.cache_dir = &#39;cached&#39; self.block_size = 512 self.do_train = True self.do_eval = True self.evaluate_during_training = False self.per_gpu_train_batch_size = 4 self.per_gpu_eval_batch_size = 4 self.gradient_accumulation_steps = 1 self.learning_rate = 5e-5 self.weight_decay = 0.0 self.adam_epsilon = 1e-8 self.max_grad_norm = 1.0 self.num_train_epochs = 3 self.max_steps = -1 self.warmup_steps = 0 self.logging_steps = 1000 self.save_steps = 3500 self.save_total_limit = None self.eval_all_checkpoints = False self.no_cuda = False self.overwrite_output_dir = True self.overwrite_cache = True self.should_continue = False self.seed = 42 self.local_rank = -1 self.fp16 = False self.fp16_opt_level = &#39;O1&#39; args = Args() . . The Data! . To train our chatbot we will be using conversations scraped from subtitles of Spanish TV shows and movies. I&#39;ve gone ahead and formated the data for us already, however, if you would like to use a different language to train your chatbot you can use this script to generate a csv with the same format I am going to use in the rest of this tutorial. . ! gdown https://drive.google.com/uc?id=1Lp-diuMohUTGyB9BSTFgeGZyY3dkNuEg . Downloading... From: https://drive.google.com/uc?id=1Lp-diuMohUTGyB9BSTFgeGZyY3dkNuEg To: /content/final_es_conv.csv 20.3MB [00:00, 55.6MB/s] . df = pd.read_csv(&#39;final_es_conv.csv&#39;) df = df.dropna() trn_df, val_df = train_test_split(df, test_size = 0.2) trn_df.head() . response context context/0 context/1 context/2 context/3 context/4 context/5 context/6 context/7 context/8 context/9 . 36917 Es tan simple. | No se que te detiene. | ¡Muy persuasiva! | No es crimen librarse de una alimaña. | ¿Por qué tener lastima de un hombre tan vil? | Además, también ha puesto sus ojos en Okayo. | Hace 4 años que soy victima de mi marido. | Solo estoy siendo franca contigo. | Calmate. | ¡Eres peor que el diablo! | ¿Comprendes? | Okayo me recuerda constantemente mi fracaso. | . 5449 Muy torpe, Joyce. | A la sala de interrogación rápido. ¡Muévanse! | De pie, muchachos. | A la sala de interrogación rápido. ¡Muévanse! | De pie, muchachos. | ¡Use su cuchillo, hombre! | ¡Adelántese, Thomson! | ¡Bien hecho, Jenkins! | Gracias. | Muy bien. | El bungaló del mayor Warden está al final del ... | Continúe, conductor. | . 37004 Pídemelo. | Sólo lo que quieras tú. | Ya no soy yo. | Eres preciosa y maravillosa. | ¿No? | Así te gustaré. | Haré y diré lo que quieras. | Nunca. | Así nunca querrás estar con otras, ¿verdad? | Siempre diré lo que tú desees y haré lo que tú... | Pero yo sí. | Pero... | . 47077 ¡Boris! | ¡Nicolás, que alegría a mi corazón, volviste! | ¡Regresan los Vencedores! | ¡Miren! | ¡Ahí vienen! | Está vivo. | Boris está vivo. | Dasha prometió avisarme cuando regrese. | Pero, en la fábrica dicen que él está en una u... | Tampoco hay noticias de Stepan. | ¡Quién sabe! | ¿Por qué entonces, no hay noticias de él? | . 41450 Entonces por qué no estamos en mejor situación... | Dora Hartley era una buena prueba. | Mire, lo que hace usted creer ¿Qué los indios ... | Aleja esa arma. | Buenas noches. | Es hora de ir a la cama. | Seguro. | Sí. recuerde que es un secreto. | Es bonita. | Está bien. | ¿Ann Martin? | Hola, Bax. | . len(trn_df), len(val_df) . (40374, 10094) . def get_counter_and_lens(data, tokenizer): flatten = lambda l: [item for sublist in l for item in sublist] toks = [tokenizer.tokenize(x) for x in data] return list(map(len, toks)), Counter(flatten(toks)), Counter(&#39; &#39;.join(data).split()) . . tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir) lens, tok_cnt, word_cnt = get_counter_and_lens(trn_df[df.columns].apply(lambda x: &#39; &#39;.join(x.astype(str)), axis = 1), tokenizer) . . def plot_counts(counts, top_k = 30): labels, values = zip(*counts.most_common()[:top_k]) indexes = np.arange(len(labels)) width = 1 plt.figure(num=None, figsize=(22, 4), dpi=60, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.bar(indexes, values, width) plt.xticks(indexes + width * 0.5, labels) plt.show() . . plot_counts(tok_cnt, top_k = 30) plot_counts(word_cnt, top_k = 30) . def plot_hist(lens, n_bins = 50): n, bins, patches = plt.hist(lens, n_bins, facecolor=&#39;blue&#39;, alpha=0.9) plt.show() . . print(f&#39;Mean: {mean(lens)}, Median: {median(lens)}, Standard Deviation: {stdev(lens)}, 90th Percentile: {np.percentile(lens, 100)}&#39;) plot_hist(lens) . Mean: 150.01203744984397, Median: 141.0, Standard Deviation: 44.59412209701778, 90th Percentile: 513.0 . Let&#39;s get our data into a format that we can feed into our model using Pytorch&#39;s Dataset and Dataloader API. All these methods do are convert our dataframes where we have multiple historical dialog, i.e., context, and a response, into a single conversation string that is separated a special token that tells our model when a person is finished speaking. . These conversation strings are then tokenized using HuggingFace&#39;s awesome tokenizers into their numerical representation that our model actual understands! . def construct_conv(row, tokenizer, eos = True): # from: https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists flatten = lambda l: [item for sublist in l for item in sublist] conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row])) conv = flatten(conv) return conv class ConversationDataset(Dataset): def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512): block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence) directory = args.cache_dir cached_features_file = os.path.join( directory, args.model_type + &quot;_cached_lm_&quot; + str(block_size) ) if os.path.exists(cached_features_file) and not args.overwrite_cache: logger.info(&quot;Loading features from cached file %s&quot;, cached_features_file) with open(cached_features_file, &quot;rb&quot;) as handle: self.examples = pickle.load(handle) else: logger.info(&quot;Creating features from dataset file at %s&quot;, directory) self.examples = [] for _, row in df.iterrows(): conv = construct_conv(row, tokenizer) if len(conv) &gt; block_size: continue self.examples.append(conv) # Note that we are loosing the last truncated example here for the sake of simplicity (no padding) # If your dataset is small, first you should loook for a bigger one :-) and second you # can change this behavior by adding (model specific) padding. logger.info(&quot;Saving features into cached file %s&quot;, cached_features_file) with open(cached_features_file, &quot;wb&quot;) as handle: pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL) def __len__(self): return len(self.examples) def __getitem__(self, item): return torch.tensor(self.examples[item], dtype=torch.long) . . Training and Evaluating . Now that we have THE DATA we can finally create our model and start training it! The training and evaluation loop are quite simple. We simplely take a batch of examples from our dataloader and use it both as our inputs and labels. We do this because GPT2 is an auto-regressive model, meaning it uses some context to predict the next token. This prediction is then added to the original context and fed back in as the new context for generating the next token. . To evaluate our model, we use the metric perplexity, which is a simple, but powerful metric. Perplexity is a measure of how unsure the model is in its choice of the next token. The more unsure our model is, the higher its perplexity. One fascinating thing about perplexity is that it correlates very well with what humans think of when it comes to coherent and specific natural conversations, which was shown in the amazing paper &quot;Towards a Human-like Open-Domain Chatbot&quot; by Daniel Adiwardana, et. al. . # Training of model def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -&gt; Tuple[int, float]: &quot;&quot;&quot; Train the model &quot;&quot;&quot; if args.local_rank in [-1, 0]: tb_writer = SummaryWriter() args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu) def collate(examples: List[torch.Tensor]): if tokenizer._pad_token is None: return pad_sequence(examples, batch_first=True) return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id) train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset) train_dataloader = DataLoader( train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True ) if args.max_steps &gt; 0: t_total = args.max_steps args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1 else: t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs model = model.module if hasattr(model, &quot;module&quot;) else model # Take care of distributed/parallel training model.resize_token_embeddings(len(tokenizer)) # add_special_tokens_(model, tokenizer) # Prepare optimizer and schedule (linear warmup and decay) no_decay = [&quot;bias&quot;, &quot;LayerNorm.weight&quot;] optimizer_grouped_parameters = [ { &quot;params&quot;: [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], &quot;weight_decay&quot;: args.weight_decay, }, {&quot;params&quot;: [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], &quot;weight_decay&quot;: 0.0}, ] optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon) scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total ) # Check if saved optimizer or scheduler states exist if ( args.model_name_or_path and os.path.isfile(os.path.join(args.model_name_or_path, &quot;optimizer.pt&quot;)) and os.path.isfile(os.path.join(args.model_name_or_path, &quot;scheduler.pt&quot;)) ): # Load in optimizer and scheduler states optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, &quot;optimizer.pt&quot;))) scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, &quot;scheduler.pt&quot;))) if args.fp16: try: from apex import amp except ImportError: raise ImportError(&quot;Please install apex from https://www.github.com/nvidia/apex to use fp16 training.&quot;) model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level) # multi-gpu training (should be after apex fp16 initialization) if args.n_gpu &gt; 1: model = torch.nn.DataParallel(model) # Distributed training (should be after apex fp16 initialization) if args.local_rank != -1: model = torch.nn.parallel.DistributedDataParallel( model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True ) # Train! logger.info(&quot;***** Running training *****&quot;) logger.info(&quot; Num examples = %d&quot;, len(train_dataset)) logger.info(&quot; Num Epochs = %d&quot;, args.num_train_epochs) logger.info(&quot; Instantaneous batch size per GPU = %d&quot;, args.per_gpu_train_batch_size) logger.info( &quot; Total train batch size (w. parallel, distributed &amp; accumulation) = %d&quot;, args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1), ) logger.info(&quot; Gradient Accumulation steps = %d&quot;, args.gradient_accumulation_steps) logger.info(&quot; Total optimization steps = %d&quot;, t_total) global_step = 0 epochs_trained = 0 steps_trained_in_current_epoch = 0 # Check if continuing training from a checkpoint if args.model_name_or_path and os.path.exists(args.model_name_or_path): try: # set global_step to gobal_step of last saved checkpoint from model path checkpoint_suffix = args.model_name_or_path.split(&quot;-&quot;)[-1].split(&quot;/&quot;)[0] global_step = int(checkpoint_suffix) epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps) steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps) logger.info(&quot; Continuing training from checkpoint, will skip to saved global_step&quot;) logger.info(&quot; Continuing training from epoch %d&quot;, epochs_trained) logger.info(&quot; Continuing training from global step %d&quot;, global_step) logger.info(&quot; Will skip the first %d steps in the first epoch&quot;, steps_trained_in_current_epoch) except ValueError: logger.info(&quot; Starting fine-tuning.&quot;) tr_loss, logging_loss = 0.0, 0.0 model.zero_grad() train_iterator = trange( epochs_trained, int(args.num_train_epochs), desc=&quot;Epoch&quot;, disable=args.local_rank not in [-1, 0] ) set_seed(args) # Added here for reproducibility for _ in train_iterator: epoch_iterator = tqdm(train_dataloader, desc=&quot;Iteration&quot;, disable=args.local_rank not in [-1, 0]) for step, batch in enumerate(epoch_iterator): # Skip past any already trained steps if resuming training if steps_trained_in_current_epoch &gt; 0: steps_trained_in_current_epoch -= 1 continue inputs, labels = (batch, batch) if inputs.shape[1] &gt; 1024: continue inputs = inputs.to(args.device) labels = labels.to(args.device) model.train() outputs = model(inputs, labels=labels) loss = outputs[0] # model outputs are always tuple in transformers (see doc) if args.n_gpu &gt; 1: loss = loss.mean() # mean() to average on multi-gpu parallel training if args.gradient_accumulation_steps &gt; 1: loss = loss / args.gradient_accumulation_steps if args.fp16: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() else: loss.backward() tr_loss += loss.item() if (step + 1) % args.gradient_accumulation_steps == 0: if args.fp16: torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm) else: torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm) optimizer.step() scheduler.step() # Update learning rate schedule model.zero_grad() global_step += 1 if args.local_rank in [-1, 0] and args.logging_steps &gt; 0 and global_step % args.logging_steps == 0: # Log metrics if ( args.local_rank == -1 and args.evaluate_during_training ): # Only evaluate when single GPU otherwise metrics may not average well results = evaluate(args, model, tokenizer) for key, value in results.items(): tb_writer.add_scalar(&quot;eval_{}&quot;.format(key), value, global_step) tb_writer.add_scalar(&quot;lr&quot;, scheduler.get_lr()[0], global_step) tb_writer.add_scalar(&quot;loss&quot;, (tr_loss - logging_loss) / args.logging_steps, global_step) logging_loss = tr_loss if args.local_rank in [-1, 0] and args.save_steps &gt; 0 and global_step % args.save_steps == 0: checkpoint_prefix = &quot;checkpoint&quot; # Save model checkpoint output_dir = os.path.join(args.output_dir, &quot;{}-{}&quot;.format(checkpoint_prefix, global_step)) os.makedirs(output_dir, exist_ok=True) model_to_save = ( model.module if hasattr(model, &quot;module&quot;) else model ) # Take care of distributed/parallel training model_to_save.save_pretrained(output_dir) tokenizer.save_pretrained(output_dir) torch.save(args, os.path.join(output_dir, &quot;training_args.bin&quot;)) logger.info(&quot;Saving model checkpoint to %s&quot;, output_dir) _rotate_checkpoints(args, checkpoint_prefix) torch.save(optimizer.state_dict(), os.path.join(output_dir, &quot;optimizer.pt&quot;)) torch.save(scheduler.state_dict(), os.path.join(output_dir, &quot;scheduler.pt&quot;)) logger.info(&quot;Saving optimizer and scheduler states to %s&quot;, output_dir) if args.max_steps &gt; 0 and global_step &gt; args.max_steps: epoch_iterator.close() break if args.max_steps &gt; 0 and global_step &gt; args.max_steps: train_iterator.close() break if args.local_rank in [-1, 0]: tb_writer.close() return global_step, tr_loss / global_step # Evaluation of some model def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=&quot;&quot;) -&gt; Dict: # Loop to handle MNLI double evaluation (matched, mis-matched) eval_output_dir = args.output_dir eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True) os.makedirs(eval_output_dir, exist_ok=True) args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu) # Note that DistributedSampler samples randomly def collate(examples: List[torch.Tensor]): if tokenizer._pad_token is None: return pad_sequence(examples, batch_first=True) return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id) eval_sampler = SequentialSampler(eval_dataset) eval_dataloader = DataLoader( eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True ) # multi-gpu evaluate if args.n_gpu &gt; 1: model = torch.nn.DataParallel(model) # Eval! logger.info(&quot;***** Running evaluation {} *****&quot;.format(prefix)) logger.info(&quot; Num examples = %d&quot;, len(eval_dataset)) logger.info(&quot; Batch size = %d&quot;, args.eval_batch_size) eval_loss = 0.0 nb_eval_steps = 0 model.eval() for batch in tqdm(eval_dataloader, desc=&quot;Evaluating&quot;): inputs, labels = (batch, batch) inputs = inputs.to(args.device) labels = labels.to(args.device) with torch.no_grad(): outputs = model(inputs, labels=labels) lm_loss = outputs[0] eval_loss += lm_loss.mean().item() nb_eval_steps += 1 eval_loss = eval_loss / nb_eval_steps perplexity = torch.exp(torch.tensor(eval_loss)) result = {&quot;perplexity&quot;: perplexity} output_eval_file = os.path.join(eval_output_dir, prefix, &quot;eval_results.txt&quot;) with open(output_eval_file, &quot;w&quot;) as writer: logger.info(&quot;***** Eval results {} *****&quot;.format(prefix)) for key in sorted(result.keys()): logger.info(&quot; %s = %s&quot;, key, str(result[key])) writer.write(&quot;%s = %s n&quot; % (key, str(result[key]))) return result . . Now let&#39;s put it all together into our runner function and let our baby cook away! . # Main show runner def main(df_trn, df_val): args = Args() if args.should_continue: sorted_checkpoints = _sorted_checkpoints(args) if len(sorted_checkpoints) == 0: raise ValueError(&quot;Used --should_continue but no checkpoint was found in --output_dir.&quot;) else: args.model_name_or_path = sorted_checkpoints[-1] if ( os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir and not args.should_continue ): raise ValueError( &quot;Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.&quot;.format( args.output_dir ) ) # Setup CUDA, GPU &amp; distributed training device = torch.device(&quot;cuda&quot;) args.n_gpu = torch.cuda.device_count() args.device = device # Setup logging logging.basicConfig( format=&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;, datefmt=&quot;%m/%d/%Y %H:%M:%S&quot;, level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN, ) logger.warning( &quot;Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s&quot;, args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16, ) # Set seed set_seed(args) config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir) tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir) model = AutoModelWithLMHead.from_pretrained( args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir, ) model.to(args.device) logger.info(&quot;Training/evaluation parameters %s&quot;, args) # Training if args.do_train: train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False) global_step, tr_loss = train(args, train_dataset, model, tokenizer) logger.info(&quot; global_step = %s, average loss = %s&quot;, global_step, tr_loss) # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained() if args.do_train: # Create output directory if needed os.makedirs(args.output_dir, exist_ok=True) logger.info(&quot;Saving model checkpoint to %s&quot;, args.output_dir) # Save a trained model, configuration and tokenizer using `save_pretrained()`. # They can then be reloaded using `from_pretrained()` model_to_save = ( model.module if hasattr(model, &quot;module&quot;) else model ) # Take care of distributed/parallel training model_to_save.save_pretrained(args.output_dir) tokenizer.save_pretrained(args.output_dir) # Good practice: save your training arguments together with the trained model torch.save(args, os.path.join(args.output_dir, &quot;training_args.bin&quot;)) # Load a trained model and vocabulary that you have fine-tuned model = AutoModelWithLMHead.from_pretrained(args.output_dir) tokenizer = AutoTokenizer.from_pretrained(args.output_dir) model.to(args.device) # Evaluation results = {} if args.do_eval and args.local_rank in [-1, 0]: checkpoints = [args.output_dir] if args.eval_all_checkpoints: checkpoints = list( os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + &quot;/**/&quot; + WEIGHTS_NAME, recursive=True)) ) logging.getLogger(&quot;transformers.modeling_utils&quot;).setLevel(logging.WARN) # Reduce logging logger.info(&quot;Evaluate the following checkpoints: %s&quot;, checkpoints) for checkpoint in checkpoints: global_step = checkpoint.split(&quot;-&quot;)[-1] if len(checkpoints) &gt; 1 else &quot;&quot; prefix = checkpoint.split(&quot;/&quot;)[-1] if checkpoint.find(&quot;checkpoint&quot;) != -1 else &quot;&quot; model = AutoModelWithLMHead.from_pretrained(checkpoint) model.to(args.device) result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix) result = dict((k + &quot;_{}&quot;.format(global_step), v) for k, v in result.items()) results.update(result) return results . . %load_ext tensorboard %tensorboard --logdir runs . Finally, we run our model! I found this can take anywhere from an hour to three hours depending on the GPU Google give to you to finish training a model that can sort of hold a coherent conversation for the Spanish language. If you are using a different language, you&#39;ll have to play around with how long to cook your model for. . main(trn_df, val_df) . Chatting with our Model . Now that we have our model trained, let&#39;s it out for a spin and have our first conversation with it! . In order to allow us to chitchat with our new bot we need to figure out when the model has finished its turn, i.e. when it has generated the [end_of_turn] token. When the model generates this token, we can switch back control of the conversation to the user so they can respond. Luckily, this is very easy to do with the Huggingface framework! . The below code is copied pretty much verbatim from the creators of the DialoGPT model, which you can find here. . tokenizer = AutoTokenizer.from_pretrained(&#39;microsoft/DialoGPT-small&#39;) model = AutoModelWithLMHead.from_pretrained(&#39;output&#39;) # Let&#39;s chat for 5 lines for step in range(6): # encode the new user input, add the eos_token and return a tensor in Pytorch new_user_input_ids = tokenizer.encode(input(&quot;&gt;&gt; User:&quot;) + tokenizer.eos_token, return_tensors=&#39;pt&#39;) # print(new_user_input_ids) # append the new user input tokens to the chat history bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids # generated a response while limiting the total chat history to 1000 tokens, chat_history_ids = model.generate( bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id, top_p=0.92, top_k = 50 ) # pretty print last ouput tokens from bot print(&quot;DialoGPT: {}&quot;.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True))) . 05/13/2020 00:27:10 - INFO - filelock - Lock 139706162979168 acquired on /root/.cache/torch/transformers/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5.lock 05/13/2020 00:27:10 - INFO - transformers.file_utils - https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpkhif9g52 05/13/2020 00:27:10 - INFO - transformers.file_utils - storing https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/config.json in cache at /root/.cache/torch/transformers/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5 05/13/2020 00:27:10 - INFO - transformers.file_utils - creating metadata file for /root/.cache/torch/transformers/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5 05/13/2020 00:27:10 - INFO - filelock - Lock 139706162979168 released on /root/.cache/torch/transformers/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5.lock 05/13/2020 00:27:10 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/config.json from cache at /root/.cache/torch/transformers/c3a09526c725b854c685b72cf60c50f1fea9b0e4d6227fa41573425ef4bd4bc6.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5 05/13/2020 00:27:10 - INFO - transformers.configuration_utils - Model config GPT2Config { &#34;activation_function&#34;: &#34;gelu_new&#34;, &#34;architectures&#34;: [ &#34;GPT2LMHeadModel&#34; ], &#34;attn_pdrop&#34;: 0.1, &#34;bos_token_id&#34;: 50256, &#34;embd_pdrop&#34;: 0.1, &#34;eos_token_id&#34;: 50256, &#34;initializer_range&#34;: 0.02, &#34;layer_norm_epsilon&#34;: 1e-05, &#34;model_type&#34;: &#34;gpt2&#34;, &#34;n_ctx&#34;: 1024, &#34;n_embd&#34;: 768, &#34;n_head&#34;: 12, &#34;n_layer&#34;: 12, &#34;n_positions&#34;: 1024, &#34;resid_pdrop&#34;: 0.1, &#34;summary_activation&#34;: null, &#34;summary_first_dropout&#34;: 0.1, &#34;summary_proj_to_labels&#34;: true, &#34;summary_type&#34;: &#34;cls_index&#34;, &#34;summary_use_proj&#34;: true, &#34;vocab_size&#34;: 50257 } 05/13/2020 00:27:10 - INFO - transformers.tokenization_utils - Model name &#39;microsoft/DialoGPT-small&#39; not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming &#39;microsoft/DialoGPT-small&#39; is a path, a model identifier, or url to a directory containing tokenizer files. . . 05/13/2020 00:27:11 - INFO - filelock - Lock 139706164883072 acquired on /root/.cache/torch/transformers/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock 05/13/2020 00:27:11 - INFO - transformers.file_utils - https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpaeb7ikva 05/13/2020 00:27:12 - INFO - transformers.file_utils - storing https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/vocab.json in cache at /root/.cache/torch/transformers/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71 05/13/2020 00:27:12 - INFO - transformers.file_utils - creating metadata file for /root/.cache/torch/transformers/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71 05/13/2020 00:27:12 - INFO - filelock - Lock 139706164883072 released on /root/.cache/torch/transformers/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock . . 05/13/2020 00:27:12 - INFO - filelock - Lock 139706162979168 acquired on /root/.cache/torch/transformers/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock 05/13/2020 00:27:12 - INFO - transformers.file_utils - https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp4k0b0lt0 05/13/2020 00:27:13 - INFO - transformers.file_utils - storing https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/merges.txt in cache at /root/.cache/torch/transformers/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda 05/13/2020 00:27:13 - INFO - transformers.file_utils - creating metadata file for /root/.cache/torch/transformers/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda 05/13/2020 00:27:13 - INFO - filelock - Lock 139706162979168 released on /root/.cache/torch/transformers/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock . . 05/13/2020 00:27:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/vocab.json from cache at /root/.cache/torch/transformers/78725a31b87003f46d5bffc3157ebd6993290e4cfb7002b5f0e52bb0f0d9c2dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71 05/13/2020 00:27:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/merges.txt from cache at /root/.cache/torch/transformers/570e31eddfc57062e4d0c5b078d44f97c0e5ac48f83a2958142849b59df6bbe6.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda 05/13/2020 00:27:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/added_tokens.json from cache at None 05/13/2020 00:27:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/special_tokens_map.json from cache at None 05/13/2020 00:27:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-small/tokenizer_config.json from cache at None 05/13/2020 00:27:14 - INFO - transformers.configuration_utils - loading configuration file output/config.json 05/13/2020 00:27:14 - INFO - transformers.configuration_utils - Model config GPT2Config { &#34;activation_function&#34;: &#34;gelu_new&#34;, &#34;architectures&#34;: [ &#34;GPT2LMHeadModel&#34; ], &#34;attn_pdrop&#34;: 0.1, &#34;bos_token_id&#34;: 50256, &#34;embd_pdrop&#34;: 0.1, &#34;eos_token_id&#34;: 50256, &#34;initializer_range&#34;: 0.02, &#34;layer_norm_epsilon&#34;: 1e-05, &#34;model_type&#34;: &#34;gpt2&#34;, &#34;n_ctx&#34;: 1024, &#34;n_embd&#34;: 768, &#34;n_head&#34;: 12, &#34;n_layer&#34;: 12, &#34;n_positions&#34;: 1024, &#34;resid_pdrop&#34;: 0.1, &#34;summary_activation&#34;: null, &#34;summary_first_dropout&#34;: 0.1, &#34;summary_proj_to_labels&#34;: true, &#34;summary_type&#34;: &#34;cls_index&#34;, &#34;summary_use_proj&#34;: true, &#34;vocab_size&#34;: 50257 } 05/13/2020 00:27:14 - INFO - transformers.modeling_utils - loading weights file output/pytorch_model.bin . &gt;&gt; User:Hola. DialoGPT: ¿Qué estás haciendo? &gt;&gt; User:Estoy leyendo un libro nuevo. DialoGPT: ¿Qué leyendo? &gt;&gt; User:Se llama &#34;The Witcher.&#34; DialoGPT: ¿Qué quieres decir? &gt;&gt; User:Es un libro sobre magia y monstruos. DialoGPT: ¿Qué quieres decir? &gt;&gt; User:¿Te gusta libros? DialoGPT: ¿Qué te pasa? &gt;&gt; User:Nada mucho. DialoGPT: ¿Por qué no me lo dijiste? . Now, it ain&#39;t the best, however, training it for longer or using the DialogGPT-medium instead of DialogGPT-small does improve results, at least in my experiments. I decided to only include the DialogGPT-small in this tutorial due to the limited (But still AMAZING) resources of Google Colab. I&#39;ve went ahead and trained a bigger DialogGPT-medium model for longer and have uploaded it to Huggingface for anyone to try out! Find the model card here. . Conclusion . In this tutorial, you learned how to train an Open-Dialog chatbot in any language we want to practice with! This involved learning about the amazing transformers library by Huggingface that has seen a lot of popularity recently. You&#39;ve also learned what an Open-Dialog chatbot is and some of the difficulties that come with training them such as constructing training examples and generating repetitive text. . This is just part one in what I am hoping will be a three part series! In the next part, we will take our model and integrate it into a web app using the awesome Streamlit library. Finally, part three will then be generating an Android application for chatting with your new language companion! . PS . If you do train a new chatbot for a language of your interest, please share it! I&#39;d love to hear about your progress with it and I&#39;m sure others would be also interested in it as these models can be quite expensive to train. . If you want an ease way to share it, I suggest submitting your trained model to Huggingface&#39;s model zoo, where others can view and download your model to use as a starting point for their applications! Here is a simple way for taking the model trained in this tutorial and uploading it to Hugginface&#39;s website following the instructions on the Huggingface website: . First make sure you have a Huggingface account: https://huggingface.co/join. Next Run the following code snippets and that&#39;s it! . ! rm -rf output/checkpoint-* ! mv output &lt;name_of_model&gt; . ! transformers-cli login # log in using the same credentials as on huggingface.co . ! transformers-cli upload &lt;name_of_model&gt; .",
            "url": "https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html",
            "relUrl": "/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "What I Learned (WIL) Neuroscience Month [Part 1]",
            "content": "If you are like me, you may have thought there was quite a lot of mystery going on in your brain that the scientific community has yet to really figure out and understand. However, through my month long study I found that neuroscience has some powerful computational models and experiments that are able to explain many of the processes going on in the brain such as the visual system, how we store memories, and the process we call intuition. Of course there are still unanswered questions such as what consciousness is and how are we able to learn things with just a few examples, but I was shocked how much is known. . I will be going through some of the amazing things that are happening in that brain of yours through this series. This first part is devoted to the “High Level” stuff, which involves the more complicated behavior such as how different parts of your brain contribute to your conscious experience, how you are able to solve seemly difficult problems with ease, and how this intuition of yours probably contributes to many of the logical errors you make. Let’s get right into it! . High Level Stuff . System 1 and System 2 . In the amazing book “Thinking Fast and Slow” by Daniel Kahneman, who is a world renowned economist and neuroscientist, the high level ways in which we “think” are examined. In this book, things like decisions, problem solving, emotional states, and why we make errors in our logic, both consciously and subconsciously, are shown. I highly recommend this book, I personally have been enjoying the Audiobook version! . . Overview of the two different Systems that make up your thinking. . Daniel Kahneman introduced the idea of two systems of thought, which he named very creatively System 1 and System 2. System 1 is more associated with intuition or fast thinking that processes things without much mental strain such as looking at a picture of a cat and understanding that what you are currently looking at is in fact a cat. System 1 is also automatic in the sense that you have no control over coming to the realization that a cat is in the picture. System 2, on the other hand, is involved in more deliberate and difficult processing that requires you to put in work for solving some task such as calculating that the multiplication of 18 and 32 is equal to 576. don’t worry, I’ll wait :). . . A Cat: Image by Free-Photos from Pixabay . Yoshua Bengio, who is a pioneer of artificial intelligence and recent receiver of the Turing award along with other amazing scientists in the field, put it quite well on how these two systems work together. System 1 is great at generating representations of things and associating them with high level objects such as cats, words, and concepts. These representations are then exploited by System 2 to avoid all of the nitty gritty details of what it means for a cat to be a cat. It instead uses these concepts to do interesting things such as finding relationships between multiple objects or performing complex calculations by planning out a series of operations to perform, i.e. generating and following an algorithm for multiplying 2 numbers. . Daniel Kahneman also mentions many fun experiments to show how System 1 is what runs most of your day to day experiences and how this leads to a lot of logical errors. One famous experiment is the bat and ball experiment. Try and solve the following problem: . The cost of a bat and ball come to a total of 1 dollar and 10 cents. If the bat costs 1 dollar more than the ball, how much does the ball cost? . If you guessed the ball costs 10 cents like I did, you were wrong and you were wrong because your System 2 is very, very lazy. A simple calculation will show that if the ball cost 10 cents and the bat is 1 dollar more, the bat would be 1 dollar and 10 cents bring the total up to 1 dollar and 20 cents. . The above example and many more show a common theme in how our minds work. System 1 is at fault here, it recognized some similarity in the problem and automatically offered up what seemed like a reasonable answer to the problem. However, it never even tried to check if the answer was correct. This checking is done by System 2, but System 1 was so sure of itself that System 2 did not even bother to check its answer because it takes work to verify. System 2 is involved in slow and deliberate thinking such as performing calculations or having to consider multiple pieces of information when making a decision. . One interesting effect that occurs when people are actively using their System 2 to solve a task is that it loosen their inhibitions. The study described in “Thinking Fast and Slow” showed that people who were asked during an experiment whether they would like a sweet treat such as a slice of chocolate cake or a salad were more likely to choose the chocolate cake if they were also given the task of keeping 7 digits in their mind for a few minutes. It has also been shown that performing System 2 style tasks has an impact on the way people behave such as with increased selfishness and even increased use of sexist language. This is all due to System 2 style tasks requiring focus and attention on the task at hand allowing for System 1, who is known to make quick judgement calls without much thought, to take over any additional tasks you are not focusing on. . Our daily lives and thought process circulate around when to use System 2 for tasks that System 1 has no way of generating a good answer for. You may ask why we do not use System 2 for more tasks, but as discussed previously System 2 is slow. It would be quite dangerous if we relied on System 2 to solve how to avoid crashing into a vehicle on the highway that unexpectedly started merging into your lane without a turn signal or to duck to avoid being hit by a fast ball that’s outside of the batter’s box. An interesting feature, however, of System 2 style tasks is that if they are seen often enough, they can be upgraded to System 1 style tasks by having System 1 learn to recognize the expected answer. This is a common occurrence for driving around an unfamiliar path. The first few times you drive your System 2 is more attentive to make sure you get to where you are going without getting lost. Eventually, if you take the same path, it becomes familiar and your System 1 is able to take over allowing you to reach your destination without much attention being paid to the path you are taking. . Brain Structure . Now, your brain does not actually have these System 1 and System 2 structures physically, they are just a great way of discussing the way in which your brain works. However, the real way your brain is structured is a lot more messy, but still beautiful :). I found the explanation and structure of how your brain is wired in “Computational Cognitive Neuroscience” by Munakata et. al. to be a great resource for learning and so I will be using it as my main source for the rest of this article. While it is a lot denser than “Thinking Fast and Slow,” it has great visualizations and goes quite in depth. . . Image by Oberholster Venita from Pixabay (Modified to have labels) . Your brain can be organized into two main parts, the Neocortex, which is what most people think of when they imagine a brain, and the Cerebellum, which is not as well known by most but is believed to play a big part in how we think. The Neocortex is the coloured part in the above image and it can be roughly broken down into 4 main lobes, but each lobe is heavily dependent on the others, so don’t think of them as truly distinct sections. You can see the responsibilities of each lobe in the table below. . . Overall responsibilities of the different lobes of the brain. . One of the most interesting parts of your brain structure is how representations of your senses like sight and sound are built up in a hierarchical fashion as your brain moves the information across the different lobes. Take your ability to easily understand what your eyes are currently seeing. This information is first processed by the Occipital lobe hierarchically at the very back of the brain, starting with identifying simple edges and then moving on to groups of edges to form basic shapes. This representation grows in complexity as the information is sent towards the other parts of the brain like the Temporal lobe where these representations are given semantic meaning in the form of words such as Cat or Human or even more specific such as Garfield the Cat. If we track the representation that your brain is building of what you are seeing into the Parietal lobe, you will find your brain generating representations for relations between the different objects in the scene such as the Cat is hanging from a tree (hang in there buddy!). Lastly, the Frontal lobe takes all these high level representations to perform any number of high level decisions and motor control movement such as petting the kitty :). . As you can see, even this very simple example of just processing what your eyes see involves multiple parts of your brain, even if a lot of the initial work is done by the Occipital lobe. Each lobe does its part in helping generate an understanding of the sensory input you are receiving. This is not just limited to vision, but could be any other type of senses such as smell, touch, and hearing. It’s all connected… . . Pepe Silvia from It’s Always Sunny in Philadelphia . Conclusions . Well, sadly that is all we are going to spend on the high level stuff of the brain. There are tons of additional stuff I could discuss about the high level part of your brain such as how the Cerebellum actually has half of all the neurons in your entire brain! And how much of its functions are not well understood because it seems to have its hands in processing everything! I could also discuss how there are different types of ways different parts of your brain represent input such as clusterization, hashing, and composition of different representations into new representations. However, there is a lot and we have already covered a lot of the super cool stuff! I hope you have enjoyed this first part of the Neuroscience series and have come away with a better or at least more confusing :) sense of how your brain works! . I will be working to get the next two parts (Mid Level and Low Level) out soon as there is not much else for me to do during this COVID-19 quarantine (don’t tell my Ph.D. advisor I said that). If you have any questions or have any comments about any interesting stuff you know about the brain please comment down below, I’d love to hear it! .",
            "url": "https://nathancooper.io/i-am-a-nerd/wil/neuroscience/2020/04/07/neuroscience-1.html",
            "relUrl": "/wil/neuroscience/2020/04/07/neuroscience-1.html",
            "date": " • Apr 7, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "How to Create an Automatic Code Comment Generator using Deep Learning!",
            "content": "About . In this post, you will be create a Deep Learning model that given a piece of code, will automatically generate a comment describing (hopefully 🤞) what the piece of code does. This post will focus on Java code, however, the same approach should be able to be applied to other programming languages such as Python or Javascript. . Collecting, preparing and exploring the data . You will be using the CodeSearchNet Challenge dataset from GitHub as it provides a large collection of clean code in multiple different languages. They have a really nice example on how to download and read in the data in their repo that you&#39;ll use to get started. . ! wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip ! unzip java.zip . --2020-03-07 16:37:37-- https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.179.149 Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.179.149|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1060569153 (1011M) [application/zip] Saving to: ‘java.zip’ java.zip 100%[===================&gt;] 1011M 88.0MB/s in 12s 2020-03-07 16:37:49 (84.9 MB/s) - ‘java.zip’ saved [1060569153/1060569153] Archive: java.zip creating: java/ creating: java/final/ creating: java/final/jsonl/ creating: java/final/jsonl/train/ inflating: java/final/jsonl/train/java_train_12.jsonl.gz inflating: java/final/jsonl/train/java_train_9.jsonl.gz inflating: java/final/jsonl/train/java_train_3.jsonl.gz inflating: java/final/jsonl/train/java_train_5.jsonl.gz inflating: java/final/jsonl/train/java_train_7.jsonl.gz inflating: java/final/jsonl/train/java_train_1.jsonl.gz inflating: java/final/jsonl/train/java_train_10.jsonl.gz inflating: java/final/jsonl/train/java_train_14.jsonl.gz inflating: java/final/jsonl/train/java_train_0.jsonl.gz inflating: java/final/jsonl/train/java_train_6.jsonl.gz inflating: java/final/jsonl/train/java_train_8.jsonl.gz inflating: java/final/jsonl/train/java_train_15.jsonl.gz inflating: java/final/jsonl/train/java_train_2.jsonl.gz inflating: java/final/jsonl/train/java_train_4.jsonl.gz inflating: java/final/jsonl/train/java_train_13.jsonl.gz inflating: java/final/jsonl/train/java_train_11.jsonl.gz creating: java/final/jsonl/test/ inflating: java/final/jsonl/test/java_test_0.jsonl.gz creating: java/final/jsonl/valid/ inflating: java/final/jsonl/valid/java_valid_0.jsonl.gz inflating: java_dedupe_definitions_v2.pkl inflating: java_licenses.pkl . jsonl_list_to_dataframe method is directly from the CodeSearchNet Challenge example code and get_dfs is just a helper for you to properly grab the data into the correct training, validation, and testing splits. Let&#39;s see what your data looks like :D! . def jsonl_list_to_dataframe(file_list, columns=None): &quot;&quot;&quot;Load a list of jsonl.gz files into a pandas DataFrame.&quot;&quot;&quot; return pd.concat([pd.read_json(f, orient=&#39;records&#39;, compression=&#39;gzip&#39;, lines=True)[columns] for f in file_list], sort=False) def get_dfs(path): &quot;&quot;&quot;Grabs the different data splits and converts them into dataframes&quot;&quot;&quot; dfs = [] for split in [&quot;train&quot;, &quot;valid&quot;, &quot;test&quot;]: files = sorted((path/split).glob(&quot;**/*.gz&quot;)) df = jsonl_list_to_dataframe(files, [&quot;code&quot;, &quot;docstring&quot;]) dfs.append(df) return dfs df_trn, df_val, df_tst = get_dfs(path/&quot;java/final/jsonl&quot;) df_trn.head() . . code docstring . 0 protected final void bindIndexed(Configuration... | Bind indexed elements to the supplied collecti... | . 1 public void setServletRegistrationBeans( n t t... | Set {@link ServletRegistrationBean}s that the ... | . 2 public void addServletRegistrationBeans( n t t... | Add {@link ServletRegistrationBean}s for the f... | . 3 public void setServletNames(Collection&lt;String&gt;... | Set servlet names that the filter will be regi... | . 4 public void addServletNames(String... servletN... | Add servlet names for the filter. n@param serv... | . You are going to only use a small subset of the data in order to train your model in a reasonable time. However, if you want to adjust the amount of data used you can just adjust the sample size. . sample = 0.2 df_trn = df_trn.sample(frac = sample) df_val = df_val.sample(frac = sample) df_tst = df_tst.sample(frac = sample) . Awesome! Now that you have the data, there&#39;s a few other preprocessing steps you need to perform. First we are going to remove any non-english comments. Next, you will also remove the JavaDocs, i.e., any line with an @ symbol or curly braces, as that will significantlly lessen the amount of learning your model will have to do. This also works out well since the JavaDoc syntax can usually be autogenerated from the method&#39;s signature. . # From https://stackoverflow.com/a/27084708/5768407 def isASCII(s): try: s.encode(encoding=&#39;utf-8&#39;).decode(&#39;ascii&#39;) except UnicodeDecodeError: return False else: return True df_trn = df_trn[df_trn[&#39;docstring&#39;].apply(lambda x: isASCII(x))] df_val = df_val[df_val[&#39;docstring&#39;].apply(lambda x: isASCII(x))] df_tst = df_tst[df_tst[&#39;docstring&#39;].apply(lambda x: isASCII(x))] . . def filter_jdocs(df): methods = [] comments = [] for i, row in progress_bar(list(df.iterrows())): comment = row[&quot;docstring&quot;] # Remove {} text in comments from https://stackoverflow.com/questions/14596884/remove-text-between-and-in-python/14598135 comment = re.sub(&quot;([ { []).*?([ ) }])&quot;, &#39;&#39;, comment) cleaned = [] for line in comment.split(&#39; n&#39;): if &quot;@&quot; in line: break cleaned.append(line) comments.append(&#39; n&#39;.join(cleaned)) methods.append(row[&quot;code&quot;]) new_df = pd.DataFrame(zip(methods, comments), columns = [&quot;code&quot;, &quot;docstring&quot;]) return new_df df_trn = filter_jdocs(df_trn); df_val = filter_jdocs(df_val); df_tst = filter_jdocs(df_tst); . . Now you are going to remove any empty comments or duplicate comments for your datasets. . df_trn = df_trn[~(df_trn[&#39;docstring&#39;] == &#39;&#39;)] df_val = df_val[~(df_val[&#39;docstring&#39;] == &#39;&#39;)] df_tst = df_tst[~(df_tst[&#39;docstring&#39;] == &#39;&#39;)] . df_trn = df_trn[~df_trn[&#39;docstring&#39;].duplicated()] df_val = df_val[~df_val[&#39;docstring&#39;].duplicated()] df_tst = df_tst[~df_tst[&#39;docstring&#39;].duplicated()] . Not bad, still leaves you with plenty of data to learn with! . len(df_trn), len(df_val), len(df_tst) . (73755, 2427, 4615) . Exploring your data! . As a good machine learning practitioner, it is extremely important to be careful with your data. This includes checking for biases, duplicates, and also describing the data that you have. Not doing so is setting you up for disaster. I have personally experienced such travesty when working on one of my own research projects where I forgot to check for duplicates before splitting my data. Sadly for me and all my restless nights working on the project, the data was full of duplicates and so my test set was contaminated with data points from my training set, which lead to inflated evaluation metrics :(. . Always explore your data! . You&#39;ll be doing some basic descriptive statistics for this Exploratory Data Analysis (EDA), which just means calculating some means, medians, and standard deviations for different views of your data. The first view you will be exploring is the tokens that make up your code and comments. To tokenize your data into these tokens you will use something called Byte Pair Encoding, which has shown great results for tokenizing both natural language and code as shown in Karampatsis and Sutton&#39;s paper &quot;Maybe Deep Neural Networks are the Best Choice for Modeling Source Code.&quot; . A great resource on learning more about how Byte Pair Encoding works is this blog post by Akashdeep Singh Jaswal and this Youtube video by Christopher Manning. Specifically, you will be using the awesome library by Google called sentencepiece. . def df_to_txt_file(df, output, col): &quot;&quot;&quot;Converts a dataframe and converts it into a text file that SentencePiece can use to train a BPE model&quot;&quot;&quot; with open(output/&#39;text.txt&#39;, &#39;w&#39;) as f: f.write(&#39; n&#39;.join(list(df[col]))) return output/&#39;text.txt&#39; def gen_sp_model(df, output, tokenizer_name, col): &quot;&quot;&quot;Trains a SentencePiece BPE model from a pandas dataframe&quot;&quot;&quot; fname = df_to_txt_file(df, output, col) sp.SentencePieceTrainer.train(f&#39;--input={fname} --model_prefix={output / tokenizer_name} --hard_vocab_limit=false&#39;) . . To use Byte Pair Encoding, you have to train the tokenizer on your data. However, no need to train your BPE on all of your data, so you will just be doing it on a subset (10%) of your training data. You are picking to train the BPE model from your training set to not perform any inadvertant data snooping by biasing your BPE model to tokenize more common words in your validation or testing set. This also will help show that you are indeed solving the out of vocabulary problem because you will most likely encounter words in your testing set that were not in your training set. . p_bpe = 0.1 method_tokenizer = &quot;method_bpe&quot; gen_sp_model(df_trn.sample(frac = p_bpe), path, method_tokenizer, col = &quot;code&quot;) comment_tokenizer = &quot;comment_bpe&quot; gen_sp_model(df_trn.sample(frac = p_bpe), path, comment_tokenizer, col = &quot;docstring&quot;) . Now that you have the ability to tokenize your text, let us explore! First you will just generate the frequency of each of your tokens and while you are at it, let&#39;s collect how long your methods are by via the common software metric Lines of Code (LOC). . def get_counter_and_lens(df, spm, col): toks = [] locs = [] for i, row in progress_bar(list(df.iterrows())): toks.extend(spm.EncodeAsPieces(row[col])) locs.append(len(row[col].split(&#39; n&#39;))) cnt = Counter() for tok in progress_bar(toks): cnt[tok] += 1 return list(map(len, toks)), cnt, locs code_lens, code_cnt, locs = get_counter_and_lens(df_trn, method_spm, &#39;code&#39;) comment_lens, comment_cnt, _ = get_counter_and_lens(df_trn, method_spm, &#39;docstring&#39;) . . def plot_counts(counts, top_k = 30): labels, values = zip(*counts.most_common()[:top_k]) indexes = np.arange(len(labels)) width = 1 plt.figure(num=None, figsize=(22, 4), dpi=60, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.bar(indexes, values, width) plt.xticks(indexes + width * 0.5, labels) plt.show() plot_counts(code_cnt, top_k = 30) plot_counts(comment_cnt, top_k = 30) . . Plotting your frequencies as a bar chart you start to see a nice picture of your data. Not that suprising, but the most common token happens to be the period and some other common syntactical tokens like curly braces and also key words like if and return. . def plot_hist(lens, n_bins = 50): n, bins, patches = plt.hist(lens, n_bins, facecolor=&#39;blue&#39;, alpha=0.9) plt.show() print(mean(code_lens), median(code_lens), stdev(code_lens)) plot_hist(code_lens) print(mean(locs), median(locs), stdev(locs)) plot_hist(locs) print(mean(comment_lens), median(comment_lens), stdev(comment_lens)) plot_hist(comment_lens) . . 3.4662519750610943 3.0 2.6490695431339177 . 18.54957629991187 10 50.99032748692644 . 3.57512896650546 3.0 2.605938655784157 . As you can see, there are HTML elements left with &lt; and &gt; occurring quite often in your comments dataset that may make it harder for your model to learn to generate the comments that contain those elements. Luckily for us, it won&#39;t really affect your models accuracy, but exploring your data like this does allow us to see how your data may be influencing your model. . TODO For You: Perform some further cleaning steps to remove HTML and any other cleaning you deem necessary and see how your performance changes. . Loading the data using FastAI . Now that you have the data processed and cleaned you need a way to get it into the format that FastAI uses. To do that you will use some code from Rachel Thomas&#39; awesome course on NLP, which allows us to create a Sequence to Sequence (since you are going from the sequence of code to the sequence of the code&#39;s docstring) DataBunch (this is just the format FastAI uses for managing loading the data into memory for training and evaluating). . def seq2seq_collate(samples, pad_idx=1, pad_first=True, backwards=False): &quot;Function that collect samples and adds padding. Flips token order if needed&quot; samples = to_data(samples) max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples]) res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx if backwards: pad_first = not pad_first for i,s in enumerate(samples): if pad_first: res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1]) else: res_x[i, :len(s[0])],res_y[i, :len(s[1])] = LongTensor(s[0]),LongTensor(s[1]) if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1) return res_x, res_y class Seq2SeqDataBunch(TextDataBunch): &quot;Create a `TextDataBunch` suitable for training an RNN classifier.&quot; @classmethod def create(cls, train_ds, valid_ds, test_ds=None, path=&#39;.&#39;, bs=32, val_bs=None, pad_idx=1, dl_tfms=None, pad_first=False, device=None, no_check=False, backwards=False, **dl_kwargs): &quot;Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`&quot; datasets = cls._init_ds(train_ds, valid_ds, test_ds) val_bs = ifnone(val_bs, bs) collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards) train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2) train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs) dataloaders = [train_dl] for ds in datasets[1:]: lengths = [len(t) for t in ds.x.items] sampler = SortSampler(ds.x, key=lengths.__getitem__) dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs)) return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check) class Seq2SeqTextList(TextList): _bunch = Seq2SeqDataBunch _label_cls = TextList . . Here is where you are telling FastAI to use your trained BPE models for tokenizing your data. FastAI&#39;s tokenizers will also do some additional processing of your text such as lower casing all words, removing repetitions, etc. You can find a full list of the processing FastAI uses here. . method_processor = SPProcessor( sp_model = path / (method_tokenizer + &quot;.model&quot;), sp_vocab = path / (method_tokenizer + &quot;.vocab&quot;), include_eos = True) comment_processor = SPProcessor( sp_model = path / (comment_tokenizer + &quot;.model&quot;), sp_vocab = path / (comment_tokenizer + &quot;.vocab&quot;), include_eos = True) . Now that you have your BPE model you will generate the DataBunches suitable for your task, which will be the Seq2Seq DataBunch. You will also filter out sequences that your too long so that you can fit everything onto a Google Colab GPU and to not have your training take too long. . def gen_dbs(df_trn, df_val, df_tst, method_processor, comment_processor, bs = 96, max_seq = 128): is_valid = [False] * len(df_trn) + [True] * len(df_val) df_merged = pd.concat([df_trn, df_val]) df_merged = pd.DataFrame(zip(df_merged[&quot;code&quot;].to_list(), df_merged[&quot;docstring&quot;].to_list(), is_valid), columns = [&quot;code&quot;, &quot;docstring&quot;, &quot;valid&quot;] ) db_trn = (Seq2SeqTextList .from_df(df_merged, path = path, cols=&#39;code&#39;, processor = method_processor) .split_from_df(col=&#39;valid&#39;) .label_from_df(cols=&#39;docstring&#39;, label_cls=TextList, processor = comment_processor) .filter_by_func(lambda x, y: len(x) &gt; max_seq or len(y) &gt; max_seq) .databunch(bs = bs)) db_tst = (Seq2SeqTextList .from_df(df_tst, path = path, cols=&#39;code&#39;, processor = method_processor) .split_by_rand_pct(valid_pct = 0.01) .label_from_df(cols=&#39;docstring&#39;, label_cls=TextList, processor = comment_processor) .filter_by_func(lambda x, y: len(x) &gt; max_seq or len(y) &gt; max_seq) .databunch(bs = 16)) return db_trn, db_tst db_trn, db_tst = gen_dbs(df_trn, df_val, df_tst, method_processor, comment_processor, bs = 96, max_seq = 128) db_trn.show_batch() . . text target . ▁ xx b os ▁boolean ▁res er ve ( int ▁column , ▁int ▁size ) ▁{ ▁if ▁( ( column ▁&lt; ▁0) ▁|| ▁( ( column ▁+ ▁size ) ▁&gt; ▁columns )) ▁throw ▁new ▁index out of bound s exception (&quot; res er ve ▁- ▁ inc or rec t ▁column ▁/ ▁size &quot;); ▁for ( int ▁i = column ; ▁i ▁&lt; ▁column ▁+ ▁size ; ▁i ++) ▁{ | ▁ xx b os ▁ xx ma j ▁re s er ve s ▁a ▁&lt; code &gt; ce ll &lt; ▁/ ▁ xx up ▁code &gt; ▁in ▁the ▁&lt; code &gt; ro w &lt; ▁/ ▁ xx up ▁code &gt; . ▁ xx e os | . ▁ xx b os ▁@ help ( ▁ help ▁= ▁&quot; get ▁all ▁the ▁ virtual network function descriptor ▁ xx m a j ▁dependency ▁of ▁a ▁network service descriptor ▁with ▁specific ▁id &quot; ▁) ▁public ▁list &lt; v n f d ep end en cy &gt; ▁get v n f dependencies ( final ▁ xx m a j ▁string ▁id ns d ) ▁throws ▁s d k exception ▁{ | ▁ xx b os ▁ xx ma j ▁return ▁a ▁ xx ma j ▁list ▁with ▁all ▁the ▁v n f de p end en c ies ▁that ▁are ▁contain ed ▁in ▁a ▁specific ▁network service descriptor . ▁ xx e os | . ▁ xx b os ▁@ override ▁public ▁void ▁delete as set and attachment s ( final ▁ xx m a j ▁string ▁as set id ) ▁throws ▁ io exception , ▁request failure exception ▁{ ▁ xx m a j ▁as set ▁as s ▁= ▁get un v er ified as set ( as set id ); ▁list &lt; attachment &gt; ▁attachment s ▁= ▁as s . get attachment s | ▁ xx b os ▁ xx ma j ▁this ▁will ▁delete ▁an ▁asset ▁and ▁all ▁its ▁attachments ▁ xx e os | . ▁ xx b os ▁public ▁list &lt; character book mark fold er s response &gt; ▁get character s character id book mark s fold er s ( integer ▁character id , ▁ xx m a j ▁string ▁data source , ▁ xx m a j ▁string ▁if n one match , ▁ xx m a j ▁ integer ▁page , ▁ xx m a j ▁string ▁token ) ▁throws ▁api | ▁ xx b os ▁ xx ma j ▁list ▁ bookmark ▁folders ▁a ▁list ▁of ▁your ▁character &amp; &#39; s ▁personal ▁ bookmark ▁folders ▁ ▁ xx ma j ▁this ▁route ▁is ▁cached ▁for ▁up ▁to ▁36 00 ▁seconds ▁ xx up ▁ s so ▁ xx ma j ▁scope : ▁ esi - bookmark s . read _ character _ bookmark s . v 1 ▁ xx e os | . ▁ xx b os ▁@ de pre c ated ▁protected ▁final ▁map &lt; db id , ▁k n n list &gt; ▁batch n n ( n ▁node , ▁db id s ▁ids , ▁int ▁k max ) ▁{ ▁map &lt; db id , ▁k n n list &gt; ▁res ▁= ▁new ▁hash map &lt;&gt;( id s . size ()); ▁for ( db id iter ▁iter ▁= ▁ids . iter (); | ▁ xx b os ▁ xx ma j ▁perform s ▁a ▁batch ▁k - ne a rest ▁neighbor ▁query ▁for ▁a ▁list ▁of ▁query ▁objects . ▁ xx e os | . def shift_tfm(b): x,y = b y = F.pad(y, (1, 0), value=1) return [x,y[:,:-1]], y[:,1:] # Add the necessary shift transformation for training your Transformer model db_trn.add_tfm(shift_tfm) db_tst.add_tfm(shift_tfm) . . Defining your model . In this example, you will be using the Transformer architecture that was developed by Vaswani et. al.. If you want a better understanding of this model, I highly suggest The Annotated Transformer blog post and the NLP course by Rachel Thomas, which this model code is copied from. . class PositionalEncoding(nn.Module): &quot;Encode the position with a sinusoid.&quot; def __init__(self, d): super().__init__() self.register_buffer(&#39;freq&#39;, 1 / (10000 ** (torch.arange(0., d, 2.)/d))) def forward(self, pos): inp = torch.ger(pos, self.freq) enc = torch.cat([inp.sin(), inp.cos()], dim=-1) return enc class TransformerEmbedding(nn.Module): &quot;Embedding + positional encoding + dropout&quot; def __init__(self, vocab_sz, emb_sz, inp_p=0.): super().__init__() self.emb_sz = emb_sz self.embed = embedding(vocab_sz, emb_sz) self.pos_enc = PositionalEncoding(emb_sz) self.drop = nn.Dropout(inp_p) def forward(self, inp): pos = torch.arange(0, inp.size(1), device=inp.device).float() return self.drop(self.embed(inp) * math.sqrt(self.emb_sz) + self.pos_enc(pos)) def feed_forward(d_model, d_ff, ff_p=0., double_drop=True): layers = [nn.Linear(d_model, d_ff), nn.ReLU()] if double_drop: layers.append(nn.Dropout(ff_p)) return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model)) class MultiHeadAttention(nn.Module): def __init__(self, n_heads, d_model, d_head=None, p=0., bias=True, scale=True): super().__init__() d_head = ifnone(d_head, d_model//n_heads) self.n_heads,self.d_head,self.scale = n_heads,d_head,scale self.q_wgt,self.k_wgt,self.v_wgt = [nn.Linear( d_model, n_heads * d_head, bias=bias) for o in range(3)] self.out = nn.Linear(n_heads * d_head, d_model, bias=bias) self.drop_att,self.drop_res = nn.Dropout(p),nn.Dropout(p) self.ln = nn.LayerNorm(d_model) def forward(self, q, kv, mask=None): return self.ln(q + self.drop_res(self.out(self._apply_attention(q, kv, mask=mask)))) def create_attn_mat(self, x, layer, bs): return layer(x).view(bs, x.size(1), self.n_heads, self.d_head ).permute(0, 2, 1, 3) def _apply_attention(self, q, kv, mask=None): bs,seq_len = q.size(0),q.size(1) wq,wk,wv = map(lambda o: self.create_attn_mat(*o,bs), zip((q,kv,kv),(self.q_wgt,self.k_wgt,self.v_wgt))) attn_score = wq @ wk.transpose(2,3) if self.scale: attn_score /= math.sqrt(self.d_head) if mask is not None: attn_score = attn_score.float().masked_fill(mask, -float(&#39;inf&#39;)).type_as(attn_score) attn_prob = self.drop_att(F.softmax(attn_score, dim=-1)) attn_vec = attn_prob @ wv return attn_vec.permute(0, 2, 1, 3).contiguous().view(bs, seq_len, -1) def get_output_mask(inp, pad_idx=1): return torch.triu(inp.new_ones(inp.size(1),inp.size(1)), diagonal=1)[None,None].byte() class EncoderBlock(nn.Module): &quot;Encoder block of a Transformer model.&quot; #Can&#39;t use Sequential directly cause more than one input... def __init__(self, n_heads, d_model, d_head, d_inner, p=0., bias=True, scale=True, double_drop=True): super().__init__() self.mha = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale) self.ff = feed_forward(d_model, d_inner, ff_p=p, double_drop=double_drop) def forward(self, x, mask=None): return self.ff(self.mha(x, x, mask=mask)) class DecoderBlock(nn.Module): &quot;Decoder block of a Transformer model.&quot; #Can&#39;t use Sequential directly cause more than one input... def __init__(self, n_heads, d_model, d_head, d_inner, p=0., bias=True, scale=True, double_drop=True): super().__init__() self.mha1 = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale) self.mha2 = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale) self.ff = feed_forward(d_model, d_inner, ff_p=p, double_drop=double_drop) def forward(self, x, enc, mask_out=None): return self.ff(self.mha2(self.mha1(x, x, mask_out), enc)) . . class Transformer(Module): def __init__(self, inp_vsz, out_vsz, n_layers=6, n_heads=8, d_model=256, d_head=32, d_inner=1024, p=0.1, bias=True, scale=True, double_drop=True, pad_idx=1): self.enc_emb = TransformerEmbedding(inp_vsz, d_model, p) self.dec_emb = TransformerEmbedding(out_vsz, d_model, 0.) args = (n_heads, d_model, d_head, d_inner, p, bias, scale, double_drop) self.encoder = nn.ModuleList([EncoderBlock(*args) for _ in range(n_layers)]) self.decoder = nn.ModuleList([DecoderBlock(*args) for _ in range(n_layers)]) self.out = nn.Linear(d_model, out_vsz) self.out.weight = self.dec_emb.embed.weight self.pad_idx = pad_idx def forward(self, inp, out): mask_out = get_output_mask(out, self.pad_idx) enc,out = self.enc_emb(inp),self.dec_emb(out) enc = compose(self.encoder)(enc) out = compose(self.decoder)(out, enc, mask_out) return self.out(out) . . To evaluate your model you will be using the commonly used BLEU score, which is a measure for determining how closely your model&#39;s generated comment is to the real comment of a method. (This code is also copied from the NLP tutorial from Rachel Thomas) . class NGram(): def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n def __eq__(self, other): if len(self.ngram) != len(other.ngram): return False return np.all(np.array(self.ngram) == np.array(other.ngram)) def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)])) def get_grams(x, n, max_n=5000): return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)] def get_correct_ngrams(pred, targ, n, max_n=5000): pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n) pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams) return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams) class CorpusBLEU(Callback): def __init__(self, vocab_sz): self.vocab_sz = vocab_sz self.name = &#39;bleu&#39; def on_epoch_begin(self, **kwargs): self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4 def on_batch_end(self, last_output, last_target, **kwargs): last_output = last_output.argmax(dim=-1) for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()): self.pred_len += len(pred) self.targ_len += len(targ) for i in range(4): c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz) self.corrects[i] += c self.counts[i] += t def on_epoch_end(self, last_metrics, **kwargs): precs = [c/t for c,t in zip(self.corrects,self.counts)] len_penalty = exp(1 - self.targ_len/self.pred_len) if self.pred_len &lt; self.targ_len else 1 bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25) return add_metrics(last_metrics, bleu) . . n_x_vocab, n_y_vocab = len(db_trn.train_ds.x.vocab.itos), len(db_trn.train_ds.y.vocab.itos) model = Transformer(n_x_vocab, n_y_vocab, d_model=256) learn = Learner(db_trn, model, metrics=[accuracy, CorpusBLEU(n_y_vocab)], loss_func = CrossEntropyFlat()) . Now you are going to use the awesome Learning Rate finder provided by FastAI, which is based on the awesome paper from Leslie N. Smith &quot;Cyclical Learning Rates for Training Neural Networks&quot;. This way you don&#39;t have to do a bunch of hyperparameter searching to find the perfect fit. . learn.lr_find() learn.recorder.plot(suggestion = True) . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss accuracy bleu time . &lt;progress value=&#39;92&#39; class=&#39;&#39; max=&#39;423&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 21.75% [92/423 01:21&lt;04:53 19.5654] &lt;/div&gt; &lt;/div&gt; LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. Min numerical gradient: 3.02E-03 Min loss divided by 10: 1.74E-02 . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; It is common to pick a point a bit before the suggested point. . max_lr = 5e-4 . DRUM ROLL PLEASE!!!!! You are now going to finally start training your model! Specifically for 8 epochs because that was what was in the original code in the NLP course and it also happened to work the best during my training. However, you are also implementing a few call backs, namely automatically saving the best performing model, early stopping, and showing the training and validation loss graph. Since you are using early stopping, feel free to try out a higher epoch number and the training will stop once it starts not improving. . def train_model(learn, epochs, model_name, max_lr = 5e-4): &quot;&quot;&quot;Trains a model using save model, early stopping, and show graph call backs.&quot;&quot;&quot; callback_fns = [ callbacks.SaveModelCallback( learn, every=&#39;improvement&#39;, monitor=&#39;valid_loss&#39;, name=f&#39;{model_name}_save_model&#39; ), callbacks.EarlyStoppingCallback( learn, monitor=&#39;valid_loss&#39;, min_delta = 0.01, patience = 3 ), ShowGraph(learn) ] learn.fit_one_cycle(epochs, max_lr, div_factor=5, callbacks = callback_fns) . epochs = 8 model_name = &#39;comment_gen&#39; . Training on Google Colab can take anywhere from ~20 to 60 minutes depending on the type of GPU they give you. So, relax, get an IV caffeine drip going, and let your model cook in peace :). . train_model(learn, epochs, model_name, max_lr = max_lr) . epoch train_loss valid_loss accuracy bleu time . 0 | 1.182219 | 1.133453 | 0.828182 | 0.791774 | 06:46 | . 1 | 0.920205 | 0.954264 | 0.841556 | 0.799681 | 06:47 | . 2 | 0.812330 | 0.875513 | 0.849487 | 0.804000 | 06:44 | . 3 | 0.752023 | 0.828835 | 0.853668 | 0.807183 | 06:45 | . 4 | 0.679716 | 0.794862 | 0.856593 | 0.809325 | 06:43 | . 5 | 0.653454 | 0.777795 | 0.859418 | 0.811010 | 06:42 | . 6 | 0.611860 | 0.770059 | 0.860419 | 0.812164 | 06:49 | . 7 | 0.605370 | 0.769881 | 0.860601 | 0.812119 | 06:45 | . Better model found at epoch 0 with valid_loss value: 1.133453130722046. . Better model found at epoch 1 with valid_loss value: 0.9542644023895264. Better model found at epoch 2 with valid_loss value: 0.8755126595497131. Better model found at epoch 3 with valid_loss value: 0.8288350701332092. Better model found at epoch 4 with valid_loss value: 0.7948615550994873. Better model found at epoch 5 with valid_loss value: 0.7777946591377258. Better model found at epoch 6 with valid_loss value: 0.7700592279434204. Better model found at epoch 7 with valid_loss value: 0.7698812484741211. . Evaluate your model . Let us now evaluated your trained model on some of your validation set so see how well your model is generating comments. . def get_predictions(learn, ds_type=DatasetType.Valid): learn.model.eval() inputs, targets, outputs = [],[],[] with torch.no_grad(): for xb,yb in progress_bar(learn.dl(ds_type)): out = learn.model(*xb) for x,y,z in zip(xb[0],xb[1],out): inputs.append(learn.data.train_ds.x.reconstruct(x.cpu())) targets.append(learn.data.train_ds.y.reconstruct(y.cpu())) outputs.append(learn.data.train_ds.y.reconstruct(z.cpu().argmax(1))) return inputs, targets, outputs inputs, targets, outputs = get_predictions(learn) . . def print_results(inputs, targets, outputs, method_spm, comment_spm, n = 10): &quot;&quot;&quot;Just a little helper function for printing out the results from your model.&quot;&quot;&quot; for i in range(n): print(&quot;Input:&quot;, &quot; &quot;.join(decode_spec_tokens(method_spm.DecodePieces(str(inputs[i]).split(&quot; &quot;)).split(&quot; &quot;))), &quot; n&quot;) print(&quot;Target:&quot;, &quot; &quot;.join(decode_spec_tokens(comment_spm.DecodePieces(str(targets[i]).split(&quot; &quot;)).split(&quot; &quot;))), &quot; n&quot;) print(&quot;Predicted:&quot;, &quot; &quot;.join(decode_spec_tokens(comment_spm.DecodePieces(str(outputs[i]).split(&quot; &quot;)).split(&quot; &quot;))), &quot; n&quot;) print_results(inputs, targets, outputs, method_spm, comment_spm) . . Input: xxbos @doesservicerequest private void putrangeinternal(final filerange range, final filerangeoperationtype operationtype, final byte[] data, final long length, final String md5, final accesscondition accesscondition, final filerequestoptions options, final operationcontext opcontext) throws storageexception { executionengine.executewithretry(this.fileserviceclient, this, putrangeimpl(range, operationtype, data, length, md5, accesscondition, options, opcontext), options.getretrypolicyfactory(), opcontext); } xxeos Target: xxbos Used for both uploadrange and clearrange. xxeos Predicted: xxbos Put to creating thes( s( xxeos Input: xxbos public static byte[] encodesequence(byte[]... encodedvalues) { int length = 0; for (byte[] encodedvalue : encodedvalues) { length += encodedvalue.length; } byte[] lengthencoded = encodelength(length); bytearraydataoutput out = bytestreams.newdataoutput(1 + lengthencoded.length + length); out.write(sequence_tag); out.write(lengthencoded); for (byte[] entry : encodedvalues) { out.write(entry); } return out.tobytearray(); } xxeos Target: xxbos Encodes a sequence of encoded values. xxeos Predicted: xxbos Encodes a byte of bytes bytes into xxeos Input: xxbos @override public String dnsresolveex(string host) { stringbuilder result = new stringbuilder(); try { inetaddress[] list = inetaddress.getallbyname(host); for (inetaddress inetaddress : list) { result.append(inetaddress.gethostaddress()); result.append(&#34;; &#34;); } } catch (unknownhostexception e) { log.log(level.fine, &#34;DNS name not resolvable {0}.&#34;, host); } return result.tostring(); } xxeos Target: xxbos *********************************************************************** dnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveex xxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeos Predicted: xxbos xxmap 51 * namepp =pxxeos Input: xxbos protected void removeallfromattributevalueset() { final collection&lt;abstracthtml5sharedobject&gt; sharedobjects = getsharedobjects(); boolean listenerinvoked = false; final collection&lt;writelock&gt; writelocks = lockandgetwritelocks(); try { getattributevalueset().clear(); setmodified(true); invokevaluechangelisteners(sharedobjects); listenerinvoked = true; } finally { for (final Lock lock : writelocks) { lock.unlock(); } } pushqueues(sharedobjects, listenerinvoked); } xxeos Target: xxbos clears all values from the value set. xxeos Predicted: xxbos s all attribute from the xxeos Input: xxbos public void registercheckwithnotes(string checkid, String name, String script, long interval, @suppresswarnings(&#34;sameparametervalue&#34;) String notes) { Check check = new Check(); check.setid(checkid); check.setname(name); check.setscript(script); check.setinterval(string.format(&#34; ⁇ ss&#34;, interval)); check.setnotes(notes); registercheck(check); } xxeos Target: xxbos Registers a Health Check with the Agent. xxeos Predicted: xxbos Registers a xxupj ealth checkxxmaj check a givenxxmaj name xxeos Input: xxbos public void assertequalsignoringcase(@nullable Description description, @nullable String actual, @nullable String expected) { if (!areequalignoringcase(actual, expected)) { String format = &#34;expecting:&lt; ⁇ s&gt; to be equal to:&lt; ⁇ s&gt;, ignoring case considerations&#34;; throw failures.failure(description, new basicerrormessagefactory(format, actual, expected)); } } xxeos Target: xxbos Verifies that two s are equal, ignoring case considerations. xxeos Predicted: xxbos Assert that the stringsxx are equal, oring the... xxeos Input: xxbos protected cronschedulebuilder createcronschedulebuilder(string cronexpr) { int i = cronexpr.indexof(&#34;[&#34;); int j = cronexpr.indexof(&#34;]&#34;); timezone timezone = defaulttimezone; if (i &gt; -1 &amp;&amp; j &gt; -1) { timezone = timezone.gettimezone(cronexpr.substring(i+1, j)); cronexpr = cronexpr.substring(0, i).trim(); } return cronschedulebuilder.cronschedule(cronexpr).intimezone(timezone); } xxeos Target: xxbos Allow timezone to be configured on a per-cron basis with [timezonename] appended to the cron format xxeos Predicted: xxbos Create to to create used to the mtimei-s of a0],]. to the givenath.xxeos Input: xxbos private &lt;T&gt; fakeencodeditem readnextitem(class&lt;t&gt; clazz) { fakeencodeditem item = data[dataposition]; if (item == null) { / / While Parcel will treat these as zeros, in tests, this is almost always an error. throw new unreliablebehaviorerror(&#34;reading uninitialized data at position &#34; + dataposition); } checkconsistentreadandincrementposition(clazz, item); return item; } xxeos Target: xxbos Reads a complete item in the byte buffer. xxeos Predicted: xxbos Read the from the given array. xxeos Input: xxbos private void hidesuggestionsifnecessary(final @nonnull querytoken currentquery, final @nonnull tokensource source) { String queryts = currentquery.gettokenstring(); String currentts = source.getcurrenttokenstring(); if (!iswaitingforresults(currentquery) &amp;&amp; queryts != null &amp;&amp; queryts.equals(currentts)) { msuggestionsvisibilitymanager.displaysuggestions(false); } } xxeos Target: xxbos Hides the suggestions if there are no more incoming queries. xxeos Predicted: xxbos Check the givenion of of the is no more . xxeos Input: xxbos public list&lt;uirow&gt; getvalues() throws efapsexception { list&lt;uirow&gt; ret = new arraylist&lt;&gt;(); if (isfiltered()) { for (final uirow row : this.values) { boolean filtered = false; for (final tablefilter filter : this.filters.values()) { filtered = filter.filterrow(row); if (filtered) { break; } } if (!filtered) { ret.add(row); } } } else { ret = this.values; } setsize(ret.size()); return ret; } xxeos Target: xxbos This is the getter method for the instance variable . xxeos Predicted: xxbos Returns method a first method for the row of. xxeos . This is great and all. However, you can see the text looks a bit off. Your model sort of starts generating some word and then switches half way through sometimes. This is because the way you are currently trying to generate tokens is using Teacher Forcing, which means you are giving the model the groundtruth for what it should have produced even if it did not. This is very helpful during training, however, it expects to have both the x and y of an input. In a real world setting, you aren&#39;t going to be given the y, obviously! . Therefore, I found a hacky way of bypassing this need for the y so that it is no longer needed. This involves using an empty array that fakes being the y, but has only ones and is updated everytime the model makes a prediction and is then fed back into the model so that is knows what it has generated before. . Heads Up The way I coded this is extremely inefficient and so running it will take a long time to generate predictions. Therefore I recommend only generating a few comments (I set it up to only do 10). . TODO For You: Come up with a more efficient solution that performs similarly to the Teacher Forcing approach of the above code. . P.S. . For other language learners provided by FastAI, you can simply use the predict function and pass some text and ask for the model to predict the next set of tokens. However, I have been unsuccessful in implementing this predict function for Sequence to Sequence models. So, another TODO For You is to see if you can implement a predict function for Sequence to Sequence models so that you can easily generate comments for methods that you just pass to the function! . If you do figure out a way to do this, I would be extremely interested! So, feel free to leave a comment about it. . def get_preds(learn, db_tst, max_seq = 128, n = 10): learn.model.eval() inpts, trgts, preds = [], [], [] for i, (xb,yb) in enumerate(progress_bar(db_tst.dl(DatasetType.Train))): if i &gt;= n: break res = torch.zeros(len(xb[0]), max_seq, device = torch.device(&#39;cuda&#39;)).long() + 1 for i in range(max_seq - 1): outs = learn.model(xb[0], res) for j, out in enumerate(outs): res[j][i + 1] = out.argmax(1)[i] for x, y, z in zip(xb[0], yb, res): inpts.append(str(learn.data.train_ds.x.reconstruct(x.cpu()))) trgts.append(str(db_tst.train_ds.y.reconstruct(y.cpu()))) preds.append(str(learn.data.train_ds.y.reconstruct(z.cpu()))) return inpts, trgts, preds inputs, targets, outputs = get_preds(learn, db_tst) print_results(inputs, targets, outputs, method_spm, comment_spm) . . &lt;progress value=&#39;10&#39; class=&#39;&#39; max=&#39;149&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 6.71% [10/149 01:14&lt;17:21] Input: xxbos protected String parseunquotedstringcontent() { final int startndx = ndx; while (true) { final char c = input[ndx]; if (c &lt;= &#39; &#39; || charutil.equalsone(c, UNQUOTED_DELIMETERS)) { final int currentndx = ndx; / / done skipwhitespaces(); return new String(input, startndx, currentndx - startndx); } ndx++; } } xxeos Target: xxbos Parses un-quoted string content. xxeos Predicted: xxbos Parses the text from the HTML text. xxeos Input: xxbos private static void checkfilecopy(final File srcfile, final File destfile) throws ioexception { checkexists(srcfile); checkisfile(srcfile); if (equals(srcfile, destfile)) { throw new ioexception(&#34;files &#39;&#34; + srcfile + &#34;&#39; and &#39;&#34; + destfile + &#34;&#39; are equal&#34;); } File destparent = destfile.getparentfile(); if (destparent != null &amp;&amp; !destparent.exists()) { checkcreatedirectory(destparent); } } xxeos Target: xxbos Checks that file copy can occur. xxeos Predicted: xxbos Checks if the file is a file. xxeos Input: xxbos long analyze() { Arc a; Arc aa; if (pre.outs == null) { return flags.reg_uimpossible; } for (a = pre.outs; a != null; a = a.outchain) { for (aa = a.to.outs; aa != null; aa = aa.outchain) { if (aa.to == post) { return flags.reg_uemptymatch; } } } return 0; } xxeos Target: xxbos analyze - ascertain potentially-useful facts about an optimized NFA xxeos Predicted: xxbos Returns the Syoooo Syna Syna Syna Sa Sa Sa Sa Sa Sa Sa Sa syna Syna xxeos Input: xxbos @suppresswarnings(&#34;unchecked&#34;) public REC next() { checkdirection(true); orecord record; / / ITERATE UNTIL THE NEXT GOOD RECORD while (hasnext()) { / / FOUND if (currentrecord != null) { try { return (REC) currentrecord; } finally { currentrecord = null; } } record = gettransactionentry(); if (record != null) return (REC) record; } return null; } xxeos Target: xxbos Return the element at the current position and move forward the cursor to the next position available. xxeos Predicted: xxbos Returns the next record in the queue. xxeos Input: xxbos public static void addtransitivematches(hollowreadstateengine stateengine, map&lt;string, bitset&gt; matches) { list&lt;hollowschema&gt; schemalist = hollowschemasorter.dependencyorderedschemalist(stateengine); collections.reverse(schemalist); for(hollowschema schema : schemalist) { bitset currentmatches = matches.get(schema.getname()); if(currentmatches != null) { addtransitivematches(stateengine, schema.getname(), matches); } } } xxeos Target: xxbos Augment the given selection by adding the references, and the &lt;i&gt;transitive&lt; / i&gt; references, of our selection. xxeos Predicted: xxbos Add a variable to the list of s. xxeos Input: xxbos protected void resolvenestedproperties(final beanproperty bp) { String name = bp.name; int dotndx; while ((dotndx = indexofdot(name)) != -1) { bp.last = false; bp.setname(name.substring(0, dotndx)); bp.updatebean(getindexproperty(bp)); name = name.substring(dotndx + 1); } bp.last = true; bp.setname(name); } xxeos Target: xxbos Resolves nested property name to the very last indexed property. If forced, &lt;code&gt;null&lt; / code&gt; or non-existing properties will be created. xxeos Predicted: xxbos Resolve the property name. xxeos Input: xxbos public xmlconfig declarenamespace(string prefix, String namespaceuri) { validate.notempty(prefix, &#34;prefix cannot be empty&#34;); validate.notempty(namespaceuri, &#34;namespace URI cannot be empty&#34;); map&lt;string, String&gt; updatednamespaces = new hashmap&lt;string, string&gt;(declarednamespaces); updatednamespaces.put(prefix, namespaceuri); return new xmlconfig(features, updatednamespaces, properties, validating, true, allowdoctypedeclaration, true); } xxeos Target: xxbos Declares a namespace and also sets } to &lt;code&gt;true&lt; / code&gt;. &lt;p / &gt; &lt;p&gt;note that you cannot use this to add namespaces for the matcher. This has to be done by providing a to the matcher instance.&lt; / p&gt; xxeos Predicted: xxbos Creates a new instance of the given name and the given name. xxeos Input: xxbos protected static int getshadowradius(drawable shadow, Drawable circle) { int radius = 0; if (shadow != null &amp;&amp; circle != null) { Rect rect = new Rect(); radius = (circle.getintrinsicwidth() + (shadow.getpadding(rect) ? rect.left + rect.right : 0)) / 2; } return Math.max(1, radius); } xxeos Target: xxbos Calculates required radius of shadow. xxeos Predicted: xxbos Get the Syyyy Syyy Syyy Syna Syna Syna Syna Syna Sna Syna Syna Syna xxeos Input: xxbos void addadviceclinitmethod(final String name) { if (adviceclinits == null) { adviceclinits = new arraylist&lt;&gt;(); } adviceclinits.add(name); } xxeos Target: xxbos Saves used static initialization blocks (clinit) of advices. xxeos Predicted: xxbos Adds a Java class to the Saa Sa Sa Syyetch Bean. xxeos Input: xxbos public static String padleft(string s, int desiredlength, String padstring) { while (s.length() &lt; desiredlength) { s = padstring + s; } return s; } xxeos Target: xxbos Pad the given string with padstring on the left up to the given length. xxeos Predicted: xxbos Compares two strings, and returns the first character in the string. xxeos . Not too shabby if I do say so myself! It seems to actually be learning about what a comment is supposed to have in it for documenting what the method is doing. Of course there are a lot of tweaks you could do such as adding the ability to generate inline comments instead of just method level, using more data, using different sampling schemes for generating the comments such as top-k or nucleus, and any other awesome things you could think of! And if you do feel free to leave a comment about your adventure. . Tip: I have done a lot of fiddling to get this to work, however, most of my models ended up overfitting. The way I fixed this issue was just being more careful in how I clean the data and also increase the data size. I know this seems simple, but it is quite effective. . Conclusion . In this tutorial, you created an Automatic Code Comment Generator! You learned about how to clean, explore, and process data and how to use the awesome Pytorch and FastAI to define and train the awesome Transformer architecture. The use of Deep Learning in the field of Software Engineering is what I am studying for my Ph.D., so I hope I have inspired you to think about some other ways you could use Deep Learning to help Software Engineering! . I hope you enjoyed this tutorial and look out for future blog posts from me about all kinds of topics as I have announced a challenge for myself for learning new things this year! Okay, I&#39;m posing a #challenge to myself: Each month devote an hour a day to learning about a subject I am weak at. At the end of the month, post a blog summarizing what you&#39;ve learned.March is devoted to #neuroscience! Already signed up for #edX course!#ChallengeAccepted 😎 . &mdash; Nathan Cooper (@ncooper57) March 5, 2020 . CodeSearchNet citation . @article{husain_codesearchnet_2019, title = {{CodeSearchNet} {Challenge}: {Evaluating} the {State} of {Semantic} {Code} {Search}}, shorttitle = {{CodeSearchNet} {Challenge}}, url = {http://arxiv.org/abs/1909.09436}, urldate = {2020-03-12}, journal = {arXiv:1909.09436 [cs, stat]}, author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc}, month = sep, year = {2019}, note = {arXiv: 1909.09436}, } . &lt;/div&gt; .",
            "url": "https://nathancooper.io/i-am-a-nerd/deep_learning/software_engineering/2020/03/07/How_to_Create_an_Automatic_Code_Comment_Generator_using_Deep_Learning.html",
            "relUrl": "/deep_learning/software_engineering/2020/03/07/How_to_Create_an_Automatic_Code_Comment_Generator_using_Deep_Learning.html",
            "date": " • Mar 7, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Awesome Things I Learned Creating My Own Website",
            "content": "Hello, Solar System! (As a space faring civilization, I feel it only customary we update our greetings to reflect such awesome accomplishments 🤓) I am a nerd and hopefully you are as well. . This post goes over many of the awesome technologies, resources, and overall tips and tricks I learned while creating my own personal website! This post is NOT a tutorial, mostly because there are tons of already existing ones on how to create a website and you don’t want to create a website or even my website (though mine is pretty awesome), you want you create your own website. For me this came from a lot of trial and error and tons of random google searches to fit some niche feature I wanted to add. So, this post is to centralize all of the niche features that went into my website in case any of you out there want to personalize some for your own website. This post is not really made to be gone through end to end, but rather for you to pick out the pieces that resonate best with you and give you inspiration for your own website. . Let’s get some of the boring stuff out of the way first, namely these are the main components that my website is made out of: . ReactJS - using create-react-app and . | Material-UI - for the style points 😎 (Sadly not as delicious as brownie points) . | GitHub Pages - for hosting the static site (ty GitHub &lt;3), requires you use gh-pages and setup your create-react-app up correctly. Here is some documentation on how that is done . | GitHub and GitHub Pages - for storing my projects and for storing the web demos of my projects (Sounds epic, right?!?!? More on this later) . | Redux - for storing and updating state of ReactJS thingies like lists of current projects and blog posts (More on this later) . | ReactMarkdown - for rendering my blog posts, which, you guessed it, are markdown files! . | Paperclips - I couldn’t afford duct tape :( . | . Niche Features and Tips . Resume of Coding Projects: . The core motivation behind my website was that I wanted a cool way to show off some of my projects that I’ve created over the years. I’ve seen how others create theirs and actually got inspired in part by https://nmarch213.github.io/Portfolio/#/projects. However, the issue is that these displays of projects need to be manually created and that was a problem for me because like most developers I am lazy and I don’t want to do that. I wanted a way where I could just create new projects and they would be automatically added in the correct format, including images, titles, descriptions, etc. I could just redirect users to my GitHub page, where I place all of my coding projects, but that seemed like a cop out and did not allow for any customization. So, like any good programmer, I made a scrapper that took the output from GitHub and converted into a format for my own usage 😊. This gave my website the ability to update the list of projects automatically as I added new repositories, which the title of each project is just determined by the repository’s name. To add an image to be displayed as the project’s logo, I just add an icon.png file to the root of the project and grab the icon from there when displaying the list. . To scrape all this information, I use the amazing GitHub API v3 that GitHub provides. This API offers a ton of useful features, but for scrapping my projects I specifically used the Repositories API. This also has information like the repository’s description, so you could include that information in your list of projects automatically if you so choose. The GitHub API v3 has a bunch of awesome functionality, another API I use is the Contents API for listing out the different posts I have created (more on this later)! For integrating these APIs using ReactJS, I suggest using Redux for storing the state, i.e., the projects and blog posts once they have returned from the GitHub API, and Axios for actually making the HTTPS requests. . Besides just hosting one website using GitHub Pages, you can have one per repository. Incorporating this with the dynamic list of my coding projects, I am able to now create their own page that can serve as documentation or can even be a web demo! Currently I am not using this feature to the best of its ability, but I plan on overhauling my more put together projects so that they have at the very least some documentation using this awesome feature. . Overall, using very simple components offered by GitHub I am able to have my custom resume of projects that dynamically updates when I create a new one and links to documentation or a web demo of the project. Any updates to the actual project are reflected on the website without additional changes to some configuration file that contains all of the projects I have. . Blog: . I now do all of my blogging using the new awesome fastpages library built by Hamel Husain and Jeremy Howard! . [Deprecated] . Aside from being able to high-light my accomplishments, I wanted to be able to express myself on a multitude of topics. I never thought I would be a blogger, but a blog post by the awesome Rachel Thomas, Why you (yes, you) should blog, inspired me to take it seriously. I tried with Medium, but I wanted something of my own and when I saw the equally awesome Jeremy Howard discussing his work on Fast Template that allows you to easily create your own personal blog using GitHub Pages and simple Markdown files I knew the time was now to commit. Now while I do not directly use Fast Template, because it is a bit too rigid for the amount of customizability I like to perform, I drew a lot of inspiration, namely writing blog posts as markdown files and having them stored statically on my website instead of on some weird MongoDB. To integrate this concept of using Markdown files I needed a way to render them easily using React, which is where I found this awesomely customizable library for doing just that called React Markdown. What’s nice about React Markdown is that each of the formatting components such as code snippets and headings are separated into their own rendering engine allowing you to swap out or customize them very easily. So since I quite enjoy dark theme, I found this awesome post by Bexultan A. Myrzatayev showing how you can use a custom react highlighting engine, I use React Syntax Highlighter, and swap out the default theme for code syntax highlighting for something like “atomOneDark”! (obviously I chose a dark theme, because dark theme is the only theme) . To write my posts, I don’t just write directly using Markdown as that would be extremely painful. I took the advice of Jeremy Howard again from his post on Syncing your blog with your PC, and using your word processor and used a word processor, in my case it is Google Docs. Sadly, Google Docs does not allow you to directly export your document as a Markdown file, but thankfully an awesome person named Mangini created gdocs2md that does the conversion and handles grabbing the images and emails them to you! . Material Design: . As a nerd and mostly a computer nerd, my artistic skills are not the best. However, I wanted my website to look stylish, clean, modern, cool, hip…. (words I searched google for when trying to create a pretty website). This brought me to Google, they always create such chic looking websites and mobile apps, in my opinion obviously, so it wasn’t too surprising to learn that Google wrote the bo… well, website for designing GUI components, which they called Material Design. This is where Material-UI comes in. It is a complete React component implementation of the Material Design language and it looks smooootthhhhh. Using it is quite simple as laid out in there website, but I’m including a code snippet because I want to flex how my website is able to render code snippets courtesy of React Markdown and Bexultan A. Myrzatayev’s awesome post on how to change the theme used in code snippets : . Code snippet from what a project entry looks like: . import { Button, Card, CardActions, CardContent, CardMedia, Typography } from &quot;@material-ui/core&quot;; …. const site = has_pages ? ( &lt;Button variant=&quot;contained&quot; color=&quot;secondary&quot; href={pages_url}&gt; View Site &lt;/Button&gt; ) : null; return ( &lt;Card className={classes.card} fullWidth&gt; &lt;CardMedia className={classes.media} image={icon_src} onError={e =&gt; { console.log(&quot;cannot find icon&quot;); }} /&gt; &lt;CardContent&gt; &lt;Typography gutterBottom variant=&quot;headline&quot; component=&quot;h4&quot;&gt; {_.startCase(_.camelCase(name))} &lt;/Typography&gt; &lt;/CardContent&gt; &lt;CardActions&gt; {site} &lt;Button variant=&quot;contained&quot; color=&quot;primary&quot; href={html_url}&gt; View Repo &lt;/Button&gt; &lt;/CardActions&gt; &lt;/Card&gt; ); …. . Development: . I think programming is an invaluable skill that I am constantly learning to improve. So, I highly recommend those creating their own website to at least try to program it themselves. It is an adventure of pain and misery that I wish to inflict onto others, hahaha, haha, ha… But extremely rewarding when you finally see that beautiful glowing (please don’t make your website glow, it’s annoying) website plastered on your web browser (please let it be Chromium based or just basically not Explorer). All programmers need some system in which to develop whatever it is they care about creating. This is where things like Integrated Development Environments (IDEs), debuggers, and testing frameworks come in handy. . To keep myself sane, I spent a long, arduous, and tedious time trying and experimenting with different workflows for developing systems. And I have found the holy grail that has answered all of my questions and that allows for 10X greater productivity for *me *(your results will most certainly differ if you decide to use the same developmental setup). For me, I found the combination of Visual Studio Code and Docker to be the textbook definition of perfection. In particular, the Remote Container extension that some genius made. This extension allows you to connect your vscode editor to a docker container’s file system. So, why is this so important to me? Well Docker allows you to spin up pretty much any environment you want such as a node server for hosting a ReactJS website :D, but most importantly it allows you to version and share these environments through Dockerfiles. This allows me to specify an environment per project, so I don’t have to maintain installing all of the dependencies that may conflict with each other on my local machine. This is why I use Docker for pretty much everything I do and I also quite enjoy using vscode, so being able to marry the two is absolute perfection! . Conclusion . So, that concludes my first blog post! I hope you are able to use some of these awesome things I learned while creating my own website for your own. Keep a lookout for my future posts, I am planning on creating posts circulating around the following topics: . Machine Language Processing (MLP) - like Natural Language Processing (NLP), but for computer nerds like us 🤓. . | Automatic Code Comment Generation using Deep Learning. . | . Also, feel free to contact me using my custom “Contact” system, which uses this awesome Google Script for sending emails without the need of manually setting up a backend server, integrated into my website or on twitter @ncooper57 :). .",
            "url": "https://nathancooper.io/i-am-a-nerd/website/awesome/2020/02/03/Awesome-Things-I-Learned-Creating-My-Own-Website.html",
            "relUrl": "/website/awesome/2020/02/03/Awesome-Things-I-Learned-Creating-My-Own-Website.html",
            "date": " • Feb 3, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nathancooper.io/i-am-a-nerd/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nathancooper.io/i-am-a-nerd/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}