{
  
    
        "post0": {
            "title": "What I Learned (WIL) Neuroscience Month [Part 1]",
            "content": "If you are like me, you may have thought there was quite a lot of mystery going on in your brain that the scientific community has yet to really figure out and understand. However, through my month long study I found that neuroscience has some powerful computational models and experiments that are able to explain many of the processes going on in the brain such as the visual system, how we store memories, and the process we call intuition. Of course there are still unanswered questions such as what consciousness is and how are we able to learn things with just a few examples, but I was shocked how much is known. . I will be going through some of the amazing things that are happening in that brain of yours through this series. This first part is devoted to the ‚ÄúHigh Level‚Äù stuff, which involves the more complicated behavior such as how different parts of your brain contribute to your conscious experience, how you are able to solve seemly difficult problems with ease, and how this intuition of yours probably contributes to many of the logical errors you make. Let‚Äôs get right into it! . High Level Stuff . System 1 and System 2 . In the amazing book ‚ÄúThinking Fast and Slow‚Äù by Daniel Kahneman, who is a world renowned economist and neuroscientist, the high level ways in which we ‚Äúthink‚Äù are examined. In this book, things like decisions, problem solving, emotional states, and why we make errors in our logic, both consciously and subconsciously, are shown. I highly recommend this book, I personally have been enjoying the Audiobook version! . . Overview of the two different Systems that make up your thinking. . Daniel Kahneman introduced the idea of two systems of thought, which he named very creatively System 1 and System 2. System 1 is more associated with intuition or fast thinking that processes things without much mental strain such as looking at a picture of a cat and understanding that what you are currently looking at is in fact a cat. System 1 is also automatic in the sense that you have no control over coming to the realization that a cat is in the picture. System 2, on the other hand, is involved in more deliberate and difficult processing that requires you to put in work for solving some task such as calculating that the multiplication of 18 and 32 is equal to 576. don‚Äôt worry, I‚Äôll wait :). . . A Cat: Image by Free-Photos from Pixabay . Yoshua Bengio, who is a pioneer of artificial intelligence and recent receiver of the Turing award along with other amazing scientists in the field, put it quite well on how these two systems work together. System 1 is great at generating representations of things and associating them with high level objects such as cats, words, and concepts. These representations are then exploited by System 2 to avoid all of the nitty gritty details of what it means for a cat to be a cat. It instead uses these concepts to do interesting things such as finding relationships between multiple objects or performing complex calculations by planning out a series of operations to perform, i.e. generating and following an algorithm for multiplying 2 numbers. . Daniel Kahneman also mentions many fun experiments to show how System 1 is what runs most of your day to day experiences and how this leads to a lot of logical errors. One famous experiment is the bat and ball experiment. Try and solve the following problem: . The cost of a bat and ball come to a total of 1 dollar and 10 cents. If the bat costs 1 dollar more than the ball, how much does the ball cost? . If you guessed the ball costs 10 cents like I did, you were wrong and you were wrong because your System 2 is very, very lazy. A simple calculation will show that if the ball cost 10 cents and the bat is 1 dollar more, the bat would be 1 dollar and 10 cents bring the total up to 1 dollar and 20 cents. . The above example and many more show a common theme in how our minds work. System 1 is at fault here, it recognized some similarity in the problem and automatically offered up what seemed like a reasonable answer to the problem. However, it never even tried to check if the answer was correct. This checking is done by System 2, but System 1 was so sure of itself that System 2 did not even bother to check its answer because it takes work to verify. System 2 is involved in slow and deliberate thinking such as performing calculations or having to consider multiple pieces of information when making a decision. . One interesting effect that occurs when people are actively using their System 2 to solve a task is that it loosen their inhibitions. The study described in ‚ÄúThinking Fast and Slow‚Äù showed that people who were asked during an experiment whether they would like a sweet treat such as a slice of chocolate cake or a salad were more likely to choose the chocolate cake if they were also given the task of keeping 7 digits in their mind for a few minutes. It has also been shown that performing System 2 style tasks has an impact on the way people behave such as with increased selfishness and even increased use of sexist language. This is all due to System 2 style tasks requiring focus and attention on the task at hand allowing for System 1, who is known to make quick judgement calls without much thought, to take over any additional tasks you are not focusing on. . Our daily lives and thought process circulate around when to use System 2 for tasks that System 1 has no way of generating a good answer for. You may ask why we do not use System 2 for more tasks, but as discussed previously System 2 is slow. It would be quite dangerous if we relied on System 2 to solve how to avoid crashing into a vehicle on the highway that unexpectedly started merging into your lane without a turn signal or to duck to avoid being hit by a fast ball that‚Äôs outside of the batter‚Äôs box. An interesting feature, however, of System 2 style tasks is that if they are seen often enough, they can be upgraded to System 1 style tasks by having System 1 learn to recognize the expected answer. This is a common occurrence for driving around an unfamiliar path. The first few times you drive your System 2 is more attentive to make sure you get to where you are going without getting lost. Eventually, if you take the same path, it becomes familiar and your System 1 is able to take over allowing you to reach your destination without much attention being paid to the path you are taking. . Brain Structure . Now, your brain does not actually have these System 1 and System 2 structures physically, they are just a great way of discussing the way in which your brain works. However, the real way your brain is structured is a lot more messy, but still beautiful :). I found the explanation and structure of how your brain is wired in ‚ÄúComputational Cognitive Neuroscience‚Äù by Munakata et. al. to be a great resource for learning and so I will be using it as my main source for the rest of this article. While it is a lot denser than ‚ÄúThinking Fast and Slow,‚Äù it has great visualizations and goes quite in depth. . . Image by Oberholster Venita from Pixabay (Modified to have labels) . Your brain can be organized into two main parts, the Neocortex, which is what most people think of when they imagine a brain, and the Cerebellum, which is not as well known by most but is believed to play a big part in how we think. The Neocortex is the coloured part in the above image and it can be roughly broken down into 4 main lobes, but each lobe is heavily dependent on the others, so don‚Äôt think of them as truly distinct sections. You can see the responsibilities of each lobe in the table below. . . Overall responsibilities of the different lobes of the brain. . One of the most interesting parts of your brain structure is how representations of your senses like sight and sound are built up in a hierarchical fashion as your brain moves the information across the different lobes. Take your ability to easily understand what your eyes are currently seeing. This information is first processed by the Occipital lobe hierarchically at the very back of the brain, starting with identifying simple edges and then moving on to groups of edges to form basic shapes. This representation grows in complexity as the information is sent towards the other parts of the brain like the Temporal lobe where these representations are given semantic meaning in the form of words such as Cat or Human or even more specific such as Garfield the Cat. If we track the representation that your brain is building of what you are seeing into the Parietal lobe, you will find your brain generating representations for relations between the different objects in the scene such as the Cat is hanging from a tree (hang in there buddy!). Lastly, the Frontal lobe takes all these high level representations to perform any number of high level decisions and motor control movement such as petting the kitty :). . As you can see, even this very simple example of just processing what your eyes see involves multiple parts of your brain, even if a lot of the initial work is done by the Occipital lobe. Each lobe does its part in helping generate an understanding of the sensory input you are receiving. This is not just limited to vision, but could be any other type of senses such as smell, touch, and hearing. It‚Äôs all connected‚Ä¶ . . Pepe Silvia from It‚Äôs Always Sunny in Philadelphia . Conclusions . Well, sadly that is all we are going to spend on the high level stuff of the brain. There are tons of additional stuff I could discuss about the high level part of your brain such as how the Cerebellum actually has half of all the neurons in your entire brain! And how much of its functions are not well understood because it seems to have its hands in processing everything! I could also discuss how there are different types of ways different parts of your brain represent input such as clusterization, hashing, and composition of different representations into new representations. However, there is a lot and we have already covered a lot of the super cool stuff! I hope you have enjoyed this first part of the Neuroscience series and have come away with a better or at least more confusing :) sense of how your brain works! . I will be working to get the next two parts (Mid Level and Low Level) out soon as there is not much else for me to do during this COVID-19 quarantine (don‚Äôt tell my Ph.D. advisor I said that). If you have any questions or have any comments about any interesting stuff you know about the brain please comment down below, I‚Äôd love to hear it! .",
            "url": "https://nathancooper.io/i-am-a-nerd/wil/neuroscience/2020/04/07/neuroscience-1.html",
            "relUrl": "/wil/neuroscience/2020/04/07/neuroscience-1.html",
            "date": " ‚Ä¢ Apr 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "How to Create an Automatic Code Comment Generator using Deep Learning!",
            "content": "About . In this post, you will be create a Deep Learning model that given a piece of code, will automatically generate a comment describing (hopefully ü§û) what the piece of code does. This post will focus on Java code, however, the same approach should be able to be applied to other programming languages such as Python or Javascript. . Collecting, preparing and exploring the data . You will be using the CodeSearchNet Challenge dataset from GitHub as it provides a large collection of clean code in multiple different languages. They have a really nice example on how to download and read in the data in their repo that you&#39;ll use to get started. . ! wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip ! unzip java.zip . --2020-03-07 16:37:37-- https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.179.149 Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.179.149|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1060569153 (1011M) [application/zip] Saving to: ‚Äòjava.zip‚Äô java.zip 100%[===================&gt;] 1011M 88.0MB/s in 12s 2020-03-07 16:37:49 (84.9 MB/s) - ‚Äòjava.zip‚Äô saved [1060569153/1060569153] Archive: java.zip creating: java/ creating: java/final/ creating: java/final/jsonl/ creating: java/final/jsonl/train/ inflating: java/final/jsonl/train/java_train_12.jsonl.gz inflating: java/final/jsonl/train/java_train_9.jsonl.gz inflating: java/final/jsonl/train/java_train_3.jsonl.gz inflating: java/final/jsonl/train/java_train_5.jsonl.gz inflating: java/final/jsonl/train/java_train_7.jsonl.gz inflating: java/final/jsonl/train/java_train_1.jsonl.gz inflating: java/final/jsonl/train/java_train_10.jsonl.gz inflating: java/final/jsonl/train/java_train_14.jsonl.gz inflating: java/final/jsonl/train/java_train_0.jsonl.gz inflating: java/final/jsonl/train/java_train_6.jsonl.gz inflating: java/final/jsonl/train/java_train_8.jsonl.gz inflating: java/final/jsonl/train/java_train_15.jsonl.gz inflating: java/final/jsonl/train/java_train_2.jsonl.gz inflating: java/final/jsonl/train/java_train_4.jsonl.gz inflating: java/final/jsonl/train/java_train_13.jsonl.gz inflating: java/final/jsonl/train/java_train_11.jsonl.gz creating: java/final/jsonl/test/ inflating: java/final/jsonl/test/java_test_0.jsonl.gz creating: java/final/jsonl/valid/ inflating: java/final/jsonl/valid/java_valid_0.jsonl.gz inflating: java_dedupe_definitions_v2.pkl inflating: java_licenses.pkl . jsonl_list_to_dataframe method is directly from the CodeSearchNet Challenge example code and get_dfs is just a helper for you to properly grab the data into the correct training, validation, and testing splits. Let&#39;s see what your data looks like :D! . #collapse_show def jsonl_list_to_dataframe(file_list, columns=None): &quot;&quot;&quot;Load a list of jsonl.gz files into a pandas DataFrame.&quot;&quot;&quot; return pd.concat([pd.read_json(f, orient=&#39;records&#39;, compression=&#39;gzip&#39;, lines=True)[columns] for f in file_list], sort=False) def get_dfs(path): &quot;&quot;&quot;Grabs the different data splits and converts them into dataframes&quot;&quot;&quot; dfs = [] for split in [&quot;train&quot;, &quot;valid&quot;, &quot;test&quot;]: files = sorted((path/split).glob(&quot;**/*.gz&quot;)) df = jsonl_list_to_dataframe(files, [&quot;code&quot;, &quot;docstring&quot;]) dfs.append(df) return dfs df_trn, df_val, df_tst = get_dfs(path/&quot;java/final/jsonl&quot;) df_trn.head() . . code docstring . 0 protected final void bindIndexed(Configuration... | Bind indexed elements to the supplied collecti... | . 1 public void setServletRegistrationBeans( n t t... | Set {@link ServletRegistrationBean}s that the ... | . 2 public void addServletRegistrationBeans( n t t... | Add {@link ServletRegistrationBean}s for the f... | . 3 public void setServletNames(Collection&lt;String&gt;... | Set servlet names that the filter will be regi... | . 4 public void addServletNames(String... servletN... | Add servlet names for the filter. n@param serv... | . You are going to only use a small subset of the data in order to train your model in a reasonable time. However, if you want to adjust the amount of data used you can just adjust the sample size. . sample = 0.2 df_trn = df_trn.sample(frac = sample) df_val = df_val.sample(frac = sample) df_tst = df_tst.sample(frac = sample) . Awesome! Now that you have the data, there&#39;s a few other preprocessing steps you need to perform. First we are going to remove any non-english comments. Next, you will also remove the JavaDocs, i.e., any line with an @ symbol or curly braces, as that will significantlly lessen the amount of learning your model will have to do. This also works out well since the JavaDoc syntax can usually be autogenerated from the method&#39;s signature. . #collapse_show # From https://stackoverflow.com/a/27084708/5768407 def isASCII(s): try: s.encode(encoding=&#39;utf-8&#39;).decode(&#39;ascii&#39;) except UnicodeDecodeError: return False else: return True df_trn = df_trn[df_trn[&#39;docstring&#39;].apply(lambda x: isASCII(x))] df_val = df_val[df_val[&#39;docstring&#39;].apply(lambda x: isASCII(x))] df_tst = df_tst[df_tst[&#39;docstring&#39;].apply(lambda x: isASCII(x))] . . #collapse_show def filter_jdocs(df): methods = [] comments = [] for i, row in progress_bar(list(df.iterrows())): comment = row[&quot;docstring&quot;] # Remove {} text in comments from https://stackoverflow.com/questions/14596884/remove-text-between-and-in-python/14598135 comment = re.sub(&quot;([ { []).*?([ ) }])&quot;, &#39;&#39;, comment) cleaned = [] for line in comment.split(&#39; n&#39;): if &quot;@&quot; in line: break cleaned.append(line) comments.append(&#39; n&#39;.join(cleaned)) methods.append(row[&quot;code&quot;]) new_df = pd.DataFrame(zip(methods, comments), columns = [&quot;code&quot;, &quot;docstring&quot;]) return new_df df_trn = filter_jdocs(df_trn); df_val = filter_jdocs(df_val); df_tst = filter_jdocs(df_tst); . . Now you are going to remove any empty comments or duplicate comments for your datasets. . df_trn = df_trn[~(df_trn[&#39;docstring&#39;] == &#39;&#39;)] df_val = df_val[~(df_val[&#39;docstring&#39;] == &#39;&#39;)] df_tst = df_tst[~(df_tst[&#39;docstring&#39;] == &#39;&#39;)] . df_trn = df_trn[~df_trn[&#39;docstring&#39;].duplicated()] df_val = df_val[~df_val[&#39;docstring&#39;].duplicated()] df_tst = df_tst[~df_tst[&#39;docstring&#39;].duplicated()] . Not bad, still leaves you with plenty of data to learn with! . len(df_trn), len(df_val), len(df_tst) . (73755, 2427, 4615) . Exploring your data! . As a good machine learning practitioner, it is extremely important to be careful with your data. This includes checking for biases, duplicates, and also describing the data that you have. Not doing so is setting you up for disaster. I have personally experienced such travesty when working on one of my own research projects where I forgot to check for duplicates before splitting my data. Sadly for me and all my restless nights working on the project, the data was full of duplicates and so my test set was contaminated with data points from my training set, which lead to inflated evaluation metrics :(. . Always explore your data! . You&#39;ll be doing some basic descriptive statistics for this Exploratory Data Analysis (EDA), which just means calculating some means, medians, and standard deviations for different views of your data. The first view you will be exploring is the tokens that make up your code and comments. To tokenize your data into these tokens you will use something called Byte Pair Encoding, which has shown great results for tokenizing both natural language and code as shown in Karampatsis and Sutton&#39;s paper &quot;Maybe Deep Neural Networks are the Best Choice for Modeling Source Code.&quot; . A great resource on learning more about how Byte Pair Encoding works is this blog post by Akashdeep Singh Jaswal and this Youtube video by Christopher Manning. Specifically, you will be using the awesome library by Google called sentencepiece. . #collapse_show def df_to_txt_file(df, output, col): &quot;&quot;&quot;Converts a dataframe and converts it into a text file that SentencePiece can use to train a BPE model&quot;&quot;&quot; with open(output/&#39;text.txt&#39;, &#39;w&#39;) as f: f.write(&#39; n&#39;.join(list(df[col]))) return output/&#39;text.txt&#39; def gen_sp_model(df, output, tokenizer_name, col): &quot;&quot;&quot;Trains a SentencePiece BPE model from a pandas dataframe&quot;&quot;&quot; fname = df_to_txt_file(df, output, col) sp.SentencePieceTrainer.train(f&#39;--input={fname} --model_prefix={output / tokenizer_name} --hard_vocab_limit=false&#39;) . . To use Byte Pair Encoding, you have to train the tokenizer on your data. However, no need to train your BPE on all of your data, so you will just be doing it on a subset (10%) of your training data. You are picking to train the BPE model from your training set to not perform any inadvertant data snooping by biasing your BPE model to tokenize more common words in your validation or testing set. This also will help show that you are indeed solving the out of vocabulary problem because you will most likely encounter words in your testing set that were not in your training set. . p_bpe = 0.1 method_tokenizer = &quot;method_bpe&quot; gen_sp_model(df_trn.sample(frac = p_bpe), path, method_tokenizer, col = &quot;code&quot;) comment_tokenizer = &quot;comment_bpe&quot; gen_sp_model(df_trn.sample(frac = p_bpe), path, comment_tokenizer, col = &quot;docstring&quot;) . Now that you have the ability to tokenize your text, let us explore! First you will just generate the frequency of each of your tokens and while you are at it, let&#39;s collect how long your methods are by via the common software metric Lines of Code (LOC). . #collapse_show def get_counter_and_lens(df, spm, col): toks = [] locs = [] for i, row in progress_bar(list(df.iterrows())): toks.extend(spm.EncodeAsPieces(row[col])) locs.append(len(row[col].split(&#39; n&#39;))) cnt = Counter() for tok in progress_bar(toks): cnt[tok] += 1 return list(map(len, toks)), cnt, locs code_lens, code_cnt, locs = get_counter_and_lens(df_trn, method_spm, &#39;code&#39;) comment_lens, comment_cnt, _ = get_counter_and_lens(df_trn, method_spm, &#39;docstring&#39;) . . #collapse_show def plot_counts(counts, top_k = 30): labels, values = zip(*counts.most_common()[:top_k]) indexes = np.arange(len(labels)) width = 1 plt.figure(num=None, figsize=(22, 4), dpi=60, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.bar(indexes, values, width) plt.xticks(indexes + width * 0.5, labels) plt.show() plot_counts(code_cnt, top_k = 30) plot_counts(comment_cnt, top_k = 30) . . Plotting your frequencies as a bar chart you start to see a nice picture of your data. Not that suprising, but the most common token happens to be the period and some other common syntactical tokens like curly braces and also key words like if and return. . #collapse_show def plot_hist(lens, n_bins = 50): n, bins, patches = plt.hist(lens, n_bins, facecolor=&#39;blue&#39;, alpha=0.9) plt.show() print(mean(code_lens), median(code_lens), stdev(code_lens)) plot_hist(code_lens) print(mean(locs), median(locs), stdev(locs)) plot_hist(locs) print(mean(comment_lens), median(comment_lens), stdev(comment_lens)) plot_hist(comment_lens) . . 3.4662519750610943 3.0 2.6490695431339177 . 18.54957629991187 10 50.99032748692644 . 3.57512896650546 3.0 2.605938655784157 . As you can see, there are HTML elements left with &lt; and &gt; occurring quite often in your comments dataset that may make it harder for your model to learn to generate the comments that contain those elements. Luckily for us, it won&#39;t really affect your models accuracy, but exploring your data like this does allow us to see how your data may be influencing your model. . TODO For You: Perform some further cleaning steps to remove HTML and any other cleaning you deem necessary and see how your performance changes. . Loading the data using FastAI . Now that you have the data processed and cleaned you need a way to get it into the format that FastAI uses. To do that you will use some code from Rachel Thomas&#39; awesome course on NLP, which allows us to create a Sequence to Sequence (since you are going from the sequence of code to the sequence of the code&#39;s docstring) DataBunch (this is just the format FastAI uses for managing loading the data into memory for training and evaluating). . #collapse_show def seq2seq_collate(samples, pad_idx=1, pad_first=True, backwards=False): &quot;Function that collect samples and adds padding. Flips token order if needed&quot; samples = to_data(samples) max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples]) res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx if backwards: pad_first = not pad_first for i,s in enumerate(samples): if pad_first: res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1]) else: res_x[i, :len(s[0])],res_y[i, :len(s[1])] = LongTensor(s[0]),LongTensor(s[1]) if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1) return res_x, res_y class Seq2SeqDataBunch(TextDataBunch): &quot;Create a `TextDataBunch` suitable for training an RNN classifier.&quot; @classmethod def create(cls, train_ds, valid_ds, test_ds=None, path=&#39;.&#39;, bs=32, val_bs=None, pad_idx=1, dl_tfms=None, pad_first=False, device=None, no_check=False, backwards=False, **dl_kwargs): &quot;Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`&quot; datasets = cls._init_ds(train_ds, valid_ds, test_ds) val_bs = ifnone(val_bs, bs) collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards) train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2) train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs) dataloaders = [train_dl] for ds in datasets[1:]: lengths = [len(t) for t in ds.x.items] sampler = SortSampler(ds.x, key=lengths.__getitem__) dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs)) return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check) class Seq2SeqTextList(TextList): _bunch = Seq2SeqDataBunch _label_cls = TextList . . Here is where you are telling FastAI to use your trained BPE models for tokenizing your data. FastAI&#39;s tokenizers will also do some additional processing of your text such as lower casing all words, removing repetitions, etc. You can find a full list of the processing FastAI uses here. . method_processor = SPProcessor( sp_model = path / (method_tokenizer + &quot;.model&quot;), sp_vocab = path / (method_tokenizer + &quot;.vocab&quot;), include_eos = True) comment_processor = SPProcessor( sp_model = path / (comment_tokenizer + &quot;.model&quot;), sp_vocab = path / (comment_tokenizer + &quot;.vocab&quot;), include_eos = True) . Now that you have your BPE model you will generate the DataBunches suitable for your task, which will be the Seq2Seq DataBunch. You will also filter out sequences that your too long so that you can fit everything onto a Google Colab GPU and to not have your training take too long. . #collapse_show def gen_dbs(df_trn, df_val, df_tst, method_processor, comment_processor, bs = 96, max_seq = 128): is_valid = [False] * len(df_trn) + [True] * len(df_val) df_merged = pd.concat([df_trn, df_val]) df_merged = pd.DataFrame(zip(df_merged[&quot;code&quot;].to_list(), df_merged[&quot;docstring&quot;].to_list(), is_valid), columns = [&quot;code&quot;, &quot;docstring&quot;, &quot;valid&quot;] ) db_trn = (Seq2SeqTextList .from_df(df_merged, path = path, cols=&#39;code&#39;, processor = method_processor) .split_from_df(col=&#39;valid&#39;) .label_from_df(cols=&#39;docstring&#39;, label_cls=TextList, processor = comment_processor) .filter_by_func(lambda x, y: len(x) &gt; max_seq or len(y) &gt; max_seq) .databunch(bs = bs)) db_tst = (Seq2SeqTextList .from_df(df_tst, path = path, cols=&#39;code&#39;, processor = method_processor) .split_by_rand_pct(valid_pct = 0.01) .label_from_df(cols=&#39;docstring&#39;, label_cls=TextList, processor = comment_processor) .filter_by_func(lambda x, y: len(x) &gt; max_seq or len(y) &gt; max_seq) .databunch(bs = 16)) return db_trn, db_tst db_trn, db_tst = gen_dbs(df_trn, df_val, df_tst, method_processor, comment_processor, bs = 96, max_seq = 128) db_trn.show_batch() . . text target . ‚ñÅ xx b os ‚ñÅboolean ‚ñÅres er ve ( int ‚ñÅcolumn , ‚ñÅint ‚ñÅsize ) ‚ñÅ{ ‚ñÅif ‚ñÅ( ( column ‚ñÅ&lt; ‚ñÅ0) ‚ñÅ|| ‚ñÅ( ( column ‚ñÅ+ ‚ñÅsize ) ‚ñÅ&gt; ‚ñÅcolumns )) ‚ñÅthrow ‚ñÅnew ‚ñÅindex out of bound s exception (&quot; res er ve ‚ñÅ- ‚ñÅ inc or rec t ‚ñÅcolumn ‚ñÅ/ ‚ñÅsize &quot;); ‚ñÅfor ( int ‚ñÅi = column ; ‚ñÅi ‚ñÅ&lt; ‚ñÅcolumn ‚ñÅ+ ‚ñÅsize ; ‚ñÅi ++) ‚ñÅ{ | ‚ñÅ xx b os ‚ñÅ xx ma j ‚ñÅre s er ve s ‚ñÅa ‚ñÅ&lt; code &gt; ce ll &lt; ‚ñÅ/ ‚ñÅ xx up ‚ñÅcode &gt; ‚ñÅin ‚ñÅthe ‚ñÅ&lt; code &gt; ro w &lt; ‚ñÅ/ ‚ñÅ xx up ‚ñÅcode &gt; . ‚ñÅ xx e os | . ‚ñÅ xx b os ‚ñÅ@ help ( ‚ñÅ help ‚ñÅ= ‚ñÅ&quot; get ‚ñÅall ‚ñÅthe ‚ñÅ virtual network function descriptor ‚ñÅ xx m a j ‚ñÅdependency ‚ñÅof ‚ñÅa ‚ñÅnetwork service descriptor ‚ñÅwith ‚ñÅspecific ‚ñÅid &quot; ‚ñÅ) ‚ñÅpublic ‚ñÅlist &lt; v n f d ep end en cy &gt; ‚ñÅget v n f dependencies ( final ‚ñÅ xx m a j ‚ñÅstring ‚ñÅid ns d ) ‚ñÅthrows ‚ñÅs d k exception ‚ñÅ{ | ‚ñÅ xx b os ‚ñÅ xx ma j ‚ñÅreturn ‚ñÅa ‚ñÅ xx ma j ‚ñÅlist ‚ñÅwith ‚ñÅall ‚ñÅthe ‚ñÅv n f de p end en c ies ‚ñÅthat ‚ñÅare ‚ñÅcontain ed ‚ñÅin ‚ñÅa ‚ñÅspecific ‚ñÅnetwork service descriptor . ‚ñÅ xx e os | . ‚ñÅ xx b os ‚ñÅ@ override ‚ñÅpublic ‚ñÅvoid ‚ñÅdelete as set and attachment s ( final ‚ñÅ xx m a j ‚ñÅstring ‚ñÅas set id ) ‚ñÅthrows ‚ñÅ io exception , ‚ñÅrequest failure exception ‚ñÅ{ ‚ñÅ xx m a j ‚ñÅas set ‚ñÅas s ‚ñÅ= ‚ñÅget un v er ified as set ( as set id ); ‚ñÅlist &lt; attachment &gt; ‚ñÅattachment s ‚ñÅ= ‚ñÅas s . get attachment s | ‚ñÅ xx b os ‚ñÅ xx ma j ‚ñÅthis ‚ñÅwill ‚ñÅdelete ‚ñÅan ‚ñÅasset ‚ñÅand ‚ñÅall ‚ñÅits ‚ñÅattachments ‚ñÅ xx e os | . ‚ñÅ xx b os ‚ñÅpublic ‚ñÅlist &lt; character book mark fold er s response &gt; ‚ñÅget character s character id book mark s fold er s ( integer ‚ñÅcharacter id , ‚ñÅ xx m a j ‚ñÅstring ‚ñÅdata source , ‚ñÅ xx m a j ‚ñÅstring ‚ñÅif n one match , ‚ñÅ xx m a j ‚ñÅ integer ‚ñÅpage , ‚ñÅ xx m a j ‚ñÅstring ‚ñÅtoken ) ‚ñÅthrows ‚ñÅapi | ‚ñÅ xx b os ‚ñÅ xx ma j ‚ñÅlist ‚ñÅ bookmark ‚ñÅfolders ‚ñÅa ‚ñÅlist ‚ñÅof ‚ñÅyour ‚ñÅcharacter &amp; &#39; s ‚ñÅpersonal ‚ñÅ bookmark ‚ñÅfolders ‚ñÅ ‚ñÅ xx ma j ‚ñÅthis ‚ñÅroute ‚ñÅis ‚ñÅcached ‚ñÅfor ‚ñÅup ‚ñÅto ‚ñÅ36 00 ‚ñÅseconds ‚ñÅ xx up ‚ñÅ s so ‚ñÅ xx ma j ‚ñÅscope : ‚ñÅ esi - bookmark s . read _ character _ bookmark s . v 1 ‚ñÅ xx e os | . ‚ñÅ xx b os ‚ñÅ@ de pre c ated ‚ñÅprotected ‚ñÅfinal ‚ñÅmap &lt; db id , ‚ñÅk n n list &gt; ‚ñÅbatch n n ( n ‚ñÅnode , ‚ñÅdb id s ‚ñÅids , ‚ñÅint ‚ñÅk max ) ‚ñÅ{ ‚ñÅmap &lt; db id , ‚ñÅk n n list &gt; ‚ñÅres ‚ñÅ= ‚ñÅnew ‚ñÅhash map &lt;&gt;( id s . size ()); ‚ñÅfor ( db id iter ‚ñÅiter ‚ñÅ= ‚ñÅids . iter (); | ‚ñÅ xx b os ‚ñÅ xx ma j ‚ñÅperform s ‚ñÅa ‚ñÅbatch ‚ñÅk - ne a rest ‚ñÅneighbor ‚ñÅquery ‚ñÅfor ‚ñÅa ‚ñÅlist ‚ñÅof ‚ñÅquery ‚ñÅobjects . ‚ñÅ xx e os | . #collapse_show def shift_tfm(b): x,y = b y = F.pad(y, (1, 0), value=1) return [x,y[:,:-1]], y[:,1:] # Add the necessary shift transformation for training your Transformer model db_trn.add_tfm(shift_tfm) db_tst.add_tfm(shift_tfm) . . Defining your model . In this example, you will be using the Transformer architecture that was developed by Vaswani et. al.. If you want a better understanding of this model, I highly suggest The Annotated Transformer blog post and the NLP course by Rachel Thomas, which this model code is copied from. . #collapse class PositionalEncoding(nn.Module): &quot;Encode the position with a sinusoid.&quot; def __init__(self, d): super().__init__() self.register_buffer(&#39;freq&#39;, 1 / (10000 ** (torch.arange(0., d, 2.)/d))) def forward(self, pos): inp = torch.ger(pos, self.freq) enc = torch.cat([inp.sin(), inp.cos()], dim=-1) return enc class TransformerEmbedding(nn.Module): &quot;Embedding + positional encoding + dropout&quot; def __init__(self, vocab_sz, emb_sz, inp_p=0.): super().__init__() self.emb_sz = emb_sz self.embed = embedding(vocab_sz, emb_sz) self.pos_enc = PositionalEncoding(emb_sz) self.drop = nn.Dropout(inp_p) def forward(self, inp): pos = torch.arange(0, inp.size(1), device=inp.device).float() return self.drop(self.embed(inp) * math.sqrt(self.emb_sz) + self.pos_enc(pos)) def feed_forward(d_model, d_ff, ff_p=0., double_drop=True): layers = [nn.Linear(d_model, d_ff), nn.ReLU()] if double_drop: layers.append(nn.Dropout(ff_p)) return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model)) class MultiHeadAttention(nn.Module): def __init__(self, n_heads, d_model, d_head=None, p=0., bias=True, scale=True): super().__init__() d_head = ifnone(d_head, d_model//n_heads) self.n_heads,self.d_head,self.scale = n_heads,d_head,scale self.q_wgt,self.k_wgt,self.v_wgt = [nn.Linear( d_model, n_heads * d_head, bias=bias) for o in range(3)] self.out = nn.Linear(n_heads * d_head, d_model, bias=bias) self.drop_att,self.drop_res = nn.Dropout(p),nn.Dropout(p) self.ln = nn.LayerNorm(d_model) def forward(self, q, kv, mask=None): return self.ln(q + self.drop_res(self.out(self._apply_attention(q, kv, mask=mask)))) def create_attn_mat(self, x, layer, bs): return layer(x).view(bs, x.size(1), self.n_heads, self.d_head ).permute(0, 2, 1, 3) def _apply_attention(self, q, kv, mask=None): bs,seq_len = q.size(0),q.size(1) wq,wk,wv = map(lambda o: self.create_attn_mat(*o,bs), zip((q,kv,kv),(self.q_wgt,self.k_wgt,self.v_wgt))) attn_score = wq @ wk.transpose(2,3) if self.scale: attn_score /= math.sqrt(self.d_head) if mask is not None: attn_score = attn_score.float().masked_fill(mask, -float(&#39;inf&#39;)).type_as(attn_score) attn_prob = self.drop_att(F.softmax(attn_score, dim=-1)) attn_vec = attn_prob @ wv return attn_vec.permute(0, 2, 1, 3).contiguous().view(bs, seq_len, -1) def get_output_mask(inp, pad_idx=1): return torch.triu(inp.new_ones(inp.size(1),inp.size(1)), diagonal=1)[None,None].byte() class EncoderBlock(nn.Module): &quot;Encoder block of a Transformer model.&quot; #Can&#39;t use Sequential directly cause more than one input... def __init__(self, n_heads, d_model, d_head, d_inner, p=0., bias=True, scale=True, double_drop=True): super().__init__() self.mha = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale) self.ff = feed_forward(d_model, d_inner, ff_p=p, double_drop=double_drop) def forward(self, x, mask=None): return self.ff(self.mha(x, x, mask=mask)) class DecoderBlock(nn.Module): &quot;Decoder block of a Transformer model.&quot; #Can&#39;t use Sequential directly cause more than one input... def __init__(self, n_heads, d_model, d_head, d_inner, p=0., bias=True, scale=True, double_drop=True): super().__init__() self.mha1 = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale) self.mha2 = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale) self.ff = feed_forward(d_model, d_inner, ff_p=p, double_drop=double_drop) def forward(self, x, enc, mask_out=None): return self.ff(self.mha2(self.mha1(x, x, mask_out), enc)) . . #collapse_show class Transformer(Module): def __init__(self, inp_vsz, out_vsz, n_layers=6, n_heads=8, d_model=256, d_head=32, d_inner=1024, p=0.1, bias=True, scale=True, double_drop=True, pad_idx=1): self.enc_emb = TransformerEmbedding(inp_vsz, d_model, p) self.dec_emb = TransformerEmbedding(out_vsz, d_model, 0.) args = (n_heads, d_model, d_head, d_inner, p, bias, scale, double_drop) self.encoder = nn.ModuleList([EncoderBlock(*args) for _ in range(n_layers)]) self.decoder = nn.ModuleList([DecoderBlock(*args) for _ in range(n_layers)]) self.out = nn.Linear(d_model, out_vsz) self.out.weight = self.dec_emb.embed.weight self.pad_idx = pad_idx def forward(self, inp, out): mask_out = get_output_mask(out, self.pad_idx) enc,out = self.enc_emb(inp),self.dec_emb(out) enc = compose(self.encoder)(enc) out = compose(self.decoder)(out, enc, mask_out) return self.out(out) . . To evaluate your model you will be using the commonly used BLEU score, which is a measure for determining how closely your model&#39;s generated comment is to the real comment of a method. (This code is also copied from the NLP tutorial from Rachel Thomas) . #collapse class NGram(): def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n def __eq__(self, other): if len(self.ngram) != len(other.ngram): return False return np.all(np.array(self.ngram) == np.array(other.ngram)) def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)])) def get_grams(x, n, max_n=5000): return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)] def get_correct_ngrams(pred, targ, n, max_n=5000): pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n) pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams) return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams) class CorpusBLEU(Callback): def __init__(self, vocab_sz): self.vocab_sz = vocab_sz self.name = &#39;bleu&#39; def on_epoch_begin(self, **kwargs): self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4 def on_batch_end(self, last_output, last_target, **kwargs): last_output = last_output.argmax(dim=-1) for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()): self.pred_len += len(pred) self.targ_len += len(targ) for i in range(4): c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz) self.corrects[i] += c self.counts[i] += t def on_epoch_end(self, last_metrics, **kwargs): precs = [c/t for c,t in zip(self.corrects,self.counts)] len_penalty = exp(1 - self.targ_len/self.pred_len) if self.pred_len &lt; self.targ_len else 1 bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25) return add_metrics(last_metrics, bleu) . . n_x_vocab, n_y_vocab = len(db_trn.train_ds.x.vocab.itos), len(db_trn.train_ds.y.vocab.itos) model = Transformer(n_x_vocab, n_y_vocab, d_model=256) learn = Learner(db_trn, model, metrics=[accuracy, CorpusBLEU(n_y_vocab)], loss_func = CrossEntropyFlat()) . Now you are going to use the awesome Learning Rate finder provided by FastAI, which is based on the awesome paper from Leslie N. Smith &quot;Cyclical Learning Rates for Training Neural Networks&quot;. This way you don&#39;t have to do a bunch of hyperparameter searching to find the perfect fit. . learn.lr_find() learn.recorder.plot(suggestion = True) . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss accuracy bleu time . &lt;progress value=&#39;92&#39; class=&#39;&#39; max=&#39;423&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 21.75% [92/423 01:21&lt;04:53 19.5654] &lt;/div&gt; &lt;/div&gt; LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. Min numerical gradient: 3.02E-03 Min loss divided by 10: 1.74E-02 . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; It is common to pick a point a bit before the suggested point. . max_lr = 5e-4 . DRUM ROLL PLEASE!!!!! You are now going to finally start training your model! Specifically for 8 epochs because that was what was in the original code in the NLP course and it also happened to work the best during my training. However, you are also implementing a few call backs, namely automatically saving the best performing model, early stopping, and showing the training and validation loss graph. Since you are using early stopping, feel free to try out a higher epoch number and the training will stop once it starts not improving. . def train_model(learn, epochs, model_name, max_lr = 5e-4): &quot;&quot;&quot;Trains a model using save model, early stopping, and show graph call backs.&quot;&quot;&quot; callback_fns = [ callbacks.SaveModelCallback( learn, every=&#39;improvement&#39;, monitor=&#39;valid_loss&#39;, name=f&#39;{model_name}_save_model&#39; ), callbacks.EarlyStoppingCallback( learn, monitor=&#39;valid_loss&#39;, min_delta = 0.01, patience = 3 ), ShowGraph(learn) ] learn.fit_one_cycle(epochs, max_lr, div_factor=5, callbacks = callback_fns) . epochs = 8 model_name = &#39;comment_gen&#39; . Training on Google Colab can take anywhere from ~20 to 60 minutes depending on the type of GPU they give you. So, relax, get an IV caffeine drip going, and let your model cook in peace :). . train_model(learn, epochs, model_name, max_lr = max_lr) . epoch train_loss valid_loss accuracy bleu time . 0 | 1.182219 | 1.133453 | 0.828182 | 0.791774 | 06:46 | . 1 | 0.920205 | 0.954264 | 0.841556 | 0.799681 | 06:47 | . 2 | 0.812330 | 0.875513 | 0.849487 | 0.804000 | 06:44 | . 3 | 0.752023 | 0.828835 | 0.853668 | 0.807183 | 06:45 | . 4 | 0.679716 | 0.794862 | 0.856593 | 0.809325 | 06:43 | . 5 | 0.653454 | 0.777795 | 0.859418 | 0.811010 | 06:42 | . 6 | 0.611860 | 0.770059 | 0.860419 | 0.812164 | 06:49 | . 7 | 0.605370 | 0.769881 | 0.860601 | 0.812119 | 06:45 | . Better model found at epoch 0 with valid_loss value: 1.133453130722046. . Better model found at epoch 1 with valid_loss value: 0.9542644023895264. Better model found at epoch 2 with valid_loss value: 0.8755126595497131. Better model found at epoch 3 with valid_loss value: 0.8288350701332092. Better model found at epoch 4 with valid_loss value: 0.7948615550994873. Better model found at epoch 5 with valid_loss value: 0.7777946591377258. Better model found at epoch 6 with valid_loss value: 0.7700592279434204. Better model found at epoch 7 with valid_loss value: 0.7698812484741211. . Evaluate your model . Let us now evaluated your trained model on some of your validation set so see how well your model is generating comments. . #collapse_show def get_predictions(learn, ds_type=DatasetType.Valid): learn.model.eval() inputs, targets, outputs = [],[],[] with torch.no_grad(): for xb,yb in progress_bar(learn.dl(ds_type)): out = learn.model(*xb) for x,y,z in zip(xb[0],xb[1],out): inputs.append(learn.data.train_ds.x.reconstruct(x.cpu())) targets.append(learn.data.train_ds.y.reconstruct(y.cpu())) outputs.append(learn.data.train_ds.y.reconstruct(z.cpu().argmax(1))) return inputs, targets, outputs inputs, targets, outputs = get_predictions(learn) . . #collapse_show def print_results(inputs, targets, outputs, method_spm, comment_spm, n = 10): &quot;&quot;&quot;Just a little helper function for printing out the results from your model.&quot;&quot;&quot; for i in range(n): print(&quot;Input:&quot;, &quot; &quot;.join(decode_spec_tokens(method_spm.DecodePieces(str(inputs[i]).split(&quot; &quot;)).split(&quot; &quot;))), &quot; n&quot;) print(&quot;Target:&quot;, &quot; &quot;.join(decode_spec_tokens(comment_spm.DecodePieces(str(targets[i]).split(&quot; &quot;)).split(&quot; &quot;))), &quot; n&quot;) print(&quot;Predicted:&quot;, &quot; &quot;.join(decode_spec_tokens(comment_spm.DecodePieces(str(outputs[i]).split(&quot; &quot;)).split(&quot; &quot;))), &quot; n&quot;) print_results(inputs, targets, outputs, method_spm, comment_spm) . . Input: xxbos @doesservicerequest private void putrangeinternal(final filerange range, final filerangeoperationtype operationtype, final byte[] data, final long length, final String md5, final accesscondition accesscondition, final filerequestoptions options, final operationcontext opcontext) throws storageexception { executionengine.executewithretry(this.fileserviceclient, this, putrangeimpl(range, operationtype, data, length, md5, accesscondition, options, opcontext), options.getretrypolicyfactory(), opcontext); } xxeos Target: xxbos Used for both uploadrange and clearrange. xxeos Predicted: xxbos Put to creating thes( s( xxeos Input: xxbos public static byte[] encodesequence(byte[]... encodedvalues) { int length = 0; for (byte[] encodedvalue : encodedvalues) { length += encodedvalue.length; } byte[] lengthencoded = encodelength(length); bytearraydataoutput out = bytestreams.newdataoutput(1 + lengthencoded.length + length); out.write(sequence_tag); out.write(lengthencoded); for (byte[] entry : encodedvalues) { out.write(entry); } return out.tobytearray(); } xxeos Target: xxbos Encodes a sequence of encoded values. xxeos Predicted: xxbos Encodes a byte of bytes bytes into xxeos Input: xxbos @override public String dnsresolveex(string host) { stringbuilder result = new stringbuilder(); try { inetaddress[] list = inetaddress.getallbyname(host); for (inetaddress inetaddress : list) { result.append(inetaddress.gethostaddress()); result.append(&#34;; &#34;); } } catch (unknownhostexception e) { log.log(level.fine, &#34;DNS name not resolvable {0}.&#34;, host); } return result.tostring(); } xxeos Target: xxbos *********************************************************************** dnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveexdnsresolveex xxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeosxxeos Predicted: xxbos xxmap 51 * namepp =pxxeos Input: xxbos protected void removeallfromattributevalueset() { final collection&lt;abstracthtml5sharedobject&gt; sharedobjects = getsharedobjects(); boolean listenerinvoked = false; final collection&lt;writelock&gt; writelocks = lockandgetwritelocks(); try { getattributevalueset().clear(); setmodified(true); invokevaluechangelisteners(sharedobjects); listenerinvoked = true; } finally { for (final Lock lock : writelocks) { lock.unlock(); } } pushqueues(sharedobjects, listenerinvoked); } xxeos Target: xxbos clears all values from the value set. xxeos Predicted: xxbos s all attribute from the xxeos Input: xxbos public void registercheckwithnotes(string checkid, String name, String script, long interval, @suppresswarnings(&#34;sameparametervalue&#34;) String notes) { Check check = new Check(); check.setid(checkid); check.setname(name); check.setscript(script); check.setinterval(string.format(&#34; ‚Åá ss&#34;, interval)); check.setnotes(notes); registercheck(check); } xxeos Target: xxbos Registers a Health Check with the Agent. xxeos Predicted: xxbos Registers a xxupj ealth checkxxmaj check a givenxxmaj name xxeos Input: xxbos public void assertequalsignoringcase(@nullable Description description, @nullable String actual, @nullable String expected) { if (!areequalignoringcase(actual, expected)) { String format = &#34;expecting:&lt; ‚Åá s&gt; to be equal to:&lt; ‚Åá s&gt;, ignoring case considerations&#34;; throw failures.failure(description, new basicerrormessagefactory(format, actual, expected)); } } xxeos Target: xxbos Verifies that two s are equal, ignoring case considerations. xxeos Predicted: xxbos Assert that the stringsxx are equal, oring the... xxeos Input: xxbos protected cronschedulebuilder createcronschedulebuilder(string cronexpr) { int i = cronexpr.indexof(&#34;[&#34;); int j = cronexpr.indexof(&#34;]&#34;); timezone timezone = defaulttimezone; if (i &gt; -1 &amp;&amp; j &gt; -1) { timezone = timezone.gettimezone(cronexpr.substring(i+1, j)); cronexpr = cronexpr.substring(0, i).trim(); } return cronschedulebuilder.cronschedule(cronexpr).intimezone(timezone); } xxeos Target: xxbos Allow timezone to be configured on a per-cron basis with [timezonename] appended to the cron format xxeos Predicted: xxbos Create to to create used to the mtimei-s of a0],]. to the givenath.xxeos Input: xxbos private &lt;T&gt; fakeencodeditem readnextitem(class&lt;t&gt; clazz) { fakeencodeditem item = data[dataposition]; if (item == null) { / / While Parcel will treat these as zeros, in tests, this is almost always an error. throw new unreliablebehaviorerror(&#34;reading uninitialized data at position &#34; + dataposition); } checkconsistentreadandincrementposition(clazz, item); return item; } xxeos Target: xxbos Reads a complete item in the byte buffer. xxeos Predicted: xxbos Read the from the given array. xxeos Input: xxbos private void hidesuggestionsifnecessary(final @nonnull querytoken currentquery, final @nonnull tokensource source) { String queryts = currentquery.gettokenstring(); String currentts = source.getcurrenttokenstring(); if (!iswaitingforresults(currentquery) &amp;&amp; queryts != null &amp;&amp; queryts.equals(currentts)) { msuggestionsvisibilitymanager.displaysuggestions(false); } } xxeos Target: xxbos Hides the suggestions if there are no more incoming queries. xxeos Predicted: xxbos Check the givenion of of the is no more . xxeos Input: xxbos public list&lt;uirow&gt; getvalues() throws efapsexception { list&lt;uirow&gt; ret = new arraylist&lt;&gt;(); if (isfiltered()) { for (final uirow row : this.values) { boolean filtered = false; for (final tablefilter filter : this.filters.values()) { filtered = filter.filterrow(row); if (filtered) { break; } } if (!filtered) { ret.add(row); } } } else { ret = this.values; } setsize(ret.size()); return ret; } xxeos Target: xxbos This is the getter method for the instance variable . xxeos Predicted: xxbos Returns method a first method for the row of. xxeos . This is great and all. However, you can see the text looks a bit off. Your model sort of starts generating some word and then switches half way through sometimes. This is because the way you are currently trying to generate tokens is using Teacher Forcing, which means you are giving the model the groundtruth for what it should have produced even if it did not. This is very helpful during training, however, it expects to have both the x and y of an input. In a real world setting, you aren&#39;t going to be given the y, obviously! . Therefore, I found a hacky way of bypassing this need for the y so that it is no longer needed. This involves using an empty array that fakes being the y, but has only ones and is updated everytime the model makes a prediction and is then fed back into the model so that is knows what it has generated before. . Heads Up The way I coded this is extremely inefficient and so running it will take a long time to generate predictions. Therefore I recommend only generating a few comments (I set it up to only do 10). . TODO For You: Come up with a more efficient solution that performs similarly to the Teacher Forcing approach of the above code. . P.S. . For other language learners provided by FastAI, you can simply use the predict function and pass some text and ask for the model to predict the next set of tokens. However, I have been unsuccessful in implementing this predict function for Sequence to Sequence models. So, another TODO For You is to see if you can implement a predict function for Sequence to Sequence models so that you can easily generate comments for methods that you just pass to the function! . If you do figure out a way to do this, I would be extremely interested! So, feel free to leave a comment about it. . #collapse_show def get_preds(learn, db_tst, max_seq = 128, n = 10): learn.model.eval() inpts, trgts, preds = [], [], [] for i, (xb,yb) in enumerate(progress_bar(db_tst.dl(DatasetType.Train))): if i &gt;= n: break res = torch.zeros(len(xb[0]), max_seq, device = torch.device(&#39;cuda&#39;)).long() + 1 for i in range(max_seq - 1): outs = learn.model(xb[0], res) for j, out in enumerate(outs): res[j][i + 1] = out.argmax(1)[i] for x, y, z in zip(xb[0], yb, res): inpts.append(str(learn.data.train_ds.x.reconstruct(x.cpu()))) trgts.append(str(db_tst.train_ds.y.reconstruct(y.cpu()))) preds.append(str(learn.data.train_ds.y.reconstruct(z.cpu()))) return inpts, trgts, preds inputs, targets, outputs = get_preds(learn, db_tst) print_results(inputs, targets, outputs, method_spm, comment_spm) . . &lt;progress value=&#39;10&#39; class=&#39;&#39; max=&#39;149&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 6.71% [10/149 01:14&lt;17:21] Input: xxbos protected String parseunquotedstringcontent() { final int startndx = ndx; while (true) { final char c = input[ndx]; if (c &lt;= &#39; &#39; || charutil.equalsone(c, UNQUOTED_DELIMETERS)) { final int currentndx = ndx; / / done skipwhitespaces(); return new String(input, startndx, currentndx - startndx); } ndx++; } } xxeos Target: xxbos Parses un-quoted string content. xxeos Predicted: xxbos Parses the text from the HTML text. xxeos Input: xxbos private static void checkfilecopy(final File srcfile, final File destfile) throws ioexception { checkexists(srcfile); checkisfile(srcfile); if (equals(srcfile, destfile)) { throw new ioexception(&#34;files &#39;&#34; + srcfile + &#34;&#39; and &#39;&#34; + destfile + &#34;&#39; are equal&#34;); } File destparent = destfile.getparentfile(); if (destparent != null &amp;&amp; !destparent.exists()) { checkcreatedirectory(destparent); } } xxeos Target: xxbos Checks that file copy can occur. xxeos Predicted: xxbos Checks if the file is a file. xxeos Input: xxbos long analyze() { Arc a; Arc aa; if (pre.outs == null) { return flags.reg_uimpossible; } for (a = pre.outs; a != null; a = a.outchain) { for (aa = a.to.outs; aa != null; aa = aa.outchain) { if (aa.to == post) { return flags.reg_uemptymatch; } } } return 0; } xxeos Target: xxbos analyze - ascertain potentially-useful facts about an optimized NFA xxeos Predicted: xxbos Returns the Syoooo Syna Syna Syna Sa Sa Sa Sa Sa Sa Sa Sa syna Syna xxeos Input: xxbos @suppresswarnings(&#34;unchecked&#34;) public REC next() { checkdirection(true); orecord record; / / ITERATE UNTIL THE NEXT GOOD RECORD while (hasnext()) { / / FOUND if (currentrecord != null) { try { return (REC) currentrecord; } finally { currentrecord = null; } } record = gettransactionentry(); if (record != null) return (REC) record; } return null; } xxeos Target: xxbos Return the element at the current position and move forward the cursor to the next position available. xxeos Predicted: xxbos Returns the next record in the queue. xxeos Input: xxbos public static void addtransitivematches(hollowreadstateengine stateengine, map&lt;string, bitset&gt; matches) { list&lt;hollowschema&gt; schemalist = hollowschemasorter.dependencyorderedschemalist(stateengine); collections.reverse(schemalist); for(hollowschema schema : schemalist) { bitset currentmatches = matches.get(schema.getname()); if(currentmatches != null) { addtransitivematches(stateengine, schema.getname(), matches); } } } xxeos Target: xxbos Augment the given selection by adding the references, and the &lt;i&gt;transitive&lt; / i&gt; references, of our selection. xxeos Predicted: xxbos Add a variable to the list of s. xxeos Input: xxbos protected void resolvenestedproperties(final beanproperty bp) { String name = bp.name; int dotndx; while ((dotndx = indexofdot(name)) != -1) { bp.last = false; bp.setname(name.substring(0, dotndx)); bp.updatebean(getindexproperty(bp)); name = name.substring(dotndx + 1); } bp.last = true; bp.setname(name); } xxeos Target: xxbos Resolves nested property name to the very last indexed property. If forced, &lt;code&gt;null&lt; / code&gt; or non-existing properties will be created. xxeos Predicted: xxbos Resolve the property name. xxeos Input: xxbos public xmlconfig declarenamespace(string prefix, String namespaceuri) { validate.notempty(prefix, &#34;prefix cannot be empty&#34;); validate.notempty(namespaceuri, &#34;namespace URI cannot be empty&#34;); map&lt;string, String&gt; updatednamespaces = new hashmap&lt;string, string&gt;(declarednamespaces); updatednamespaces.put(prefix, namespaceuri); return new xmlconfig(features, updatednamespaces, properties, validating, true, allowdoctypedeclaration, true); } xxeos Target: xxbos Declares a namespace and also sets } to &lt;code&gt;true&lt; / code&gt;. &lt;p / &gt; &lt;p&gt;note that you cannot use this to add namespaces for the matcher. This has to be done by providing a to the matcher instance.&lt; / p&gt; xxeos Predicted: xxbos Creates a new instance of the given name and the given name. xxeos Input: xxbos protected static int getshadowradius(drawable shadow, Drawable circle) { int radius = 0; if (shadow != null &amp;&amp; circle != null) { Rect rect = new Rect(); radius = (circle.getintrinsicwidth() + (shadow.getpadding(rect) ? rect.left + rect.right : 0)) / 2; } return Math.max(1, radius); } xxeos Target: xxbos Calculates required radius of shadow. xxeos Predicted: xxbos Get the Syyyy Syyy Syyy Syna Syna Syna Syna Syna Sna Syna Syna Syna xxeos Input: xxbos void addadviceclinitmethod(final String name) { if (adviceclinits == null) { adviceclinits = new arraylist&lt;&gt;(); } adviceclinits.add(name); } xxeos Target: xxbos Saves used static initialization blocks (clinit) of advices. xxeos Predicted: xxbos Adds a Java class to the Saa Sa Sa Syyetch Bean. xxeos Input: xxbos public static String padleft(string s, int desiredlength, String padstring) { while (s.length() &lt; desiredlength) { s = padstring + s; } return s; } xxeos Target: xxbos Pad the given string with padstring on the left up to the given length. xxeos Predicted: xxbos Compares two strings, and returns the first character in the string. xxeos . Not too shabby if I do say so myself! It seems to actually be learning about what a comment is supposed to have in it for documenting what the method is doing. Of course there are a lot of tweaks you could do such as adding the ability to generate inline comments instead of just method level, using more data, using different sampling schemes for generating the comments such as top-k or nucleus, and any other awesome things you could think of! And if you do feel free to leave a comment about your adventure. . Tip: I have done a lot of fiddling to get this to work, however, most of my models ended up overfitting. The way I fixed this issue was just being more careful in how I clean the data and also increase the data size. I know this seems simple, but it is quite effective. . Conclusion . In this tutorial, you created an Automatic Code Comment Generator! You learned about how to clean, explore, and process data and how to use the awesome Pytorch and FastAI to define and train the awesome Transformer architecture. The use of Deep Learning in the field of Software Engineering is what I am studying for my Ph.D., so I hope I have inspired you to think about some other ways you could use Deep Learning to help Software Engineering! . I hope you enjoyed this tutorial and look out for future blog posts from me about all kinds of topics as I have announced a challenge for myself for learning new things this year! Okay, I&#39;m posing a #challenge to myself: Each month devote an hour a day to learning about a subject I am weak at. At the end of the month, post a blog summarizing what you&#39;ve learned.March is devoted to #neuroscience! Already signed up for #edX course!#ChallengeAccepted üòé . &mdash; Nathan Cooper #Masks4All (@ncooper57) March 5, 2020 . &lt;/div&gt; .",
            "url": "https://nathancooper.io/i-am-a-nerd/deep_learning/software_engineering/2020/03/07/How_to_Create_an_Automatic_Code_Comment_Generator_using_Deep_Learning.html",
            "relUrl": "/deep_learning/software_engineering/2020/03/07/How_to_Create_an_Automatic_Code_Comment_Generator_using_Deep_Learning.html",
            "date": " ‚Ä¢ Mar 7, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Awesome Things I Learned Creating My Own Website",
            "content": "Hello, Solar System! (As a space faring civilization, I feel it only customary we update our greetings to reflect such awesome accomplishments ü§ì) I am a nerd and hopefully you are as well. . This post goes over many of the awesome technologies, resources, and overall tips and tricks I learned while creating my own personal website! This post is NOT a tutorial, mostly because there are tons of already existing ones on how to create a website and you don‚Äôt want to create a website or even my website (though mine is pretty awesome), you want you create your own website. For me this came from a lot of trial and error and tons of random google searches to fit some niche feature I wanted to add. So, this post is to centralize all of the niche features that went into my website in case any of you out there want to personalize some for your own website. This post is not really made to be gone through end to end, but rather for you to pick out the pieces that resonate best with you and give you inspiration for your own website. . Let‚Äôs get some of the boring stuff out of the way first, namely these are the main components that my website is made out of: . ReactJS - using create-react-app and . | Material-UI - for the style points üòé (Sadly not as delicious as brownie points) . | GitHub Pages - for hosting the static site (ty GitHub &lt;3), requires you use gh-pages and setup your create-react-app up correctly. Here is some documentation on how that is done . | GitHub and GitHub Pages - for storing my projects and for storing the web demos of my projects (Sounds epic, right?!?!? More on this later) . | Redux - for storing and updating state of ReactJS thingies like lists of current projects and blog posts (More on this later) . | ReactMarkdown - for rendering my blog posts, which, you guessed it, are markdown files! . | Paperclips - I couldn‚Äôt afford duct tape :( . | . Niche Features and Tips . Resume of Coding Projects: . The core motivation behind my website was that I wanted a cool way to show off some of my projects that I‚Äôve created over the years. I‚Äôve seen how others create theirs and actually got inspired in part by https://nmarch213.github.io/Portfolio/#/projects. However, the issue is that these displays of projects need to be manually created and that was a problem for me because like most developers I am lazy and I don‚Äôt want to do that. I wanted a way where I could just create new projects and they would be automatically added in the correct format, including images, titles, descriptions, etc. I could just redirect users to my GitHub page, where I place all of my coding projects, but that seemed like a cop out and did not allow for any customization. So, like any good programmer, I made a scrapper that took the output from GitHub and converted into a format for my own usage üòä. This gave my website the ability to update the list of projects automatically as I added new repositories, which the title of each project is just determined by the repository‚Äôs name. To add an image to be displayed as the project‚Äôs logo, I just add an icon.png file to the root of the project and grab the icon from there when displaying the list. . To scrape all this information, I use the amazing GitHub API v3 that GitHub provides. This API offers a ton of useful features, but for scrapping my projects I specifically used the Repositories API. This also has information like the repository‚Äôs description, so you could include that information in your list of projects automatically if you so choose. The GitHub API v3 has a bunch of awesome functionality, another API I use is the Contents API for listing out the different posts I have created (more on this later)! For integrating these APIs using ReactJS, I suggest using Redux for storing the state, i.e., the projects and blog posts once they have returned from the GitHub API, and Axios for actually making the HTTPS requests. . Besides just hosting one website using GitHub Pages, you can have one per repository. Incorporating this with the dynamic list of my coding projects, I am able to now create their own page that can serve as documentation or can even be a web demo! Currently I am not using this feature to the best of its ability, but I plan on overhauling my more put together projects so that they have at the very least some documentation using this awesome feature. . Overall, using very simple components offered by GitHub I am able to have my custom resume of projects that dynamically updates when I create a new one and links to documentation or a web demo of the project. Any updates to the actual project are reflected on the website without additional changes to some configuration file that contains all of the projects I have. . Blog: . I now do all of my blogging using the new awesome fastpages library built by Hamel Husain and Jeremy Howard! . [Deprecated] . Aside from being able to high-light my accomplishments, I wanted to be able to express myself on a multitude of topics. I never thought I would be a blogger, but a blog post by the awesome Rachel Thomas, Why you (yes, you) should blog, inspired me to take it seriously. I tried with Medium, but I wanted something of my own and when I saw the equally awesome Jeremy Howard discussing his work on Fast Template that allows you to easily create your own personal blog using GitHub Pages and simple Markdown files I knew the time was now to commit. Now while I do not directly use Fast Template, because it is a bit too rigid for the amount of customizability I like to perform, I drew a lot of inspiration, namely writing blog posts as markdown files and having them stored statically on my website instead of on some weird MongoDB. To integrate this concept of using Markdown files I needed a way to render them easily using React, which is where I found this awesomely customizable library for doing just that called React Markdown. What‚Äôs nice about React Markdown is that each of the formatting components such as code snippets and headings are separated into their own rendering engine allowing you to swap out or customize them very easily. So since I quite enjoy dark theme, I found this awesome post by Bexultan A. Myrzatayev showing how you can use a custom react highlighting engine, I use React Syntax Highlighter, and swap out the default theme for code syntax highlighting for something like ‚ÄúatomOneDark‚Äù! (obviously I chose a dark theme, because dark theme is the only theme) . To write my posts, I don‚Äôt just write directly using Markdown as that would be extremely painful. I took the advice of Jeremy Howard again from his post on Syncing your blog with your PC, and using your word processor and used a word processor, in my case it is Google Docs. Sadly, Google Docs does not allow you to directly export your document as a Markdown file, but thankfully an awesome person named Mangini created gdocs2md that does the conversion and handles grabbing the images and emails them to you! . Material Design: . As a nerd and mostly a computer nerd, my artistic skills are not the best. However, I wanted my website to look stylish, clean, modern, cool, hip‚Ä¶. (words I searched google for when trying to create a pretty website). This brought me to Google, they always create such chic looking websites and mobile apps, in my opinion obviously, so it wasn‚Äôt too surprising to learn that Google wrote the bo‚Ä¶ well, website for designing GUI components, which they called Material Design. This is where Material-UI comes in. It is a complete React component implementation of the Material Design language and it looks smooootthhhhh. Using it is quite simple as laid out in there website, but I‚Äôm including a code snippet because I want to flex how my website is able to render code snippets courtesy of React Markdown and Bexultan A. Myrzatayev‚Äôs awesome post on how to change the theme used in code snippets : . Code snippet from what a project entry looks like: . import { Button, Card, CardActions, CardContent, CardMedia, Typography } from &quot;@material-ui/core&quot;; ‚Ä¶. const site = has_pages ? ( &lt;Button variant=&quot;contained&quot; color=&quot;secondary&quot; href={pages_url}&gt; View Site &lt;/Button&gt; ) : null; return ( &lt;Card className={classes.card} fullWidth&gt; &lt;CardMedia className={classes.media} image={icon_src} onError={e =&gt; { console.log(&quot;cannot find icon&quot;); }} /&gt; &lt;CardContent&gt; &lt;Typography gutterBottom variant=&quot;headline&quot; component=&quot;h4&quot;&gt; {_.startCase(_.camelCase(name))} &lt;/Typography&gt; &lt;/CardContent&gt; &lt;CardActions&gt; {site} &lt;Button variant=&quot;contained&quot; color=&quot;primary&quot; href={html_url}&gt; View Repo &lt;/Button&gt; &lt;/CardActions&gt; &lt;/Card&gt; ); ‚Ä¶. . Development: . I think programming is an invaluable skill that I am constantly learning to improve. So, I highly recommend those creating their own website to at least try to program it themselves. It is an adventure of pain and misery that I wish to inflict onto others, hahaha, haha, ha‚Ä¶ But extremely rewarding when you finally see that beautiful glowing (please don‚Äôt make your website glow, it‚Äôs annoying) website plastered on your web browser (please let it be Chromium based or just basically not Explorer). All programmers need some system in which to develop whatever it is they care about creating. This is where things like Integrated Development Environments (IDEs), debuggers, and testing frameworks come in handy. . To keep myself sane, I spent a long, arduous, and tedious time trying and experimenting with different workflows for developing systems. And I have found the holy grail that has answered all of my questions and that allows for 10X greater productivity for *me *(your results will most certainly differ if you decide to use the same developmental setup). For me, I found the combination of Visual Studio Code and Docker to be the textbook definition of perfection. In particular, the Remote Container extension that some genius made. This extension allows you to connect your vscode editor to a docker container‚Äôs file system. So, why is this so important to me? Well Docker allows you to spin up pretty much any environment you want such as a node server for hosting a ReactJS website :D, but most importantly it allows you to version and share these environments through Dockerfiles. This allows me to specify an environment per project, so I don‚Äôt have to maintain installing all of the dependencies that may conflict with each other on my local machine. This is why I use Docker for pretty much everything I do and I also quite enjoy using vscode, so being able to marry the two is absolute perfection! . Conclusion . So, that concludes my first blog post! I hope you are able to use some of these awesome things I learned while creating my own website for your own. Keep a lookout for my future posts, I am planning on creating posts circulating around the following topics: . Machine Language Processing (MLP) - like Natural Language Processing (NLP), but for computer nerds like us ü§ì. . | Automatic Code Comment Generation using Deep Learning. . | . Also, feel free to contact me using my custom ‚ÄúContact‚Äù system, which uses this awesome Google Script for sending emails without the need of manually setting up a backend server, integrated into my website or on twitter @ncooper57 :). .",
            "url": "https://nathancooper.io/i-am-a-nerd/website/awesome/2020/02/03/Awesome-Things-I-Learned-Creating-My-Own-Website.html",
            "relUrl": "/website/awesome/2020/02/03/Awesome-Things-I-Learned-Creating-My-Own-Website.html",
            "date": " ‚Ä¢ Feb 3, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "&lt;!DOCTYPE html&gt; . Redirecting to https://nathancooper.io &lt;link rel=‚Äùcanonical‚Äù href=https://nathancooper.io/#/about&gt; .",
          "url": "https://nathancooper.io/i-am-a-nerd/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}