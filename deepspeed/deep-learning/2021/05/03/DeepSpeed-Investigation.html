<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>DeepSpeed Investigation: What I Learned | IAmANerd</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="DeepSpeed Investigation: What I Learned" />
<meta name="author" content="Nathan Cooper" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An investigation into the awesome DeepSpeed library for training large models on a single GPU!" />
<meta property="og:description" content="An investigation into the awesome DeepSpeed library for training large models on a single GPU!" />
<link rel="canonical" href="https://nathancooper.io/i-am-a-nerd/deepspeed/deep-learning/2021/05/03/DeepSpeed-Investigation.html" />
<meta property="og:url" content="https://nathancooper.io/i-am-a-nerd/deepspeed/deep-learning/2021/05/03/DeepSpeed-Investigation.html" />
<meta property="og:site_name" content="IAmANerd" />
<meta property="og:image" content="https://nathancooper.io/i-am-a-nerd/images/deepspeed_overview.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-03T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://nathancooper.io/i-am-a-nerd/deepspeed/deep-learning/2021/05/03/DeepSpeed-Investigation.html","@type":"BlogPosting","headline":"DeepSpeed Investigation: What I Learned","dateModified":"2021-05-03T00:00:00-05:00","datePublished":"2021-05-03T00:00:00-05:00","image":"https://nathancooper.io/i-am-a-nerd/images/deepspeed_overview.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://nathancooper.io/i-am-a-nerd/deepspeed/deep-learning/2021/05/03/DeepSpeed-Investigation.html"},"author":{"@type":"Person","name":"Nathan Cooper"},"description":"An investigation into the awesome DeepSpeed library for training large models on a single GPU!","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/i-am-a-nerd/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nathancooper.io/i-am-a-nerd/feed.xml" title="IAmANerd" /><link rel="shortcut icon" type="image/x-icon" href="/i-am-a-nerd/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/i-am-a-nerd/">IAmANerd</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/i-am-a-nerd/about/">About Me</a><a class="page-link" href="/i-am-a-nerd/search/">Search</a><a class="page-link" href="/i-am-a-nerd/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">DeepSpeed Investigation: What I Learned</h1><p class="page-description">An investigation into the awesome DeepSpeed library for training large models on a single GPU!</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-03T00:00:00-05:00" itemprop="datePublished">
        May 3, 2021
      </time>‚Ä¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Nathan Cooper</span></span>
       ‚Ä¢ <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/i-am-a-nerd/categories/#deepspeed">deepspeed</a>
        &nbsp;
      
        <a class="category-tags-link" href="/i-am-a-nerd/categories/#deep-learning">deep-learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#what-is-deepspeed">What is DeepSpeed?</a></li>
<li class="toc-entry toc-h2"><a href="#putting-deepspeed-to-the-test">Putting DeepSpeed to the Test!</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion-time">Conclusion Time</a></li>
</ul><p>Deep learning is awesome, but the large compute and data requirements can prevent a lot of amazing people from using the models and contributing to the field. So, when I read about the amazing <a href="https://www.deepspeed.ai/">DeepSpeed</a> library allowing people with just a single GPU (like myself) to train massive models that would normally require multiple GPUs to just fit in memory, I had to investigate further!</p>

<h2 id="what-is-deepspeed">
<a class="anchor" href="#what-is-deepspeed" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is DeepSpeed?</h2>

<p>Here is a brief blurb from the DeepSpeed website on what it is and what it can do:</p>

<p>‚Äú</p>

<p>DeepSpeed is a deep learning optimization library that makes distributed training easy, efficient, and effective.</p>

<p><strong><em>10x Larger Models</em></strong></p>

<p><strong><em>10x Faster Training</em></strong></p>

<p><strong><em>Minimal Code Change</em></strong></p>

<p>DeepSpeed delivers extreme-scale model training for everyone, from data scientists training on massive supercomputers to those training on low-end clusters or even on a single GPU</p>

<p>‚Äú</p>

<p>Some impressive statements, but are they true? Kind of. Let‚Äôs dig a bit deeper into how this works.</p>

<p><img src="https://www.microsoft.com/en-us/research/uploads/prod/2020/05/1400x788DeepSpeedslowed.gif" alt="Overview of the large improvement ZeRO-2 and the DeepSpeed library has over ZeRO-2 and previous approaches.">
<em>From https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/</em></p>

<p>DeepSpeed is a library that enables the awesome <a href="https://arxiv.org/abs/1910.02054">Zero Redundancy Optimizer (ZeRO)</a>, which is a highly optimized optimizer (oh how clever) that improves memory management and communication in data or model parallelized work loads by removing redundancy. Now, this might bring up the question ‚Äúparallelized work loads, I thought we could use this on a single GPU, what‚Äôs the deal?‚Äù So, the deal is that ZeRO was made to solve the problem of communication between multiple devices by doing some nifty memory tricks that are beyond the scope of this blog post (and my understanding. See <a href="https://youtu.be/tC01FRB0M7w">here</a> for a full explanation of this.). It just so happens that the ZeRO optimizer also performs CPU offloading, which moves some of the computation off your GPU and onto your CPU. With things being computed on your CPU, some of the model is stored in RAM rather than the GPUs VRAM. This significantly slows computation since CPUs and RAM wasn‚Äôt built with this in mind, but it means you are allowed to train bigger models and train with bigger batch sizes ü§ì.</p>

<h2 id="putting-deepspeed-to-the-test">
<a class="anchor" href="#putting-deepspeed-to-the-test" aria-hidden="true"><span class="octicon octicon-link"></span></a>Putting DeepSpeed to the Test!</h2>

<p>To test out DeepSpeed, I used the awesome HuggingFace transformers library, which supports using DeepSpeed on their non-stable branch (though support is coming to the stable branch in 4.6 ü§ì). I followed these awesome <a href="https://huggingface.co/transformers/master/main_classes/trainer.html#deepspeed">instructions</a> on the HuggingFace‚Äôs website for getting started with DeepSpeed and HuggingFace. If you want to follow along at home, I created a Github <a href="https://github.com/ncoop57/deepspeed_testing">repository</a> with the Dockerfile (I‚Äôm addicted to docker and will probably make a blog post on docker too :)) and the test script I used to run my experiments on. I tried training the different versions of the awesome <a href="https://arxiv.org/abs/1910.10683">T5 model</a> that ranged from smallish ~60 million parameters to humungous 3 billion parameters. And here are my results:</p>

<p><img src="/i-am-a-nerd/images/deepspeed_chart.png" alt="Bar chart showing DeepSpeed increases time to train, but allows training larger models compared to not using DeepSpeed.">
<em>This was run on a machine with Ubuntu 20.04, 32GBs of RAM, Ryzen 5600x, and NVIDIA RTX 3080 GPU.</em></p>

<p>This is a chart of the different models‚Äô training time in seconds with and without DeepSpeed and the biggest batch size I could fit for each. As you can see, using DeepSpeed increases training time, except for t5-small where the time is nearly identical. However, you‚Äôll notice for t5-base (~220 million parameters) and t5-large (~770 million parameters) I am able to use a larger batch size, this is most noticable in the t5-large model where I can double the batch size. This is the important part that DeepSpeed gives you, it allows you to use larger batch sizes even if it increases the training time. Having large batch sizes is critical for many deep learning models as it allows the model to see more examples when doing updates thereby improving performance. This is the use case of using DeepSpeed, using big models with large batch sizes. If what you are doing doesn‚Äôt involve these two things, then you probably should skip DeepSpeed.</p>

<p><strong>Note:</strong> You‚Äôll notice there is no bar for a 3 billion parameter model (t5-3b). This is because my PC cried out when I attempted to train the model even with DeepSpeed and a batch size of 1.</p>

<h2 id="conclusion-time">
<a class="anchor" href="#conclusion-time" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion Time</h2>

<p>So, with all things considered, DeepSpeed is an awesome library and ZeRO is an amazing optimizer. However, if you were looking for super speed boosts for a single GPU like I was, it ain‚Äôt it chief. ZeRO is designed for speeding up multi-GPU setups by efficiently handling memory resources and communication and in doing so does reduce the memory footprint on GPUs. It also does some awesome CPU offloading, which will allow you to train huge models with large batch sizes on a single GPU that you would not be able to normally. The part of larger batch sizes is super important for many deep learning models as it can improve their performance. So, my take away from this investigation is this: If you are using a multi-GPU setup, DeepSpeed is the way to go. However, for single GPU uses, only use it if you need a larger model andlarger batch sizes than what your normal GPU can handle.</p>

<p>Hope you‚Äôve enjoyed this blog post and learned some information along the way. Comment down below with any questions you have, I‚Äôd be happy to help answer them!</p>

<p>Connect with me:</p>

<p>Website - <a href="https://nathancooper.io/#/">https://nathancooper.io/#/</a></p>

<p>YouTube - <a href="https://www.youtube.com/channel/UCKfOCnojK5YV7_hdPjAtY7Q">https://www.youtube.com/channel/UCKfOCnojK5YV7_hdPjAtY7Q</a></p>

<p>Github - <a href="https://github.com/ncoop57">https://github.com/ncoop57</a></p>

<p>Twitter - <a href="https://twitter.com/ncooper57">https://twitter.com/ncooper57</a></p>

<p>LinkedIn - <a href="https://www.linkedin.com/in/nathan-cooper-820292106/">https://www.linkedin.com/in/nathan-cooper-820292106/</a></p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ncoop57/i-am-a-nerd"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/i-am-a-nerd/deepspeed/deep-learning/2021/05/03/DeepSpeed-Investigation.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/i-am-a-nerd/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/i-am-a-nerd/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/i-am-a-nerd/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A singular place for all of my nerdy ramblings.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/ncoop57" title="ncoop57"><svg class="svg-icon grey"><use xlink:href="/i-am-a-nerd/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/ncooper57" title="ncooper57"><svg class="svg-icon grey"><use xlink:href="/i-am-a-nerd/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
